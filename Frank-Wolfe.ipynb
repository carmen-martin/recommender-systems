{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: \n",
    "\n",
    "### The end goal is to find a matrix which gives the minimum square loss w.r.t. the original matrix (i.e. reviews). \n",
    "\n",
    "\n",
    "$$min_{Z \\in \\mathbb{R}^{m\\times n}}\\ \\  f(Z) :=  \\frac{1}{2}\\sum_{(i,j)\\in \\Omega} \\left( Z_{ij} - X_{ij}\\right)^{2}$$\n",
    "\n",
    "Subject to having a low rank (or a small nuclear norm). \n",
    "\n",
    "\n",
    "NOTE: It would seem to me that the gradient of this function would simply be: \n",
    "$$ \\nabla f(Z) = \\sum_{ij}(Z_{ij} - X_{ij}) $$\n",
    "right?\n",
    "\n",
    "\n",
    "### This found matrix needs to have low rank for two reasons: \n",
    "1. It is a constraint that allows for the problem to be well posed. (Otherwise the matrix elements could be anything). \n",
    "2. Rank identifies the number of linearly indipendent columns, since we want to build a recommender system we would like there to be relations among the columns, the constraint is not a weakness, it is a desired property.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### How is this matrix found? (i think)\n",
    "\n",
    "1. The Frank-Wolfe algorithm starts at the zeroth (k) iteration with a guess of a good solution, this solution is called $Z^{k}$. \n",
    "2. This solution (it is a matrix) is then used to solve a \"sub\"-minimization problem. Frank-Wolfe looks for a matrix $Z$ (with nuclear norm below a certain quantity) which minimizes the following quantity: $$ \\nabla f(Z^{k}) \\cdot Z $$ The matrix which satisfies this is called $\\tilde{Z}^{k}$. \n",
    "3. Then the algorithm chooses $Z^{k+1}$ by means of a standard stepping procedure: $$ Z^{k+1} = (1-\\alpha_{k}) Z^{k} + \\alpha_{k}\\tilde{Z}^{k}$$ \n",
    "\n",
    "\n",
    "Note that the FW algorithms already gives a recipe to calculate the matrix $\\tilde{Z}^{k}$ at step 2. It holds that: \n",
    "$$ \\tilde{Z}^{k} = -\\delta u_{1}(v_{1})^{T}$$ is a rank-one matrix. Where $u_{1}, v_{1}$ are the singular vectors (why only 2 ?) associated with the largest singular value of $\\nabla f(Z^{k})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------IDEAS TO APPLY TO ACTUAL DATA---------------\n",
    "\n",
    "# df.fillna(0)\n",
    "# function to replace NaNs with 0s. (nobody gives 0 rating, come on.)\n",
    "\n",
    "# Then I can use https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html\n",
    "# to search for the singular values of the gradient of the matrix. \n",
    "\n",
    "# Then I update the matrix with the procedure in the paper/slides\n",
    "\n",
    "# Repeat until tolerance/max iterations reached (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Example of nuclear norm:\n",
      "\n",
      " The matrix is: \n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "\n",
      " Its nuclear norm is:  3.0\n"
     ]
    }
   ],
   "source": [
    "# (probably won't need this function, maybe as a check on the nuclear norm of the solution at each step (?) ) #\n",
    "\n",
    "\n",
    "\n",
    "#This function calculates the nuclear norm of a matrix.\n",
    "# (the nuclear norm is the sum of all non-zero singular values of a matrix)\n",
    "\n",
    "\n",
    "#the [1] accesses the eigenvalues of the diagonal matrix D -----> X^{T}*D*X\n",
    "def nuclear_norm(A):\n",
    "    return np.sum(np.linalg.svd(A)[1])\n",
    "\n",
    "X = np.array([[1,0,0], [0,1,0], [0, 0, 1]])\n",
    "Y= np.array([[1,0,1], [0,1,0], [1, 0, 1]])\n",
    "\n",
    "print('\\n Example of nuclear norm:')\n",
    "print('\\n The matrix is: \\n', X)\n",
    "print('\\n Its nuclear norm is: ', nuclear_norm(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def step(data, previous, delta):\n",
    "    # might not be a good idea to pass the original data matrix everytime\n",
    "    \n",
    "    # ----------calculate gradient here (is it a number? an array? a matrix?)\n",
    "    \n",
    "    #-----------find SVD of gradient\n",
    "    \n",
    "    #-----------take u_1 and v_1 and delta and create update TILDE ZK (check that it has rank one)\n",
    "    \n",
    "    #------------create new iterate Z^{k+1}\n",
    "\n",
    "    #------------repeat\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
