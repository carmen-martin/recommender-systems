{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Imports\n",
    "from scipy import sparse\n",
    "import scipy\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from numpy import linalg as LA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Frank-Wolfe - standard algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kobsOmgimqL_",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FW objective function\n",
    "def FW_objective_function(diff_vec):\n",
    "    return 0.5*(np.power(diff_vec,2).sum())\n",
    "\n",
    "# Regular FW algorithm\n",
    "def FrankWolfe(X, objective_function, delta, empties = 0, printing_res = True, Z_init = None, max_iter = 150, patience = 1e-3):\n",
    "    '''\n",
    "    :param X: sparse matrix with ratings and 'empty values', rows - users, columns - books.\n",
    "    :param objective_function: objective function that we would like to minimize with FW\n",
    "    :param delta: Radius of the feasible's set ball\n",
    "    :param empties (optional): Empty values of X are zeros (0) or NaN ('nan'). Default = 0\n",
    "    :param Z_init (optional): In case we want to initialize Z with a known matrix, if not given Z_init will be a zeros matrix. Default = None.\n",
    "    :param max_iter (optional): max number of iterations for the method. Default = 150.\n",
    "    :param patience (optional): once reached this tolerance provide the result. Default = 1e-3.\n",
    "    :return: Z: matrix of predicted ratings - it should be like X but with no 'empty values'\n",
    "    :return: accuracy: difference between original values (X) and predicted ones (Z)\n",
    "    '''\n",
    "\n",
    "    # Get X indexes for not empty values\n",
    "    if empties == 0:\n",
    "        idx_ratings = np.argwhere(X != 0)\n",
    "    elif empties == 'nan':\n",
    "        idx_ratings = np.argwhere(~np.isnan(X))\n",
    "    else:\n",
    "        return print('Error: Empties argument', empties, 'not valid.')\n",
    "\n",
    "    idx_rows = idx_ratings[:,0]\n",
    "    idx_cols = idx_ratings[:,1]\n",
    "\n",
    "    # Initialize Z\n",
    "    if Z_init == 'random uniform':\n",
    "        Z = np.random.uniform(low = 0.01, high = 1, size = X.shape)\n",
    "    elif Z_init is not None:\n",
    "        Z = Z_init\n",
    "    else:\n",
    "        Z = np.zeros(X.shape)\n",
    "\n",
    "    # Create vectors with the not empty features of the sparse matrix\n",
    "    X_rated = X[idx_rows, idx_cols]\n",
    "    Z_rated = Z[idx_rows, idx_cols]\n",
    "    diff_vec = np.array(Z_rated - X_rated)[0]\n",
    "\n",
    "    # Create needed variables\n",
    "    res_list = []\n",
    "    diff_loss = patience + 1\n",
    "    loss = objective_function(diff_vec)\n",
    "    it = 0\n",
    "    while (diff_loss > patience) and (it < max_iter):\n",
    "\n",
    "        # Gradient\n",
    "        grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "\n",
    "        # SVD - Compute k = 1 singular values and its vectors, starting from the largest (which = 'LM')\n",
    "        u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')   #\n",
    "\n",
    "        # Update\n",
    "        Zk_tilde = -delta*np.outer(u_max,v_max)\n",
    "\n",
    "        alpha_k = 2/(it+2) #alpha - as studied in class\n",
    "        Z = (1-alpha_k)*Z + alpha_k*Zk_tilde\n",
    "\n",
    "        # Loss\n",
    "        diff_vec = np.array(Z[idx_rows, idx_cols] - X_rated)[0]\n",
    "        new_loss = objective_function(diff_vec)\n",
    "\n",
    "        # Improvement at this iteration\n",
    "        diff_loss = np.abs(loss - new_loss)\n",
    "        loss = new_loss\n",
    "\n",
    "        if printing_res == True:\n",
    "            if it == 1 or it % 10 == 0:\n",
    "                print('Iteration:', it, 'Loss:', loss, 'Loss diff:', diff_loss, 'Rank(Z): ', np.linalg.matrix_rank(Z))\n",
    "\n",
    "        # Count iteration\n",
    "        it += 1\n",
    "\n",
    "        res_list.append(loss)\n",
    "        \n",
    "    return Z, loss, res_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y-VOlLMvfq8"
   },
   "source": [
    "# Frank-Wolfe In-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "time_out = time.process_time() + 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Binary search for alpha stop\n",
    "def alpha_binary_search(Zk, Dk, delta, max_value = 1, min_value = 0, tol = 0.3):\n",
    "\n",
    "    #Inizialization\n",
    "\n",
    "    best_alpha = (max_value - min_value) / 2\n",
    "\n",
    "    testing_matrix = Zk + best_alpha * Dk\n",
    "\n",
    "    sentinel = False\n",
    "\n",
    "    while time.process_time() <= time_out:\n",
    "\n",
    "      testing_mat_nuclear_norm = LA.norm(testing_matrix, ord = 'nuc')\n",
    "\n",
    "      sentinel = True\n",
    "\n",
    "    #Binary Search\n",
    "\n",
    "    if sentinel == True:\n",
    "\n",
    "      while testing_mat_nuclear_norm <= delta and (max_value - min_value) >= tol and time.process_time() <= time_out:\n",
    "\n",
    "          min_value = best_alpha\n",
    "\n",
    "          best_alpha = (max_value - min_value) / 2\n",
    "\n",
    "          testing_matrix = Zk + best_alpha * Dk\n",
    "\n",
    "          testing_mat_nuclear_norm = LA.norm(testing_matrix, ord = 'nuc')\n",
    "\n",
    "    return best_alpha"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def FW_inface(X, objective_function, delta, L = 1, D = None, gamma1 = 0, gamma2 = 1, THRES = 0.001, empties = 0, max_iter=150, patience=1e-3, printing = True):\n",
    "    '''\n",
    "    :param X: sparse matrix with ratings and 'empty values', rows - users, columns - books.\n",
    "    :param objective_function: objective function that we would like to minimize with FW.\n",
    "    :param delta: Radius of the feasible's set ball\n",
    "    :param L: must be greater than 1\n",
    "    :param D: if not inputed = 2*delta\n",
    "    :param gamma1:\n",
    "    :param gamma2:\n",
    "    :param THRES:\n",
    "    :param empties (optional): Empty values of X are zeros (0) or NaN ('nan'). Default = 0\n",
    "    :param max_iter: max number of iterations for the method.\n",
    "    :param patience: once reached this tolerance provide the result.\n",
    "    :return: Z: matrix of predicted ratings - it should be like X but with no 'empty values'\n",
    "            loss: difference between original values (X) and predicted ones (Z).\n",
    "    '''\n",
    "\n",
    "    # Get X indexes for not empty values\n",
    "    if empties == 0:\n",
    "        idx_ratings = np.argwhere(X != 0)\n",
    "    elif empties == 'nan':\n",
    "        idx_ratings = np.argwhere(~np.isnan(X))\n",
    "    else:\n",
    "        return print('Empties argument', empties, 'not valid.')\n",
    "\n",
    "    idx_rows = idx_ratings[:,0]\n",
    "    idx_cols = idx_ratings[:,1]\n",
    "\n",
    "    # Initialize Z_{-1}\n",
    "    Z0 = np.zeros(X.shape)\n",
    "\n",
    "    # Create vectors with the not empty features of the sparse matrix\n",
    "    X_rated = X[idx_rows, idx_cols]\n",
    "    Z_rated = Z0[idx_rows, idx_cols]\n",
    "    diff_vec = np.array(Z_rated - X_rated)[0]\n",
    "\n",
    "    # Initial gradient and Z0\n",
    "    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "    u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "    Zk = -delta*np.outer(u_max,v_max)\n",
    "    Z_rated = Zk[idx_rows, idx_cols]\n",
    "\n",
    "    print('Initial Zk Rank: ', LA.matrix_rank(Zk))\n",
    "\n",
    "    # Initialize lower bound on the optimal objective function (f*)\n",
    "    diff_vec = np.array(Z_rated - X_rated)[0]\n",
    "    new_low_bound = np.max((objective_function(diff_vec) + np.multiply(diff_vec,Z_rated)), 0).sum()\n",
    "    #new_low_bound = 0 #used 0 otherwise the other new_low_bound was too high!\n",
    "\n",
    "    # Set D\n",
    "    if D is not None:\n",
    "        D = D\n",
    "    else:\n",
    "        D = 2*delta\n",
    "\n",
    "    rank_Z = LA.matrix_rank(Zk)   # rank of Zk to find thin SVD size\n",
    "\n",
    "    # Additional needed parameters\n",
    "    diff_loss = patience + 1\n",
    "    loss = objective_function(diff_vec)\n",
    "    it = 0\n",
    "    res_list = [loss]\n",
    "\n",
    "    B_used = 0\n",
    "    A_used = 0\n",
    "    not_entered = 0\n",
    "    regularFW = 0\n",
    "\n",
    "    while (diff_loss > patience) and (it < max_iter):\n",
    "\n",
    "        # Thin SVD\n",
    "        U_thin, s_thin, Vh_thin = sparse.linalg.svds(Zk, k = rank_Z, which='LM')   # Compute k = rank singular values\n",
    "\n",
    "        # Lower bound update\n",
    "        low_bound = new_low_bound\n",
    "\n",
    "        # In-face direction with the away step strategy: two calculations depending of where Z lies within the feasible set\n",
    "        if s_thin.sum() <= delta: # Z in border (sum of singular values == radius of feasible set)\n",
    "            print('Zk in border!')\n",
    "            G = 0.5 * (Vh_thin.dot(grad.T.dot(U_thin)) + U_thin.T.dot(grad.dot(Vh_thin.T)))\n",
    "            # Obtain unitary eigenvector corresponding to smallest eigenvalue of G\n",
    "            #lamb, u = sparse.linalg.eigs(G, k = 1, which = 'SM')\n",
    "            eigvalues, eigvectors = LA.eig(G)  #find the eigenvalues\n",
    "            min_eig = np.argmin(eigvalues)  #find the index of the smallest eigenvalue\n",
    "            u = eigvectors[:, min_eig]  #take the eigenvector corresponding to the smallest eigenvalue\n",
    "\n",
    "            # Update value\n",
    "            M = np.outer(u,u.T)\n",
    "            Zk_tilde = delta*U_thin.dot(M.dot(Vh_thin))\n",
    "            Dk = Zk - Zk_tilde\n",
    "            #alpha_B = 0.5\n",
    "\n",
    "            inv_s_thin = np.diag(1/s_thin)\n",
    "            alpha_B = 1/(delta*u.T.dot(inv_s_thin).dot(u)-1)\n",
    "\n",
    "        else: #inside\n",
    "            grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "            u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "            Zk_tilde = delta*np.outer(u_max,v_max)\n",
    "            Dk = Zk - Zk_tilde\n",
    "            #BINARY SEARCH\n",
    "            '''\n",
    "            alpha_B = alpha_binary_search(Zk,\n",
    "                                          Dk,\n",
    "                                          delta)\n",
    "            '''\n",
    "            alpha_B = 0.5\n",
    "\n",
    "        nuclear_norm = s_thin.sum()\n",
    "        print('thres:', abs(delta - nuclear_norm))\n",
    "        print('rank', rank_Z)\n",
    "        if abs(delta - nuclear_norm) < THRES and rank_Z > 1:\n",
    "            Z_B = Zk + alpha_B*Dk\n",
    "            diff_vec_B = Z_B[idx_rows, idx_cols] - X_rated\n",
    "\n",
    "            if 1/(objective_function(diff_vec_B)-low_bound) >= (1/(loss-low_bound)+gamma1/(2*L*D**2)):\n",
    "              # 1. Move to a lower dimensional face\n",
    "              print('Went to a lower-dimensional face')\n",
    "              B_used += 1\n",
    "              Zk = Z_B\n",
    "\n",
    "            else:\n",
    "                beta = alpha_B/5 # FIND A GOOD VALUE -- a binary search is also suggested by the paper\n",
    "                Z_A = Zk + beta*Dk\n",
    "                diff_vec_A = Z_A[idx_rows, idx_cols] - X_rated\n",
    "                if 1/(objective_function(diff_vec_A)-low_bound) >= (1/(loss-low_bound)+gamma2/(2*L*D**2)):\n",
    "                    # 2. Stay in the current face\n",
    "                    print('Stayed in the current face')\n",
    "                    A_used += 1\n",
    "                    Zk = Z_A\n",
    "                else:\n",
    "                    # 3. Do a regular FW step and update the lower bound\n",
    "                    print('Do a regular FW step')\n",
    "                    regularFW += 1\n",
    "                    #Zk update\n",
    "                    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "                    u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "                    Zk_tilde = -delta*np.outer(u_max,v_max)\n",
    "\n",
    "                    '''# Lower bound update\n",
    "                    B_w = loss + grad.T.dot(Zk_tilde - Zk)\n",
    "                    print(B_w, low_bound)\n",
    "                    new_low_bound = np.max(low_bound, B_w)'''\n",
    "\n",
    "                    direction_vec = Zk_tilde.flatten() - Zk.flatten()\n",
    "                    grad = grad.toarray()\n",
    "                    wolfe_gap = grad.T.flatten() * direction_vec\n",
    "                    B_w = loss + wolfe_gap.sum()\n",
    "\n",
    "                    #TRIED THIS INSTEAD\n",
    "                    if low_bound >= B_w:\n",
    "                      new_low_bound = low_bound\n",
    "                    else:\n",
    "                      new_low_bound = B_w\n",
    "\n",
    "                    # Z_(k+1)\n",
    "                    alpha_k = 2/(it+2)\n",
    "                    Zk = (1-alpha_k)*Zk + alpha_k*Zk_tilde\n",
    "\n",
    "        else:\n",
    "\n",
    "            # 3. Do a regular FW step and update the lower bound\n",
    "            print('Do a regular FW step - not entered initial if')\n",
    "            not_entered += 1\n",
    "            #Zk update\n",
    "            grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "            u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "            Zk_tilde = -delta*np.outer(u_max,v_max)\n",
    "\n",
    "            '''# Lower bound update\n",
    "            B_w = loss + grad.T.dot(Zk_tilde - Zk)\n",
    "            print(B_w, low_bound)\n",
    "            new_low_bound = np.max(low_bound, B_w)'''\n",
    "\n",
    "            direction_vec = Zk_tilde.flatten() - Zk.flatten()\n",
    "            grad = grad.toarray()\n",
    "            wolfe_gap = grad.T.flatten() * direction_vec\n",
    "            B_w = loss + wolfe_gap.sum()\n",
    "\n",
    "            #TRIED THIS INSTEAD\n",
    "            if low_bound >= B_w:\n",
    "                new_low_bound = low_bound\n",
    "            else:\n",
    "                new_low_bound = B_w\n",
    "\n",
    "            # Z_(k+1)\n",
    "            alpha_k = 2/(it+2)\n",
    "            Zk = (1-alpha_k)*Zk + alpha_k*Zk_tilde\n",
    "\n",
    "        # Loss\n",
    "        diff_vec = np.array(Zk[idx_rows, idx_cols] - X_rated)[0]\n",
    "        new_loss = objective_function(diff_vec)\n",
    "\n",
    "        # Improvement at this iteration\n",
    "        diff_loss = np.abs(loss - new_loss)\n",
    "        loss = new_loss\n",
    "\n",
    "        rank_Z = LA.matrix_rank(Zk)   # rank of Zk to find thin SVD size\n",
    "\n",
    "        # Gradient\n",
    "        #grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "\n",
    "        # Count iteration\n",
    "        it += 1\n",
    "\n",
    "        res_list.append(loss)\n",
    "\n",
    "        if printing == True:\n",
    "          if it % 1 == 0 or it == 1:\n",
    "            print('Iteration:', it, 'Loss:', loss, 'Loss diff:', diff_loss, 'Rank(Z): ', rank_Z)\n",
    "    print('Went to lower dim face:', B_used)\n",
    "    print('Stayed:', A_used)\n",
    "    print('Regular:', regularFW)\n",
    "    print('Not entered if:', not_entered)\n",
    "    return Zk, loss, res_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Zk Rank:  1\n",
      "Zk in border!\n",
      "thres: 3.3306690738754696e-16\n",
      "rank 1\n",
      "Do a regular FW step - not entered initial if\n",
      "Iteration: 1 Loss: 0.5884274837060905 Loss diff: 0.3557793190841786 Rank(Z):  1\n",
      "thres: 2.220446049250313e-16\n",
      "rank 1\n",
      "Do a regular FW step - not entered initial if\n",
      "Iteration: 2 Loss: 0.3280182451599031 Loss diff: 0.26040923854618747 Rank(Z):  2\n",
      "Zk in border!\n",
      "thres: 0.1581994647578966\n",
      "rank 2\n",
      "Do a regular FW step\n",
      "Iteration: 3 Loss: 0.2545835594764261 Loss diff: 0.073434685683477 Rank(Z):  3\n",
      "Zk in border!\n",
      "thres: 0.16669523549971332\n",
      "rank 3\n",
      "Do a regular FW step\n",
      "Iteration: 4 Loss: 0.25797517047560814 Loss diff: 0.0033916109991820598 Rank(Z):  4\n",
      "Zk in border!\n",
      "thres: 0.10772017543507673\n",
      "rank 4\n",
      "Went to a lower-dimensional face\n",
      "Iteration: 5 Loss: 0.2510203720733919 Loss diff: 0.006954798402216245 Rank(Z):  3\n",
      "Zk in border!\n",
      "thres: 0.17590905073120489\n",
      "rank 3\n",
      "Do a regular FW step\n",
      "Iteration: 6 Loss: 0.2422039641819696 Loss diff: 0.008816407891422295 Rank(Z):  4\n",
      "Zk in border!\n",
      "thres: 0.1258874858868515\n",
      "rank 4\n",
      "Stayed in the current face\n",
      "Iteration: 7 Loss: 0.2387789626137598 Loss diff: 0.0034250015682097956 Rank(Z):  4\n",
      "Zk in border!\n",
      "thres: 0.13584263220436155\n",
      "rank 4\n",
      "Do a regular FW step\n",
      "Iteration: 8 Loss: 0.22268037622992523 Loss diff: 0.016098586383834573 Rank(Z):  5\n",
      "Zk in border!\n",
      "thres: 0.11524201659119315\n",
      "rank 5\n",
      "Do a regular FW step\n",
      "Iteration: 9 Loss: 0.21710354638378837 Loss diff: 0.00557682984613686 Rank(Z):  6\n",
      "Zk in border!\n",
      "thres: 0.09607248396422796\n",
      "rank 6\n",
      "Do a regular FW step\n",
      "Iteration: 10 Loss: 0.20867242856159546 Loss diff: 0.008431117822192913 Rank(Z):  7\n",
      "Zk in border!\n",
      "thres: 0.08008210660202297\n",
      "rank 7\n",
      "Do a regular FW step\n",
      "Iteration: 11 Loss: 0.2075806694245347 Loss diff: 0.0010917591370607493 Rank(Z):  8\n",
      "Zk in border!\n",
      "thres: 0.0667908897417987\n",
      "rank 8\n",
      "Do a regular FW step\n",
      "Iteration: 12 Loss: 0.2029205183388823 Loss diff: 0.004660151085652403 Rank(Z):  9\n",
      "Zk in border!\n",
      "thres: 0.057583832447203775\n",
      "rank 9\n",
      "Do a regular FW step\n",
      "Iteration: 13 Loss: 0.2000576736223782 Loss diff: 0.0028628447165041093 Rank(Z):  10\n",
      "Zk in border!\n",
      "thres: 0.04944218495484254\n",
      "rank 10\n",
      "Do a regular FW step\n",
      "Iteration: 14 Loss: 0.20126122383606376 Loss diff: 0.001203550213685567 Rank(Z):  11\n",
      "Zk in border!\n",
      "thres: 0.042902022666379436\n",
      "rank 11\n",
      "Went to a lower-dimensional face\n",
      "Iteration: 15 Loss: 0.20006407048672475 Loss diff: 0.0011971533493390085 Rank(Z):  10\n",
      "Zk in border!\n",
      "thres: 0.049519608521065184\n",
      "rank 10\n",
      "Do a regular FW step\n",
      "Iteration: 16 Loss: 0.20054517029591262 Loss diff: 0.0004810998091878682 Rank(Z):  11\n",
      "Zk in border!\n",
      "thres: 0.04370722320625664\n",
      "rank 11\n",
      "Do a regular FW step\n",
      "Iteration: 17 Loss: 0.19683090947640283 Loss diff: 0.0037142608195097904 Rank(Z):  12\n",
      "Zk in border!\n",
      "thres: 0.03907784445384188\n",
      "rank 12\n",
      "Do a regular FW step\n",
      "Iteration: 18 Loss: 0.19494612974886555 Loss diff: 0.0018847797275372835 Rank(Z):  13\n",
      "Zk in border!\n",
      "thres: 0.03505415087410735\n",
      "rank 13\n",
      "Do a regular FW step\n",
      "Iteration: 19 Loss: 0.19386273768522622 Loss diff: 0.0010833920636393324 Rank(Z):  14\n",
      "Zk in border!\n",
      "thres: 0.031626921194244706\n",
      "rank 14\n",
      "Do a regular FW step\n",
      "Iteration: 20 Loss: 0.1939788571195147 Loss diff: 0.00011611943428849192 Rank(Z):  15\n",
      "Zk in border!\n",
      "thres: 0.028903629022372423\n",
      "rank 15\n",
      "Do a regular FW step\n",
      "Iteration: 21 Loss: 0.19249650073269337 Loss diff: 0.001482356386821343 Rank(Z):  16\n",
      "Zk in border!\n",
      "thres: 0.026337766266119167\n",
      "rank 16\n",
      "Do a regular FW step\n",
      "Iteration: 22 Loss: 0.19172265543753642 Loss diff: 0.0007738452951569497 Rank(Z):  17\n",
      "Zk in border!\n",
      "thres: 0.02427441880880432\n",
      "rank 17\n",
      "Do a regular FW step\n",
      "Iteration: 23 Loss: 0.1912598939253997 Loss diff: 0.0004627615121367279 Rank(Z):  18\n",
      "Zk in border!\n",
      "thres: 0.02227513552592708\n",
      "rank 18\n",
      "Do a regular FW step\n",
      "Iteration: 24 Loss: 0.1918370101135488 Loss diff: 0.0005771161881491238 Rank(Z):  19\n",
      "Zk in border!\n",
      "thres: 0.02051086575223071\n",
      "rank 19\n",
      "Do a regular FW step\n",
      "Iteration: 25 Loss: 0.19038395009180611 Loss diff: 0.001453060021742697 Rank(Z):  20\n",
      "Zk in border!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmart\\AppData\\Local\\Temp\\ipykernel_40660\\3884968031.py:93: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  inv_s_thin = np.diag(1/s_thin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thres: 0.018948440462874583\n",
      "rank 20\n",
      "Do a regular FW step\n",
      "Iteration: 26 Loss: 0.1896859758633929 Loss diff: 0.0006979742284132229 Rank(Z):  21\n",
      "Zk in border!\n",
      "thres: 0.01755287646222048\n",
      "rank 21\n",
      "Do a regular FW step\n",
      "Iteration: 27 Loss: 0.18926588325599594 Loss diff: 0.0004200926073969491 Rank(Z):  22\n",
      "Zk in border!\n",
      "thres: 0.016328038718359306\n",
      "rank 22\n",
      "Do a regular FW step\n",
      "Iteration: 28 Loss: 0.1892537809083828 Loss diff: 1.2102347613146547e-05 Rank(Z):  23\n",
      "Zk in border!\n",
      "thres: 0.015224681637605375\n",
      "rank 23\n",
      "Do a regular FW step\n",
      "Iteration: 29 Loss: 0.18867976350925414 Loss diff: 0.0005740173991286568 Rank(Z):  24\n",
      "Zk in border!\n",
      "thres: 0.014219414514206496\n",
      "rank 24\n",
      "Do a regular FW step\n",
      "Iteration: 30 Loss: 0.18846069895847262 Loss diff: 0.00021906455078152254 Rank(Z):  25\n",
      "Zk in border!\n",
      "thres: 0.013342046868287882\n",
      "rank 25\n",
      "Do a regular FW step\n",
      "Iteration: 31 Loss: 0.1886037172411087 Loss diff: 0.00014301828263607597 Rank(Z):  26\n",
      "Zk in border!\n",
      "thres: 0.0125236899478407\n",
      "rank 26\n",
      "Do a regular FW step\n",
      "Iteration: 32 Loss: 0.18807665086338385 Loss diff: 0.0005270663777248441 Rank(Z):  27\n",
      "Zk in border!\n",
      "thres: 0.011787620078798189\n",
      "rank 27\n",
      "Do a regular FW step\n",
      "Iteration: 33 Loss: 0.18789067861626357 Loss diff: 0.00018597224712027516 Rank(Z):  28\n",
      "Zk in border!\n",
      "thres: 0.0111103611723119\n",
      "rank 28\n",
      "Do a regular FW step\n",
      "Iteration: 34 Loss: 0.1880221649537389 Loss diff: 0.00013148633747533944 Rank(Z):  29\n",
      "Zk in border!\n",
      "thres: 0.01048015985202344\n",
      "rank 29\n",
      "Do a regular FW step\n",
      "Iteration: 35 Loss: 0.18761251959317518 Loss diff: 0.0004096453605637318 Rank(Z):  30\n",
      "Zk in border!\n",
      "thres: 0.009912338816205346\n",
      "rank 30\n",
      "Do a regular FW step\n",
      "Iteration: 36 Loss: 0.18742345959435774 Loss diff: 0.00018905999881743685 Rank(Z):  31\n",
      "Zk in border!\n",
      "thres: 0.009372419853200364\n",
      "rank 31\n",
      "Do a regular FW step\n",
      "Iteration: 37 Loss: 0.1875007280534102 Loss diff: 7.726845905245416e-05 Rank(Z):  32\n",
      "Zk in border!\n",
      "thres: 0.008885893515143684\n",
      "rank 32\n",
      "Do a regular FW step\n",
      "Iteration: 38 Loss: 0.18734371930901206 Loss diff: 0.0001570087443981405 Rank(Z):  33\n",
      "Zk in border!\n",
      "thres: 0.008449648561799417\n",
      "rank 33\n",
      "Do a regular FW step\n",
      "Iteration: 39 Loss: 0.18700435872297827 Loss diff: 0.0003393605860337878 Rank(Z):  34\n",
      "Zk in border!\n",
      "thres: 0.008033915304127559\n",
      "rank 34\n",
      "Do a regular FW step\n",
      "Iteration: 40 Loss: 0.1867996424516792 Loss diff: 0.00020471627129906023 Rank(Z):  35\n",
      "Zk in border!\n",
      "thres: 0.0076626054282051825\n",
      "rank 35\n",
      "Do a regular FW step\n",
      "Iteration: 41 Loss: 0.1866903132797042 Loss diff: 0.00010932917197500891 Rank(Z):  36\n",
      "Zk in border!\n",
      "thres: 0.007301624688925035\n",
      "rank 36\n",
      "Do a regular FW step\n",
      "Iteration: 42 Loss: 0.18668363486138234 Loss diff: 6.678418321859336e-06 Rank(Z):  37\n",
      "Zk in border!\n",
      "thres: 0.0069725687019430715\n",
      "rank 37\n",
      "Do a regular FW step\n",
      "Iteration: 43 Loss: 0.18666647039733616 Loss diff: 1.7164464046176553e-05 Rank(Z):  38\n",
      "Zk in border!\n",
      "thres: 0.006650359703412212\n",
      "rank 38\n",
      "Do a regular FW step\n",
      "Iteration: 44 Loss: 0.18652448470006952 Loss diff: 0.00014198569726664445 Rank(Z):  39\n",
      "Zk in border!\n",
      "thres: 0.006368872816856719\n",
      "rank 39\n",
      "Do a regular FW step\n",
      "Iteration: 45 Loss: 0.18649346510752798 Loss diff: 3.1019592541536856e-05 Rank(Z):  40\n",
      "Zk in border!\n",
      "thres: 0.00609715627913332\n",
      "rank 40\n",
      "Do a regular FW step\n",
      "Iteration: 46 Loss: 0.18638347643575998 Loss diff: 0.00010998867176800764 Rank(Z):  41\n",
      "Zk in border!\n",
      "thres: 0.005840172607002314\n",
      "rank 41\n",
      "Do a regular FW step\n",
      "Iteration: 47 Loss: 0.186264564123734 Loss diff: 0.00011891231202598718 Rank(Z):  42\n",
      "Zk in border!\n",
      "thres: 0.00560063413908829\n",
      "rank 42\n",
      "Do a regular FW step\n",
      "Iteration: 48 Loss: 0.18619621741564985 Loss diff: 6.834670808414378e-05 Rank(Z):  43\n",
      "Zk in border!\n",
      "thres: 0.005388459505396459\n",
      "rank 43\n",
      "Do a regular FW step\n",
      "Iteration: 49 Loss: 0.18635229444108709 Loss diff: 0.0001560770254372401 Rank(Z):  44\n",
      "Zk in border!\n",
      "thres: 0.005174340245473585\n",
      "rank 44\n",
      "Do a regular FW step\n",
      "Iteration: 50 Loss: 0.186138289045504 Loss diff: 0.0002140053955830834 Rank(Z):  45\n",
      "Zk in border!\n",
      "thres: 0.004973415549694771\n",
      "rank 45\n",
      "Do a regular FW step\n",
      "Iteration: 51 Loss: 0.18607256716832016 Loss diff: 6.572187718384415e-05 Rank(Z):  46\n",
      "Zk in border!\n",
      "thres: 0.004783418636151171\n",
      "rank 46\n",
      "Do a regular FW step\n",
      "Iteration: 52 Loss: 0.18596618552127456 Loss diff: 0.00010638164704560027 Rank(Z):  47\n",
      "Zk in border!\n",
      "thres: 0.004604677103355659\n",
      "rank 47\n",
      "Do a regular FW step\n",
      "Iteration: 53 Loss: 0.1858939685912539 Loss diff: 7.221693002065765e-05 Rank(Z):  48\n",
      "Zk in border!\n",
      "thres: 0.00443550546324567\n",
      "rank 48\n",
      "Do a regular FW step\n",
      "Iteration: 54 Loss: 0.18592292227782886 Loss diff: 2.895368657496178e-05 Rank(Z):  49\n",
      "Zk in border!\n",
      "thres: 0.00427678019933353\n",
      "rank 49\n",
      "Do a regular FW step\n",
      "Iteration: 55 Loss: 0.18588375317060957 Loss diff: 3.916910721929456e-05 Rank(Z):  50\n",
      "Zk in border!\n",
      "thres: 0.004126866739898372\n",
      "rank 50\n",
      "Do a regular FW step\n",
      "Iteration: 56 Loss: 0.18593144522482163 Loss diff: 4.769205421206735e-05 Rank(Z):  51\n",
      "Zk in border!\n",
      "thres: 0.00398367894028151\n",
      "rank 51\n",
      "Do a regular FW step\n",
      "Iteration: 57 Loss: 0.1858288075411625 Loss diff: 0.00010263768365914583 Rank(Z):  52\n",
      "Zk in border!\n",
      "thres: 0.003851470722565198\n",
      "rank 52\n",
      "Do a regular FW step\n",
      "Iteration: 58 Loss: 0.18578803213471023 Loss diff: 4.077540645225364e-05 Rank(Z):  53\n",
      "Zk in border!\n",
      "thres: 0.003723377074502765\n",
      "rank 53\n",
      "Do a regular FW step\n",
      "Iteration: 59 Loss: 0.18576037638546344 Loss diff: 2.765574924679104e-05 Rank(Z):  54\n",
      "Zk in border!\n",
      "thres: 0.003604179525693252\n",
      "rank 54\n",
      "Do a regular FW step\n",
      "Iteration: 60 Loss: 0.18566248974117855 Loss diff: 9.788664428489469e-05 Rank(Z):  55\n",
      "Zk in border!\n",
      "thres: 0.0034870299675836547\n",
      "rank 55\n",
      "Do a regular FW step\n",
      "Iteration: 61 Loss: 0.18561155201349241 Loss diff: 5.093772768613447e-05 Rank(Z):  56\n",
      "Zk in border!\n",
      "thres: 0.003375287784986858\n",
      "rank 56\n",
      "Do a regular FW step\n",
      "Iteration: 62 Loss: 0.18562794684007217 Loss diff: 1.6394826579751998e-05 Rank(Z):  57\n",
      "Zk in border!\n",
      "thres: 0.003270062617634073\n",
      "rank 57\n",
      "Do a regular FW step\n",
      "Iteration: 63 Loss: 0.18559949095332973 Loss diff: 2.845588674244004e-05 Rank(Z):  58\n",
      "Zk in border!\n",
      "thres: 0.003168930312840823\n",
      "rank 58\n",
      "Do a regular FW step\n",
      "Iteration: 64 Loss: 0.18562443228224215 Loss diff: 2.494132891242562e-05 Rank(Z):  59\n",
      "Zk in border!\n",
      "thres: 0.0030734090728390395\n",
      "rank 59\n",
      "Do a regular FW step\n",
      "Iteration: 65 Loss: 0.18553218760895832 Loss diff: 9.224467328383668e-05 Rank(Z):  60\n",
      "Zk in border!\n",
      "thres: 0.002981379683091978\n",
      "rank 60\n",
      "Do a regular FW step\n",
      "Iteration: 66 Loss: 0.18549983998831646 Loss diff: 3.2347620641859365e-05 Rank(Z):  61\n",
      "Zk in border!\n",
      "thres: 0.0028941600369931653\n",
      "rank 61\n",
      "Do a regular FW step\n",
      "Iteration: 67 Loss: 0.18545442015876934 Loss diff: 4.541982954711643e-05 Rank(Z):  62\n",
      "Zk in border!\n",
      "thres: 0.002820817050352309\n",
      "rank 62\n",
      "Do a regular FW step\n",
      "Iteration: 68 Loss: 0.1854528948982448 Loss diff: 1.5252605245485995e-06 Rank(Z):  63\n",
      "Zk in border!\n",
      "thres: 0.0027405660320636294\n",
      "rank 63\n",
      "Do a regular FW step\n",
      "Iteration: 69 Loss: 0.18541534282710898 Loss diff: 3.755207113581571e-05 Rank(Z):  64\n",
      "Zk in border!\n",
      "thres: 0.0026632275146499795\n",
      "rank 64\n",
      "Do a regular FW step\n",
      "Iteration: 70 Loss: 0.1854409310032502 Loss diff: 2.5588176141233543e-05 Rank(Z):  65\n",
      "Zk in border!\n",
      "thres: 0.0025897378167879515\n",
      "rank 65\n",
      "Do a regular FW step\n",
      "Iteration: 71 Loss: 0.1854238810306994 Loss diff: 1.7049972550808867e-05 Rank(Z):  66\n",
      "Zk in border!\n",
      "thres: 0.0025202465472031754\n",
      "rank 66\n",
      "Do a regular FW step\n",
      "Iteration: 72 Loss: 0.18547558760450955 Loss diff: 5.170657381015076e-05 Rank(Z):  67\n",
      "Zk in border!\n",
      "thres: 0.00245333430591832\n",
      "rank 67\n",
      "Do a regular FW step\n",
      "Iteration: 73 Loss: 0.18539309566112955 Loss diff: 8.24919433800031e-05 Rank(Z):  68\n",
      "Zk in border!\n",
      "thres: 0.002398520331757581\n",
      "rank 68\n",
      "Do a regular FW step\n",
      "Iteration: 74 Loss: 0.1853685600016312 Loss diff: 2.4535659498337026e-05 Rank(Z):  69\n",
      "Zk in border!\n",
      "thres: 0.0023356241003767986\n",
      "rank 69\n",
      "Do a regular FW step\n",
      "Iteration: 75 Loss: 0.18531303370166619 Loss diff: 5.552629996502523e-05 Rank(Z):  70\n",
      "Zk in border!\n",
      "thres: 0.0022748216127619347\n",
      "rank 70\n",
      "Do a regular FW step\n",
      "Iteration: 76 Loss: 0.18528837017681482 Loss diff: 2.4663524851364915e-05 Rank(Z):  71\n",
      "Zk in border!\n",
      "thres: 0.0022166590115642038\n",
      "rank 71\n",
      "Do a regular FW step\n",
      "Iteration: 77 Loss: 0.18526620692470247 Loss diff: 2.2163252112350484e-05 Rank(Z):  72\n",
      "Zk in border!\n",
      "thres: 0.002160288365326002\n",
      "rank 72\n",
      "Do a regular FW step\n",
      "Iteration: 78 Loss: 0.1852889431208562 Loss diff: 2.2736196153727573e-05 Rank(Z):  73\n",
      "Zk in border!\n",
      "thres: 0.002106385917919895\n",
      "rank 73\n",
      "Do a regular FW step\n",
      "Iteration: 79 Loss: 0.1852538693633889 Loss diff: 3.5073757467307676e-05 Rank(Z):  74\n",
      "Zk in border!\n",
      "thres: 0.0020540558361564765\n",
      "rank 74\n",
      "Do a regular FW step\n",
      "Iteration: 80 Loss: 0.18527750972981283 Loss diff: 2.3640366423938186e-05 Rank(Z):  75\n",
      "Zk in border!\n",
      "thres: 0.002004569089631314\n",
      "rank 75\n",
      "Do a regular FW step\n",
      "Iteration: 81 Loss: 0.18525782873781746 Loss diff: 1.968099199536888e-05 Rank(Z):  76\n",
      "Zk in border!\n",
      "thres: 0.001956959768823485\n",
      "rank 76\n",
      "Do a regular FW step\n",
      "Iteration: 82 Loss: 0.18526930202911038 Loss diff: 1.1473291292918297e-05 Rank(Z):  77\n",
      "Zk in border!\n",
      "thres: 0.0019110308469606485\n",
      "rank 77\n",
      "Do a regular FW step\n",
      "Iteration: 83 Loss: 0.18523342368985618 Loss diff: 3.5878339254197256e-05 Rank(Z):  78\n",
      "Zk in border!\n",
      "thres: 0.001866117858113081\n",
      "rank 78\n",
      "Do a regular FW step\n",
      "Iteration: 84 Loss: 0.1852076986443294 Loss diff: 2.572504552678101e-05 Rank(Z):  79\n",
      "Zk in border!\n",
      "thres: 0.0018338361415012328\n",
      "rank 79\n",
      "Do a regular FW step\n",
      "Iteration: 85 Loss: 0.18517800811178303 Loss diff: 2.9690532546367e-05 Rank(Z):  80\n",
      "Zk in border!\n",
      "thres: 0.0017915818822008545\n",
      "rank 80\n",
      "Do a regular FW step\n",
      "Iteration: 86 Loss: 0.1851756422201965 Loss diff: 2.3658915865309993e-06 Rank(Z):  81\n",
      "Zk in border!\n",
      "thres: 0.0017506700594899227\n",
      "rank 81\n",
      "Do a regular FW step\n",
      "Iteration: 87 Loss: 0.18514133887673653 Loss diff: 3.4303343459968216e-05 Rank(Z):  82\n",
      "Zk in border!\n",
      "thres: 0.001711057263122373\n",
      "rank 82\n",
      "Do a regular FW step\n",
      "Iteration: 88 Loss: 0.1851290418640865 Loss diff: 1.2297012650042127e-05 Rank(Z):  83\n",
      "Zk in border!\n",
      "thres: 0.001673017892944384\n",
      "rank 83\n",
      "Do a regular FW step\n",
      "Iteration: 89 Loss: 0.1851632947817774 Loss diff: 3.425291769090788e-05 Rank(Z):  84\n",
      "Zk in border!\n",
      "thres: 0.0016364499113071806\n",
      "rank 84\n",
      "Do a regular FW step\n",
      "Iteration: 90 Loss: 0.18513438267493243 Loss diff: 2.8912106844969365e-05 Rank(Z):  85\n",
      "Zk in border!\n",
      "thres: 0.0016008294279385993\n",
      "rank 85\n",
      "Do a regular FW step\n",
      "Iteration: 91 Loss: 0.18513049636562814 Loss diff: 3.886309304285129e-06 Rank(Z):  86\n",
      "Zk in border!\n",
      "thres: 0.001567475516884298\n",
      "rank 86\n",
      "Do a regular FW step\n",
      "Iteration: 92 Loss: 0.1851136037611431 Loss diff: 1.6892604485035934e-05 Rank(Z):  87\n",
      "Zk in border!\n",
      "thres: 0.00153443428856459\n",
      "rank 87\n",
      "Do a regular FW step\n",
      "Iteration: 93 Loss: 0.18520544283142382 Loss diff: 9.183907028070815e-05 Rank(Z):  88\n",
      "Zk in border!\n",
      "thres: 0.0015029654227473044\n",
      "rank 88\n",
      "Do a regular FW step\n",
      "Iteration: 94 Loss: 0.18513456933801825 Loss diff: 7.087349340556548e-05 Rank(Z):  89\n",
      "Zk in border!\n",
      "thres: 0.0014716624822960211\n",
      "rank 89\n",
      "Do a regular FW step\n",
      "Iteration: 95 Loss: 0.18509744439400436 Loss diff: 3.712494401389432e-05 Rank(Z):  90\n",
      "Zk in border!\n",
      "thres: 0.0014414036881625902\n",
      "rank 90\n",
      "Do a regular FW step\n",
      "Iteration: 96 Loss: 0.18507347117865944 Loss diff: 2.397321534491903e-05 Rank(Z):  91\n",
      "Zk in border!\n",
      "thres: 0.0014119337278734045\n",
      "rank 91\n",
      "Do a regular FW step\n",
      "Iteration: 97 Loss: 0.18506216406442227 Loss diff: 1.1307114237163463e-05 Rank(Z):  92\n",
      "Zk in border!\n",
      "thres: 0.0013834482062615727\n",
      "rank 92\n",
      "Do a regular FW step\n",
      "Iteration: 98 Loss: 0.18504575749779367 Loss diff: 1.6406566628601738e-05 Rank(Z):  93\n",
      "Zk in border!\n",
      "thres: 0.0013558107852663648\n",
      "rank 93\n",
      "Do a regular FW step\n",
      "Iteration: 99 Loss: 0.18505264488513817 Loss diff: 6.887387344500784e-06 Rank(Z):  94\n",
      "Zk in border!\n",
      "thres: 0.001329064717209394\n",
      "rank 94\n",
      "Do a regular FW step\n",
      "Iteration: 100 Loss: 0.18503983307108934 Loss diff: 1.2811814048829806e-05 Rank(Z):  95\n",
      "Went to lower dim face: 2\n",
      "Stayed: 1\n",
      "Regular: 95\n",
      "Not entered if: 2\n"
     ]
    }
   ],
   "source": [
    "pred_ratings, loss, res_listInFW = FW_inface(X_test, FW_objective_function, gamma1 = 0.05, gamma2 = 0.15, delta = 1, THRES = 10, max_iter = 100, patience = 1e-7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(res_listInFW)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuclear norm: 0.5632052043683061\n",
      "Data shape: (200, 400)\n",
      "Number of observed values: 14942\n",
      "Rank of the matrix: 1\n",
      "Minimum and maximum values: 0.05306863232623509 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n = 400\n",
    "m = 200\n",
    "r = 10\n",
    "rho = 0.10\n",
    "SNR = 5\n",
    "delta = 3.75\n",
    "\n",
    "\n",
    "\n",
    "# taking data\n",
    "U = sparse.random(m, r, density=0.1, format='csr', data_rvs=None)\n",
    "V = sparse.random(r, n, density=0.1, format='csr', data_rvs=None)\n",
    "E = sparse.random(m, n, density=0.1, format='csr', data_rvs=None)\n",
    "\n",
    "\n",
    "\n",
    "VT = V.transpose(copy=True)\n",
    "\n",
    "UVT = U*V\n",
    "#print(UVT. shape)\n",
    "\n",
    "w1 = 1/(sparse.linalg.norm(UVT, ord='fro'))\n",
    "\n",
    "w2 = 1/(SNR*sparse.linalg.norm(E, ord='fro'))\n",
    "\n",
    "#Finally observed data matrix is:\n",
    "X_test = w1*UVT + w2*E\n",
    "\n",
    "# Non zero values\n",
    "idx_ratings = np.argwhere(X_test != 0.)\n",
    "idx_rows = idx_ratings[:,0]\n",
    "idx_cols = idx_ratings[:,1]\n",
    "\n",
    "# Nuclear norm of the test set\n",
    "rank = np.linalg.matrix_rank(X_test)\n",
    "U_thin, s_thin, Vh_thin = sparse.linalg.svds(X_test, k = rank, which='LM')\n",
    "nuc_norm = s_thin.sum()\n",
    "\n",
    "# Print some info about the generated data\n",
    "print('Nuclear norm:', nuc_norm)\n",
    "print('Data shape:', np.shape(X_test))\n",
    "print('Number of observed values:', len(idx_rows))\n",
    "print('Rank of the matrix:', rank)\n",
    "print('Minimum and maximum values:', X_test.max(), X_test.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 20.710085461897688 Loss diff: 20.178013507822605 Rank(Z):  1\n",
      "Iteration: 1 Loss: 8.136005495734203 Loss diff: 12.574079966163485 Rank(Z):  2\n",
      "Iteration: 10 Loss: 0.5841890868123718 Loss diff: 0.5057625973947553 Rank(Z):  11\n",
      "Iteration: 20 Loss: 0.38644839266838615 Loss diff: 0.12540854574466803 Rank(Z):  21\n",
      "Iteration: 30 Loss: 0.27455173635972413 Loss diff: 0.039281464643407704 Rank(Z):  31\n",
      "Iteration: 40 Loss: 0.14399512309815454 Loss diff: 0.02499449833787848 Rank(Z):  41\n",
      "Iteration: 50 Loss: 0.09198025468452456 Loss diff: 0.0028837977552780697 Rank(Z):  51\n",
      "Iteration: 60 Loss: 0.07665822248490584 Loss diff: 0.013324781893819443 Rank(Z):  61\n",
      "Iteration: 70 Loss: 0.07333132593891507 Loss diff: 0.0006173570934196876 Rank(Z):  71\n",
      "Iteration: 80 Loss: 0.052422250273545315 Loss diff: 0.0017050065465505915 Rank(Z):  81\n",
      "Iteration: 90 Loss: 0.04357177196057917 Loss diff: 0.0007945935581365055 Rank(Z):  91\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 750 ms\n"
     ]
    }
   ],
   "source": [
    "%time pred_ratings_reg, loss_reg, loss_track_reg = FrankWolfe(X_test, FW_objective_function, delta = 10, max_iter=100, patience=1e-7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_track)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1\n",
      "Do regular FW step\n",
      "Iteration: 1 Loss: 0.5853282793478831 Loss diff: 49.525331316145206 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 2 Loss: 0.3537519534474485 Loss diff: 0.23157632590043464 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 3 Loss: 0.45288475571449016 Loss diff: 0.09913280226704169 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 4 Loss: 0.4304950862546344 Loss diff: 0.022389669459855766 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 5 Loss: 0.4321174627365453 Loss diff: 0.001622376481910881 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 6 Loss: 0.43225871872437643 Loss diff: 0.000141255987831157 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 7 Loss: 0.43228699437825624 Loss diff: 2.8275653879805684e-05 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 8 Loss: 0.4322951641290038 Loss diff: 8.169750747577531e-06 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 9 Loss: 0.43229810535310204 Loss diff: 2.941224098218509e-06 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 10 Loss: 0.43229933533377773 Loss diff: 1.229980675698794e-06 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 11 Loss: 0.4322999093266066 Loss diff: 5.73992828878378e-07 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 12 Loss: 0.43230020073842723 Loss diff: 2.914118206187588e-07 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 13 Loss: 0.43230035893328034 Loss diff: 1.5819485310597514e-07 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 14 Loss: 0.4323004496315471 Loss diff: 9.069826678587134e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 15 Loss: 0.4323005040504303 Loss diff: 5.441888317747612e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 16 Loss: 0.4323005379821563 Loss diff: 3.393172598453731e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 17 Loss: 0.4323005598492383 Loss diff: 2.1867082033999452e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 18 Loss: 0.43230057435054714 Loss diff: 1.4501308820413783e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 19 Loss: 0.4323005842114248 Loss diff: 9.860877669964907e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 20 Loss: 0.4323005910670745 Loss diff: 6.8556497145877415e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 21 Loss: 0.4323005959283478 Loss diff: 4.861273272815936e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 22 Loss: 0.43230059943691534 Loss diff: 3.5085675476054234e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 23 Loss: 0.43230060200986214 Loss diff: 2.57294680006126e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 24 Loss: 0.4323006039241327 Loss diff: 1.914270575387178e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 25 Loss: 0.432300605367197 Loss diff: 1.4430642791829484e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 26 Loss: 0.4323006064682006 Loss diff: 1.1010036216063668e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 27 Loss: 0.4323006073175455 Loss diff: 8.493448722113328e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 28 Loss: 0.43230060797944825 Loss diff: 6.619027548282475e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 29 Loss: 0.4323006085001447 Loss diff: 5.20696430417189e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 30 Loss: 0.4323006089133422 Loss diff: 4.131975317456238e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 31 Loss: 0.4323006092439 Loss diff: 3.305578033518941e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 32 Loss: 0.4323006095103494 Loss diff: 2.664493625736952e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 33 Loss: 0.4323006097266435 Loss diff: 2.1629414925783408e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 34 Loss: 0.4323006099033866 Loss diff: 1.7674306462822642e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 35 Loss: 0.43230061004870857 Loss diff: 1.453219766744951e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 36 Loss: 0.43230061016889376 Loss diff: 1.20185195129352e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 37 Loss: 0.43230061026883704 Loss diff: 9.994327587747875e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 38 Loss: 0.4323006103523794 Loss diff: 8.354233971274994e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 39 Loss: 0.43230061042255497 Loss diff: 7.017558756317044e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 40 Loss: 0.43230061048177626 Loss diff: 5.922129453495018e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 41 Loss: 0.43230061053197333 Loss diff: 5.0197068723889515e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 42 Loss: 0.43230061057469915 Loss diff: 4.2725822879674524e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 43 Loss: 0.43230061061121033 Loss diff: 3.651118296588152e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 44 Loss: 0.4323006106425288 Loss diff: 3.1318447835104735e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 45 Loss: 0.4323006106694899 Loss diff: 2.696110001920715e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 46 Loss: 0.43230061069277964 Loss diff: 2.3289759010225453e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 47 Loss: 0.4323006107129641 Loss diff: 2.018446521034889e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 48 Loss: 0.4323006107305122 Loss diff: 1.7548074104922762e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 49 Loss: 0.4323006107458143 Loss diff: 1.530209292610607e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 50 Loss: 0.4323006107591959 Loss diff: 1.3381629138109474e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 51 Loss: 0.4323006107709306 Loss diff: 1.1734724303380517e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 52 Loss: 0.4323006107812483 Loss diff: 1.0317691145900199e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 53 Loss: 0.4323006107903432 Loss diff: 9.094891506578051e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 54 Loss: 0.4323006107983797 Loss diff: 8.03651589720289e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 55 Loss: 0.43230061080549775 Loss diff: 7.118028388930497e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 56 Loss: 0.43230061081181664 Loss diff: 6.31888985580531e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 57 Loss: 0.4323006108174382 Loss diff: 5.621558774038249e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 58 Loss: 0.43230061082245 Loss diff: 5.011824288914113e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 59 Loss: 0.4323006108269273 Loss diff: 4.4772519025571e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 60 Loss: 0.43230061083093474 Loss diff: 4.007461029686965e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 61 Loss: 0.4323006108345284 Loss diff: 3.5936809084091692e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 62 Loss: 0.43230061083775717 Loss diff: 3.2287506002148803e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 63 Loss: 0.43230061084066296 Loss diff: 2.9057867223514222e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 64 Loss: 0.43230061084328275 Loss diff: 2.619793271207982e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 65 Loss: 0.4323006108456484 Loss diff: 2.3656632208712836e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 66 Loss: 0.43230061084778815 Loss diff: 2.139732835360064e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 67 Loss: 0.43230061084972643 Loss diff: 1.9382828675418295e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 68 Loss: 0.43230061085148497 Loss diff: 1.7585377598550167e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 69 Loss: 0.43230061085308275 Loss diff: 1.597777465889294e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 70 Loss: 0.4323006108545365 Loss diff: 1.4537815395954112e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 71 Loss: 0.432300610855861 Loss diff: 1.3244960683778118e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 72 Loss: 0.4323006108570694 Loss diff: 1.2083667400020204e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 73 Loss: 0.43230061085817323 Loss diff: 1.1038392422335619e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 74 Loss: 0.4323006108591829 Loss diff: 1.0096923297453486e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 75 Loss: 0.43230061086010757 Loss diff: 9.246492460590616e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 76 Loss: 0.4323006108609554 Loss diff: 8.478218127550008e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 77 Loss: 0.43230061086173355 Loss diff: 7.781553179597722e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 78 Loss: 0.4323006108624488 Loss diff: 7.152611836147571e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 79 Loss: 0.4323006108631068 Loss diff: 6.57973675544099e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 80 Loss: 0.43230061086371274 Loss diff: 6.059597268404104e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 81 Loss: 0.43230061086427146 Loss diff: 5.5871973714261e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 82 Loss: 0.432300610864787 Loss diff: 5.155320614846914e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 83 Loss: 0.43230061086526334 Loss diff: 4.763411887154234e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 84 Loss: 0.43230061086570387 Loss diff: 4.405364961712621e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 85 Loss: 0.43230061086611155 Loss diff: 4.076738946423575e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 86 Loss: 0.4323006108664892 Loss diff: 3.77642361826247e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 87 Loss: 0.43230061086683946 Loss diff: 3.502753642692369e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 88 Loss: 0.43230061086716454 Loss diff: 3.2507330161024584e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 89 Loss: 0.43230061086746646 Loss diff: 3.019251515468113e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 90 Loss: 0.43230061086774707 Loss diff: 2.806088694740083e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 91 Loss: 0.4323006108680082 Loss diff: 2.611244553918368e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 92 Loss: 0.43230061086825144 Loss diff: 2.432498646953718e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 93 Loss: 0.4323006108684781 Loss diff: 2.266520304772257e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 94 Loss: 0.43230061086868943 Loss diff: 2.1133095273739855e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 95 Loss: 0.4323006108688867 Loss diff: 1.9728663147589032e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 96 Loss: 0.432300610869071 Loss diff: 1.8429702208777599e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 97 Loss: 0.4323006108692432 Loss diff: 1.7219559111936178e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 98 Loss: 0.4323006108694043 Loss diff: 1.6109336087311021e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 99 Loss: 0.43230061086955507 Loss diff: 1.5076828674409626e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 100 Loss: 0.4323006108696963 Loss diff: 1.412203687323199e-13 Rank(Z):  1\n",
      "CPU times: total: 5.05 s\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%time pred_ratings_inface, loss_inface, loss_track_inface = FW_inface(X_test, FW_objective_function, gamma1 = 0.25, gamma2 = 0.5, delta = 1, THRES = 10, max_iter = 100, patience = 1e-7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_track_inface)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwQyN3Cy_uF5"
   },
   "source": [
    "# Tuned Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_track_reg, label = 'Frank Wolfe')\n",
    "plt.plot(loss_track_inface, label = 'In-Face Frank Wolfe')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0manG1buepT"
   },
   "source": [
    "# Grid Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_MUb9nrwJyA"
   },
   "source": [
    "##Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUZGgSUsudah",
    "outputId": "bd7fb5f1-3a5b-4467-e5cc-23f3784569fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 0.001    ------     Final Loss: 1995299.6906607011   at iteration 2\n",
      "Delta: 0.01    ------     Final Loss: 1995296.9066038157   at iteration 2\n",
      "Delta: 0.1    ------     Final Loss: 1995269.065718345   at iteration 2\n",
      "Delta: 1    ------     Final Loss: 1994990.6592616318   at iteration 4\n",
      "Delta: 10    ------     Final Loss: 1992207.8429168023   at iteration 19\n",
      "Delta: 100    ------     Final Loss: 1964496.978598407   at iteration 84\n",
      "Delta: 1000    ------     Final Loss: 1699162.6623283497   at iteration 201\n",
      "Delta: 10000    ------     Final Loss: 302150.77467486466   at iteration 64\n",
      "Delta: 100000    ------     Final Loss: 175474.63085276185   at iteration 70\n",
      "Delta: 1000000    ------     Final Loss: 146671.28662048402   at iteration 201\n",
      "Delta: 10000000    ------     Final Loss: 672843.1813172555   at iteration 201\n",
      "Delta: 100000000    ------     Final Loss: 1757112480.4694064   at iteration 201\n"
     ]
    }
   ],
   "source": [
    "deltas = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter=201, patience = 0.01, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--BkRQqIxmSw",
    "outputId": "f9c17983-9fb7-40c8-dc83-c9d8c099cf3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 250000    ------     Final Loss: 184966.58801373874   at iteration 142\n",
      "Delta: 500000    ------     Final Loss: 169552.51866399613   at iteration 201\n",
      "Delta: 750000    ------     Final Loss: 420615.6132565984   at iteration 201\n",
      "Delta: 1250000    ------     Final Loss: 216412.31880627264   at iteration 201\n",
      "Delta: 1500000    ------     Final Loss: 273680.42343568715   at iteration 201\n",
      "Delta: 2500000    ------     Final Loss: 417712.4612960932   at iteration 201\n",
      "Delta: 5000000    ------     Final Loss: 571545.4612269908   at iteration 201\n",
      "Delta: 7500000    ------     Final Loss: 632556.1542439449   at iteration 201\n"
     ]
    }
   ],
   "source": [
    "deltas = [250000,\n",
    "          500000,\n",
    "          750000,\n",
    "          1250000,\n",
    "          1500000,\n",
    "          2500000,\n",
    "          5000000,\n",
    "          7500000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter=201, patience = 0.01, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5Xei-9Gy3hh",
    "outputId": "47a9fd59-b0a5-4bca-ab08-992677a90c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 150000    ------     Final Loss: 180502.69121015765   at iteration 95\n",
      "Delta: 300000    ------     Final Loss: 186151.14181067253   at iteration 165\n",
      "Delta: 450000    ------     Final Loss: 188191.3045520871   at iteration 231\n",
      "Delta: 600000    ------     Final Loss: 189243.6150128576   at iteration 297\n",
      "Delta: 900000    ------     Final Loss: 190318.32554160611   at iteration 425\n",
      "Delta: 1200000    ------     Final Loss: 189679.94593613478   at iteration 501\n",
      "Delta: 1500000    ------     Final Loss: 149600.4326080835   at iteration 501\n"
     ]
    }
   ],
   "source": [
    "deltas = [150000,\n",
    "          300000,\n",
    "          450000,\n",
    "          600000,\n",
    "          900000,\n",
    "          1200000,\n",
    "          1500000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter=501, patience = 0.01, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2W5PqtaH0eZu",
    "outputId": "c519e67f-c98e-4186-a5a2-8d0d6a031872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 1750000    ------     Final Loss: 191384.43819455765   at iteration 786\n",
      "Delta: 2000000    ------     Final Loss: 191527.2106760225   at iteration 889\n",
      "Delta: 2250000    ------     Final Loss: 191638.53051798942   at iteration 991\n"
     ]
    }
   ],
   "source": [
    "deltas = [1750000,\n",
    "          2000000,\n",
    "          2250000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter = 1000, patience = 0.001, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkpvJqQi2YxJ",
    "outputId": "c4eeadcf-f4a5-4873-94aa-33a40b766bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 1375000    ------     Final Loss: 191074.29727334835   at iteration 630\n",
      "Delta: 1500000    ------     Final Loss: 191194.6863524153   at iteration 682\n",
      "Delta: 1625000    ------     Final Loss: 191296.77299220837   at iteration 734\n"
     ]
    }
   ],
   "source": [
    "deltas = [1375000,\n",
    "          1500000,\n",
    "          1625000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter = 1000, patience = 0.001, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO5uv6UZ33wJ"
   },
   "source": [
    "## Gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45FR0xoX323p",
    "outputId": "51369768-d4dc-4455-f573-83822546ef54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma1: 0   Gamma2: 0.1    ------     Final Loss: 335449.4737096828   at iteration 500\n",
      "Gamma1: 0   Gamma2: 1    ------     Final Loss: 335449.4737096833   at iteration 500\n",
      "Gamma1: 0.1   Gamma2: 1    ------     Final Loss: 149460.45148459333   at iteration 500\n",
      "Gamma1: 1   Gamma2: 1    ------     Final Loss: 149460.451484515   at iteration 500\n",
      "Gamma1: 1   Gamma2: 10    ------     Final Loss: 335449.47370968235   at iteration 500\n",
      "Gamma1: 1   Gamma2: 100    ------     Final Loss: 149460.4514845148   at iteration 500\n",
      "Gamma1: 10   Gamma2: 100    ------     Final Loss: 149460.451484515   at iteration 500\n"
     ]
    }
   ],
   "source": [
    "gammas = [[0,    0.1],\n",
    "          [0,    1],\n",
    "          [0.1,  1],\n",
    "          [1,    1],\n",
    "          [1,   10],\n",
    "          [1,  100],\n",
    "          [10, 100]]\n",
    "\n",
    "for gamma1, gamma2 in gammas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, gamma1 = gamma1, gamma2 = gamma2, delta = 1500000, max_iter = 500, patience = 0.001, printing = False)\n",
    "  print('Gamma1: ' + str(gamma1) + '   Gamma2: ' + str(gamma2) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOD4CGHO884Z",
    "outputId": "f4ddeb2d-b605-4c9c-fab8-d096648bc3e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma1: 1   Gamma2: 100    ------     Final Loss: 149460.45148459318   at iteration 500\n",
      "Gamma1: 100   Gamma2: 100    ------     Final Loss: 149460.45148451466   at iteration 500\n",
      "Gamma1: 0.1   Gamma2: 100    ------     Final Loss: 149460.4514845149   at iteration 500\n",
      "Gamma1: 1   Gamma2: 1000    ------     Final Loss: 335449.4737115356   at iteration 500\n"
     ]
    }
   ],
   "source": [
    "gammas = [[1,  100],\n",
    "          [100, 100],\n",
    "          [0.1,  100],\n",
    "          [1,    1000]]\n",
    "\n",
    "for gamma1, gamma2 in gammas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, gamma1 = gamma1, gamma2 = gamma2, delta = 1500000, max_iter = 500, patience = 0.001, printing = False)\n",
    "  print('Gamma1: ' + str(gamma1) + '   Gamma2: ' + str(gamma2) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMpXvq5S-ZvP",
    "outputId": "ac623233-b516-433e-a7f7-041981b1ed94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma1: 0   Gamma2: 100    ------     Final Loss: 335449.47371153446   at iteration 500\n",
      "Gamma1: 0   Gamma2: 1000    ------     Final Loss: 149460.4514845933   at iteration 500\n",
      "Gamma1: 0.1   Gamma2: 0.1    ------     Final Loss: 149460.45148451495   at iteration 500\n"
     ]
    }
   ],
   "source": [
    "gammas = [[0,  100],\n",
    "          [0, 1000],\n",
    "          [0.1,  0.1]]\n",
    "\n",
    "for gamma1, gamma2 in gammas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, gamma1 = gamma1, gamma2 = gamma2, delta = 1500000, max_iter = 500, patience = 0.001, printing = False)\n",
    "  print('Gamma1: ' + str(gamma1) + '   Gamma2: ' + str(gamma2) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpS5jRz4vpHa"
   },
   "source": [
    "## Sub-Chapter"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FW_GoodReads_recommender - Copia.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}