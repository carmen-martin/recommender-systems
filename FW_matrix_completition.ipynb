{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Imports\n",
    "from scipy import sparse\n",
    "import scipy\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from numpy import linalg as LA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Frank-Wolfe - standard algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kobsOmgimqL_",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FW objective function\n",
    "def FW_objective_function(diff_vec):\n",
    "    return 0.5*(np.power(diff_vec,2).sum())\n",
    "\n",
    "# Regular FW algorithm\n",
    "def FrankWolfe(X, objective_function, delta, empties = 0, printing_res = True, Z_init = None, max_iter = 150, patience = 1e-3):\n",
    "    '''\n",
    "    :param X: sparse matrix with ratings and 'empty values', rows - users, columns - books.\n",
    "    :param objective_function: objective function that we would like to minimize with FW\n",
    "    :param delta: Radius of the feasible's set ball\n",
    "    :param empties (optional): Empty values of X are zeros (0) or NaN ('nan'). Default = 0\n",
    "    :param Z_init (optional): In case we want to initialize Z with a known matrix, if not given Z_init will be a zeros matrix. Default = None.\n",
    "    :param max_iter (optional): max number of iterations for the method. Default = 150.\n",
    "    :param patience (optional): once reached this tolerance provide the result. Default = 1e-3.\n",
    "    :return: Z: matrix of predicted ratings - it should be like X but with no 'empty values'\n",
    "    :return: accuracy: difference between original values (X) and predicted ones (Z)\n",
    "    '''\n",
    "\n",
    "    # Get X indexes for not empty values\n",
    "    if empties == 0:\n",
    "        idx_ratings = np.argwhere(X != 0)\n",
    "    elif empties == 'nan':\n",
    "        idx_ratings = np.argwhere(~np.isnan(X))\n",
    "    else:\n",
    "        return print('Error: Empties argument', empties, 'not valid.')\n",
    "\n",
    "    idx_rows = idx_ratings[:,0]\n",
    "    idx_cols = idx_ratings[:,1]\n",
    "\n",
    "    # Initialize Z\n",
    "    if Z_init == 'random uniform':\n",
    "        Z = np.random.uniform(low = 0.01, high = 1, size = X.shape)\n",
    "    elif Z_init is not None:\n",
    "        Z = Z_init\n",
    "    else:\n",
    "        Z = np.zeros(X.shape)\n",
    "\n",
    "    # Create vectors with the not empty features of the sparse matrix\n",
    "    X_rated = X[idx_rows, idx_cols]\n",
    "    Z_rated = Z[idx_rows, idx_cols]\n",
    "    diff_vec = np.array(Z_rated - X_rated)[0]\n",
    "\n",
    "    # Create needed variables\n",
    "    res_list = []\n",
    "    diff_loss = patience + 1\n",
    "    loss = objective_function(diff_vec)\n",
    "    it = 0\n",
    "    while (diff_loss > patience) and (it < max_iter):\n",
    "\n",
    "        # Gradient\n",
    "        grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "\n",
    "        # SVD - Compute k = 1 singular values and its vectors, starting from the largest (which = 'LM')\n",
    "        u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')   #\n",
    "\n",
    "        # Update\n",
    "        Zk_tilde = -delta*np.outer(u_max,v_max)\n",
    "\n",
    "        alpha_k = 2/(it+2) #alpha - as studied in class\n",
    "        Z = (1-alpha_k)*Z + alpha_k*Zk_tilde\n",
    "\n",
    "        # Loss\n",
    "        diff_vec = np.array(Z[idx_rows, idx_cols] - X_rated)[0]\n",
    "        new_loss = objective_function(diff_vec)\n",
    "\n",
    "        # Improvement at this iteration\n",
    "        diff_loss = np.abs(loss - new_loss)\n",
    "        loss = new_loss\n",
    "\n",
    "        if printing_res == True:\n",
    "            if it == 1 or it % 10 == 0:\n",
    "                print('Iteration:', it, 'Loss:', loss, 'Loss diff:', diff_loss, 'Rank(Z): ', np.linalg.matrix_rank(Z))\n",
    "\n",
    "        # Count iteration\n",
    "        it += 1\n",
    "\n",
    "        res_list.append(loss)\n",
    "        \n",
    "    return Z, loss, res_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y-VOlLMvfq8"
   },
   "source": [
    "# Frank-Wolfe In-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "time_out = time.process_time() + 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Binary search for alpha stop\n",
    "def alpha_binary_search(Zk, Dk, delta, max_value = 1, min_value = 0, tol = 0.3):\n",
    "\n",
    "    #Inizialization\n",
    "\n",
    "    best_alpha = (max_value - min_value) / 2\n",
    "\n",
    "    testing_matrix = Zk + best_alpha * Dk\n",
    "\n",
    "    sentinel = False\n",
    "\n",
    "    while time.process_time() <= time_out:\n",
    "\n",
    "      testing_mat_nuclear_norm = LA.norm(testing_matrix, ord = 'nuc')\n",
    "\n",
    "      sentinel = True\n",
    "\n",
    "    #Binary Search\n",
    "\n",
    "    if sentinel == True:\n",
    "\n",
    "      while testing_mat_nuclear_norm <= delta and (max_value - min_value) >= tol and time.process_time() <= time_out:\n",
    "\n",
    "          min_value = best_alpha\n",
    "\n",
    "          best_alpha = (max_value - min_value) / 2\n",
    "\n",
    "          testing_matrix = Zk + best_alpha * Dk\n",
    "\n",
    "          testing_mat_nuclear_norm = LA.norm(testing_matrix, ord = 'nuc')\n",
    "\n",
    "    return best_alpha\n",
    "\n",
    "def FW_inface(X, objective_function, delta, L=1, D = None, gamma1 = 0, gamma2 = 1, THRES = 0.001, empties = 0, max_iter = 150, patience = 1e-3, printing = True):\n",
    "    '''\n",
    "    :param X: sparse matrix with ratings and 'empty values', rows - users, columns - books.\n",
    "    :param objective_function: objective function that we would like to minimize with FW.\n",
    "    :param delta: Radius of the feasible's set ball\n",
    "    :param L: must be greater than 1\n",
    "    :param D: if not inputed = 2*delta\n",
    "    :param gamma1:\n",
    "    :param gamma2:\n",
    "    :param THRES:\n",
    "    :param empties (optional): Empty values of X are zeros (0) or NaN ('nan'). Default = 0\n",
    "    :param max_iter: max number of iterations for the method.\n",
    "    :param patience: once reached this tolerance provide the result.\n",
    "    :return: Z: matrix of predicted ratings - it should be like X but with no 'empty values'\n",
    "            loss: difference between original values (X) and predicted ones (Z).\n",
    "    '''\n",
    "\n",
    "    # Get X indexes for not empty values\n",
    "    if empties == 0:\n",
    "        idx_ratings = np.argwhere(X != 0)\n",
    "    elif empties == 'nan':\n",
    "        idx_ratings = np.argwhere(~np.isnan(X))\n",
    "    else:\n",
    "        return print('Empties argument', empties, 'not valid.')\n",
    "    idx_rows = idx_ratings[:,0]\n",
    "    idx_cols = idx_ratings[:,1]\n",
    "\n",
    "    # Initialize Z_{-1}\n",
    "    Z = np.zeros(X.shape)\n",
    "\n",
    "    # Create vectors with the not empty features of the sparse matrix\n",
    "    X_rated = X[idx_rows, idx_cols]\n",
    "    Z_rated = Z[idx_rows, idx_cols]\n",
    "    diff_vec = np.array(Z_rated - X_rated)[0]\n",
    "\n",
    "    # Initial gradient and Z0\n",
    "    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "    u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "    Z = -delta*np.outer(u_max,v_max)\n",
    "    Z_rated = Z[idx_rows, idx_cols]\n",
    "\n",
    "    # Initialize lower bound on the optimal objective function (f*)\n",
    "    diff_vec = np.array(Z_rated - X_rated)[0]\n",
    "    new_low_bound = np.max((objective_function(diff_vec) + np.multiply(diff_vec,Z_rated)),0)\n",
    "\n",
    "    # Set D constant\n",
    "    D = 2*delta\n",
    "\n",
    "    # Compute thin SVD for Z0\n",
    "    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "    r_grad = np.linalg.matrix_rank(grad)   # Compute rank of the gradient sparse matrix to find thin SVD size\n",
    "    print('rank', r_grad)\n",
    "    U_thin, D_thin, Vh_thin = sparse.linalg.svds(grad, k = r_grad, which = 'LM')   # Compute k = rank singular values\n",
    "\n",
    "    # Additional needed parameters\n",
    "    res_list = []\n",
    "    diff_loss = patience + 1\n",
    "    loss = objective_function(diff_vec)\n",
    "    it = 0\n",
    "    while (loss > patience) and (it < max_iter):\n",
    "\n",
    "        # Lower bound update\n",
    "        low_bound = new_low_bound\n",
    "\n",
    "        # In-face direction with the away step strategy: two calculations depending of where Z lies within the feasible set\n",
    "        if D_thin.sum() == delta: # Z in border (sum of singular values == radious of feasible set)\n",
    "            G = 0.5*(Vh_thin.dot(grad.T.dot(U_thin)) + U_thin.T.dot(grad.dot(Vh_thin.T)))\n",
    "            u = sparse.linalg.eigs(G, k = 1, which = 'SM')#unitary eigenvector corresponding to smallest eigenvalue of G\n",
    "            M = np.outer(u,u)\n",
    "            Zk_tilde = delta*U_thin.dot(M.dot(Vh_thin))\n",
    "            update_direction = Z - Zk_tilde\n",
    "            alpha_B = scipy.linalg.inv(delta*u.T.dot(scipy.linalg.inv(update_direction).dot(u))-1)\n",
    "            print('Zk in border!')\n",
    "\n",
    "        else: #inside\n",
    "            idx_max_s = np.argmax(D_thin)\n",
    "            Zk_tilde = delta*np.outer(U_thin[idx_max_s,:],Vh_thin[:,idx_max_s]) # are this the correct row and column?\n",
    "            update_direction = Z - Zk_tilde\n",
    "            #BINARY SEARCH to find alpha stop\n",
    "            alpha_B = alpha_binary_search(Z,\n",
    "                                          update_direction,\n",
    "                                          delta)\n",
    "\n",
    "        nuclear_norm = D_thin.sum()\n",
    "\n",
    "        if abs(delta - nuclear_norm) < THRES and r_grad > 1:\n",
    "            Z_B = Z + alpha_B*update_direction\n",
    "            diff_vec_B = Z_B[idx_rows, idx_cols] - X_rated\n",
    "\n",
    "            if 1/(objective_function(diff_vec_B)-low_bound) >= (1/(loss-low_bound)+gamma1/(2*L*D**2)):\n",
    "              # 1. Move to a lower dimensional face\n",
    "              print('Went to a lower-dimensional face')\n",
    "              Z = Z_B\n",
    "\n",
    "            else:\n",
    "                beta = 0.5 # FIND A GOOD VALUE -- a binary search is also suggested by the paper xdd\n",
    "                Z_A = Z + beta*update_direction\n",
    "                diff_vec_A = Z_A[idx_rows, idx_cols] - X_rated\n",
    "                if 1/(objective_function(diff_vec_A)-low_bound) >= (1/(loss-low_bound)+gamma2/(2*L*D**2)):\n",
    "                    # 2. Stay in the current face\n",
    "                    print('Stayed in the current face')\n",
    "                    Z = Z_A\n",
    "                else:\n",
    "                    # 3. Do a regular FW step and update the lower bound\n",
    "                    print('Do a regular FW step')\n",
    "                    #Zk update\n",
    "                    idx_max_s = np.argmax(D_thin)\n",
    "                    update_Z = -delta*np.outer(U_thin[idx_max_s,:],Vh_thin[:,idx_max_s]) # Am i selecting right the vectors??\n",
    "                    alpha_k = 2/(it+2)\n",
    "                    Z = (1-alpha_k)*Z + alpha_k*update_Z\n",
    "\n",
    "                    # Lower bound update\n",
    "                    direction_vec = update_Z.flatten() - Z.flatten()\n",
    "\n",
    "                    grad = grad.toarray() # this method converts the sparse matrix into a numpy array!\n",
    "\n",
    "                    wolfe_gap = grad.T.flatten() * direction_vec #added the flatten otherwise you can't do the operation\n",
    "                    B_w = loss + wolfe_gap.sum()\n",
    "                    #new_low_bound = np.max(low_bound, B_w)   # gave problems during the execution: wanted both numbers as integers??\n",
    "\n",
    "                    ''' TRIED THIS INSTEAD'''\n",
    "                    if low_bound >= B_w:\n",
    "                      new_low_bound = low_bound\n",
    "                    else:\n",
    "                      new_low_bound = B_w\n",
    "                    ''' '''\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            # 3. Do a regular FW step and update the lower bound\n",
    "            print('Do regular FW step')\n",
    "            #Zk update\n",
    "            idx_max_s = np.argmax(D_thin)\n",
    "            update_Z = -delta*np.outer(U_thin[idx_max_s,:],Vh_thin[:,idx_max_s]) # Am i selecting right the vectors??\n",
    "            alpha_k = 2/(it+2)\n",
    "            Z = (1-alpha_k)*Z + alpha_k*update_Z\n",
    "\n",
    "            # Lower bound update\n",
    "            direction_vec = update_Z.flatten() - Z.flatten()\n",
    "\n",
    "            grad = grad.toarray() # this method converts the sparse matrix into a numpy array!\n",
    "\n",
    "            wolfe_gap = grad.T.flatten() * direction_vec #added the flatten otherwise you can't do the operation\n",
    "            B_w = loss + wolfe_gap.sum()\n",
    "            #new_low_bound = np.max(low_bound, B_w)   # gave problems during the execution: wanted both numbers as integers??\n",
    "\n",
    "            ''' TRIED THIS INSTEAD'''\n",
    "            if low_bound >= B_w:\n",
    "              new_low_bound = low_bound\n",
    "            else:\n",
    "              new_low_bound = B_w\n",
    "            ''' '''\n",
    "\n",
    "        # Loss\n",
    "        diff_vec = np.array(Z[idx_rows, idx_cols] - X_rated)[0]\n",
    "        new_loss = objective_function(diff_vec)\n",
    "\n",
    "        # Improvement at this iteration\n",
    "        diff_loss = np.abs(loss - new_loss)\n",
    "        loss = new_loss\n",
    "\n",
    "        # Gradient\n",
    "        grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "\n",
    "        # Thin SVD\n",
    "        r_grad = np.linalg.matrix_rank(grad)   # Compute rank of the gradient sparse matrix to find thin SVD size\n",
    "        U_thin, D_thin, Vh_thin = sparse.linalg.svds(grad, k = r_grad, which='LM')   # Compute k = rank singular values # replaced r_grad with 1\n",
    "\n",
    "        # Count iteration\n",
    "        it += 1\n",
    "\n",
    "        if printing == True:\n",
    "          if it % 1 == 0 or it == 1:\n",
    "            print('Iteration:', it, 'Loss:', loss, 'Loss diff:', diff_loss, 'Rank(Z): ', np.linalg.matrix_rank(Z))\n",
    "\n",
    "        res_list.append(loss)\n",
    "\n",
    "    return Z, loss, res_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def FW_inface(X, objective_function, delta, L = 1, D = None, gamma1 = 0, gamma2 = 1, THRES = 0.001, empties = 0, max_iter=150, patience=1e-3, printing = True):\n",
    "    '''\n",
    "    :param X: sparse matrix with ratings and 'empty values', rows - users, columns - books.\n",
    "    :param objective_function: objective function that we would like to minimize with FW.\n",
    "    :param delta: Radius of the feasible's set ball\n",
    "    :param L: must be greater than 1\n",
    "    :param D: if not inputed = 2*delta\n",
    "    :param gamma1:\n",
    "    :param gamma2:\n",
    "    :param THRES:\n",
    "    :param empties (optional): Empty values of X are zeros (0) or NaN ('nan'). Default = 0\n",
    "    :param max_iter: max number of iterations for the method.\n",
    "    :param patience: once reached this tolerance provide the result.\n",
    "    :return: Z: matrix of predicted ratings - it should be like X but with no 'empty values'\n",
    "            loss: difference between original values (X) and predicted ones (Z).\n",
    "    '''\n",
    "\n",
    "    # Get X indexes for not empty values\n",
    "    if empties == 0:\n",
    "        idx_ratings = np.argwhere(X != 0)\n",
    "    elif empties == 'nan':\n",
    "        idx_ratings = np.argwhere(~np.isnan(X))\n",
    "    else:\n",
    "        return print('Empties argument', empties, 'not valid.')\n",
    "\n",
    "    idx_rows = idx_ratings[:,0]\n",
    "    idx_cols = idx_ratings[:,1]\n",
    "\n",
    "    # Initialize Z_{-1}\n",
    "    Z0 = np.zeros(X.shape)\n",
    "\n",
    "    # Create vectors with the not empty features of the sparse matrix\n",
    "    X_rated = X[idx_rows, idx_cols]\n",
    "    Z_rated = Z0[idx_rows, idx_cols]\n",
    "    diff_vec = np.array(Z_rated - X_rated)[0]\n",
    "\n",
    "    # Initial gradient and Z0\n",
    "    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "    u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "    Zk = -delta*np.outer(u_max,v_max)\n",
    "    Z_rated = Zk[idx_rows, idx_cols]\n",
    "\n",
    "    print('Initial Zk Rank: ', LA.matrix_rank(Zk))\n",
    "\n",
    "    # Initialize lower bound on the optimal objective function (f*)\n",
    "    diff_vec = np.array(Z_rated - X_rated)[0]\n",
    "    new_low_bound = np.max((objective_function(diff_vec) + np.multiply(diff_vec,Z_rated)), 0).sum()\n",
    "    #new_low_bound = 0 #used 0 otherwise the other new_low_bound was too high!\n",
    "\n",
    "    # Set D\n",
    "    if D is not None:\n",
    "        D = D\n",
    "    else:\n",
    "        D = 2*delta\n",
    "\n",
    "    rank_Z = LA.matrix_rank(Zk)   # rank of Zk to find thin SVD size\n",
    "\n",
    "    # Additional needed parameters\n",
    "    diff_loss = patience + 1\n",
    "    loss = objective_function(diff_vec)\n",
    "    it = 0\n",
    "    res_list = [loss]\n",
    "\n",
    "    B_used = 0\n",
    "    A_used = 0\n",
    "    not_entered = 0\n",
    "    regularFW = 0\n",
    "\n",
    "    while (diff_loss > patience) and (it < max_iter):\n",
    "\n",
    "        # Thin SVD\n",
    "        U_thin, s_thin, Vh_thin = sparse.linalg.svds(Zk, k = rank_Z, which='LM')   # Compute k = rank singular values\n",
    "\n",
    "        # Lower bound update\n",
    "        low_bound = new_low_bound\n",
    "\n",
    "        # In-face direction with the away step strategy: two calculations depending of where Z lies within the feasible set\n",
    "        if s_thin.sum() <= delta: # Z in border (sum of singular values == radius of feasible set)\n",
    "            print('Zk in border!')\n",
    "            G = 0.5 * (Vh_thin.dot(grad.T.dot(U_thin)) + U_thin.T.dot(grad.dot(Vh_thin.T)))\n",
    "            # Obtain unitary eigenvector corresponding to smallest eigenvalue of G\n",
    "            #lamb, u = sparse.linalg.eigs(G, k = 1, which = 'SM')\n",
    "            eigvalues, eigvectors = LA.eig(G)  #find the eigenvalues\n",
    "            min_eig = np.argmin(eigvalues)  #find the index of the smallest eigenvalue\n",
    "            u = eigvectors[:, min_eig]  #take the eigenvector corresponding to the smallest eigenvalue\n",
    "\n",
    "            # Update value\n",
    "            M = np.outer(u,u.T)\n",
    "            Zk_tilde = delta*U_thin.dot(M.dot(Vh_thin))\n",
    "            Dk = Zk - Zk_tilde\n",
    "            #alpha_B = 0.5\n",
    "\n",
    "            inv_s_thin = np.diag(1/s_thin)\n",
    "            alpha_B = 1/(delta*u.T.dot(inv_s_thin).dot(u)-1)\n",
    "\n",
    "        else: #inside\n",
    "            grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "            u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "            Zk_tilde = delta*np.outer(u_max,v_max)\n",
    "            Dk = Zk - Zk_tilde\n",
    "            #BINARY SEARCH\n",
    "            '''\n",
    "            alpha_B = alpha_binary_search(Zk,\n",
    "                                          Dk,\n",
    "                                          delta)\n",
    "            '''\n",
    "            alpha_B = 0.5\n",
    "\n",
    "        nuclear_norm = s_thin.sum()\n",
    "        print('thres:', abs(delta - nuclear_norm))\n",
    "        print('rank', rank_Z)\n",
    "        if abs(delta - nuclear_norm) < THRES and rank_Z > 1:\n",
    "            Z_B = Zk + alpha_B*Dk\n",
    "            diff_vec_B = Z_B[idx_rows, idx_cols] - X_rated\n",
    "\n",
    "            if 1/(objective_function(diff_vec_B)-low_bound) >= (1/(loss-low_bound)+gamma1/(2*L*D**2)):\n",
    "              # 1. Move to a lower dimensional face\n",
    "              print('Went to a lower-dimensional face')\n",
    "              B_used += 1\n",
    "              Zk = Z_B\n",
    "\n",
    "            else:\n",
    "                beta = alpha_B/5 # FIND A GOOD VALUE -- a binary search is also suggested by the paper\n",
    "                Z_A = Zk + beta*Dk\n",
    "                diff_vec_A = Z_A[idx_rows, idx_cols] - X_rated\n",
    "                if 1/(objective_function(diff_vec_A)-low_bound) >= (1/(loss-low_bound)+gamma2/(2*L*D**2)):\n",
    "                    # 2. Stay in the current face\n",
    "                    print('Stayed in the current face')\n",
    "                    A_used += 1\n",
    "                    Zk = Z_A\n",
    "                else:\n",
    "                    # 3. Do a regular FW step and update the lower bound\n",
    "                    print('Do a regular FW step')\n",
    "                    regularFW += 1\n",
    "                    #Zk update\n",
    "                    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "                    u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "                    Zk_tilde = -delta*np.outer(u_max,v_max)\n",
    "\n",
    "                    '''# Lower bound update\n",
    "                    B_w = loss + grad.T.dot(Zk_tilde - Zk)\n",
    "                    print(B_w, low_bound)\n",
    "                    new_low_bound = np.max(low_bound, B_w)'''\n",
    "\n",
    "                    direction_vec = Zk_tilde.flatten() - Zk.flatten()\n",
    "                    grad = grad.toarray()\n",
    "                    wolfe_gap = grad.T.flatten() * direction_vec\n",
    "                    B_w = loss + wolfe_gap.sum()\n",
    "\n",
    "                    #TRIED THIS INSTEAD\n",
    "                    if low_bound >= B_w:\n",
    "                      new_low_bound = low_bound\n",
    "                    else:\n",
    "                      new_low_bound = B_w\n",
    "\n",
    "                    # Z_(k+1)\n",
    "                    alpha_k = 2/(it+2)\n",
    "                    Zk = (1-alpha_k)*Zk + alpha_k*Zk_tilde\n",
    "\n",
    "        else:\n",
    "\n",
    "            # 3. Do a regular FW step and update the lower bound\n",
    "            print('Do a regular FW step - not entered initial if')\n",
    "            not_entered += 1\n",
    "            #Zk update\n",
    "            grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "            u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "            Zk_tilde = -delta*np.outer(u_max,v_max)\n",
    "\n",
    "            '''# Lower bound update\n",
    "            B_w = loss + grad.T.dot(Zk_tilde - Zk)\n",
    "            print(B_w, low_bound)\n",
    "            new_low_bound = np.max(low_bound, B_w)'''\n",
    "\n",
    "            direction_vec = Zk_tilde.flatten() - Zk.flatten()\n",
    "            grad = grad.toarray()\n",
    "            wolfe_gap = grad.T.flatten() * direction_vec\n",
    "            B_w = loss + wolfe_gap.sum()\n",
    "\n",
    "            #TRIED THIS INSTEAD\n",
    "            if low_bound >= B_w:\n",
    "                new_low_bound = low_bound\n",
    "            else:\n",
    "                new_low_bound = B_w\n",
    "\n",
    "            # Z_(k+1)\n",
    "            alpha_k = 2/(it+2)\n",
    "            Zk = (1-alpha_k)*Zk + alpha_k*Zk_tilde\n",
    "\n",
    "        # Loss\n",
    "        diff_vec = np.array(Zk[idx_rows, idx_cols] - X_rated)[0]\n",
    "        new_loss = objective_function(diff_vec)\n",
    "\n",
    "        # Improvement at this iteration\n",
    "        diff_loss = np.abs(loss - new_loss)\n",
    "        loss = new_loss\n",
    "\n",
    "        rank_Z = LA.matrix_rank(Zk)   # rank of Zk to find thin SVD size\n",
    "\n",
    "        # Gradient\n",
    "        #grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "\n",
    "        # Count iteration\n",
    "        it += 1\n",
    "\n",
    "        res_list.append(loss)\n",
    "\n",
    "        if printing == True:\n",
    "          if it % 1 == 0 or it == 1:\n",
    "            print('Iteration:', it, 'Loss:', loss, 'Loss diff:', diff_loss, 'Rank(Z): ', rank_Z)\n",
    "    print('Went to lower dim face:', B_used)\n",
    "    print('Stayed:', A_used)\n",
    "    print('Regular:', regularFW)\n",
    "    print('Not entered if:', not_entered)\n",
    "    return Zk, loss, it, res_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Zk Rank:  1\n",
      "Zk in border!\n",
      "thres: 4.440892098500626e-16\n",
      "rank 1\n",
      "Do a regular FW step - not entered initial if\n",
      "Iteration: 1 Loss: 0.5884274837060901 Loss diff: 0.35577931908417815 Rank(Z):  1\n",
      "Zk in border!\n",
      "thres: 0.0\n",
      "rank 1\n",
      "Do a regular FW step - not entered initial if\n",
      "Iteration: 2 Loss: 0.32801824515990163 Loss diff: 0.26040923854618847 Rank(Z):  2\n",
      "Zk in border!\n",
      "thres: 0.15819946475789526\n",
      "rank 2\n",
      "Do a regular FW step\n",
      "Iteration: 3 Loss: 0.2545835594764254 Loss diff: 0.07343468568347622 Rank(Z):  3\n",
      "Zk in border!\n",
      "thres: 0.16669523549971177\n",
      "rank 3\n",
      "Do a regular FW step\n",
      "Iteration: 4 Loss: 0.257975170475608 Loss diff: 0.0033916109991825594 Rank(Z):  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmart\\AppData\\Local\\Temp\\ipykernel_40660\\1582850291.py:94: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha_B = 1/(delta*u.T.dot(inv_s_thin).dot(u)-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zk in border!\n",
      "thres: 0.1077201754350755\n",
      "rank 4\n",
      "Went to a lower-dimensional face\n",
      "Iteration: 5 Loss: 0.25102037207339184 Loss diff: 0.0069547984022161335 Rank(Z):  3\n",
      "Zk in border!\n",
      "thres: 0.17590905073120378\n",
      "rank 3\n",
      "Do a regular FW step\n",
      "Iteration: 6 Loss: 0.24220396418196935 Loss diff: 0.00881640789142249 Rank(Z):  4\n",
      "Zk in border!\n",
      "thres: 0.12588748588685028\n",
      "rank 4\n",
      "Do a regular FW step\n",
      "Iteration: 7 Loss: 0.22044694369059736 Loss diff: 0.021757020491371987 Rank(Z):  5\n",
      "Zk in border!\n",
      "thres: 0.1025859342842742\n",
      "rank 5\n",
      "Do a regular FW step\n",
      "Iteration: 8 Loss: 0.2174325184321883 Loss diff: 0.003014425258409059 Rank(Z):  6\n",
      "Zk in border!\n",
      "thres: 0.08478117627411685\n",
      "rank 6\n",
      "Do a regular FW step\n",
      "Iteration: 9 Loss: 0.20713591119008046 Loss diff: 0.010296607242107847 Rank(Z):  7\n",
      "Zk in border!\n",
      "thres: 0.06903817158671877\n",
      "rank 7\n",
      "Do a regular FW step\n",
      "Iteration: 10 Loss: 0.20552782450445425 Loss diff: 0.0016080866856262033 Rank(Z):  8\n",
      "Zk in border!\n",
      "thres: 0.05659626893892267\n",
      "rank 8\n",
      "Do a regular FW step\n",
      "Iteration: 11 Loss: 0.2004376198701257 Loss diff: 0.005090204634328549 Rank(Z):  9\n",
      "Zk in border!\n",
      "thres: 0.048041726095970905\n",
      "rank 9\n",
      "Do a regular FW step\n",
      "Iteration: 12 Loss: 0.198105830798179 Loss diff: 0.0023317890719466994 Rank(Z):  10\n",
      "Zk in border!\n",
      "thres: 0.04081065296212749\n",
      "rank 10\n",
      "Do a regular FW step\n",
      "Iteration: 13 Loss: 0.19824873001664084 Loss diff: 0.00014289921846183384 Rank(Z):  11\n",
      "Zk in border!\n",
      "thres: 0.03501082360664132\n",
      "rank 11\n",
      "Do a regular FW step\n",
      "Iteration: 14 Loss: 0.19456926321110757 Loss diff: 0.0036794668055332713 Rank(Z):  12\n",
      "Zk in border!\n",
      "thres: 0.030444983763303846\n",
      "rank 12\n",
      "Do a regular FW step\n",
      "Iteration: 15 Loss: 0.1930478749039825 Loss diff: 0.00152138830712506 Rank(Z):  13\n",
      "Zk in border!\n",
      "thres: 0.026775544300262855\n",
      "rank 13\n",
      "Do a regular FW step\n",
      "Iteration: 16 Loss: 0.19252480964128246 Loss diff: 0.0005230652627000454 Rank(Z):  14\n",
      "Zk in border!\n",
      "thres: 0.02366452368920391\n",
      "rank 14\n",
      "Do a regular FW step\n",
      "Iteration: 17 Loss: 0.19245061440616495 Loss diff: 7.419523511750792e-05 Rank(Z):  15\n",
      "Zk in border!\n",
      "thres: 0.02113922330580309\n",
      "rank 15\n",
      "Do a regular FW step\n",
      "Iteration: 18 Loss: 0.19093810902184502 Loss diff: 0.0015125053843199343 Rank(Z):  16\n",
      "Zk in border!\n",
      "thres: 0.01895969988339219\n",
      "rank 16\n",
      "Do a regular FW step\n",
      "Iteration: 19 Loss: 0.1902782060219553 Loss diff: 0.0006599029998897166 Rank(Z):  17\n",
      "Zk in border!\n",
      "thres: 0.017319594874070754\n",
      "rank 17\n",
      "Do a regular FW step\n",
      "Iteration: 20 Loss: 0.19108445408919927 Loss diff: 0.0008062480672439698 Rank(Z):  18\n",
      "Zk in border!\n",
      "thres: 0.015694529006445523\n",
      "rank 18\n",
      "Do a regular FW step\n",
      "Iteration: 21 Loss: 0.18947654584147572 Loss diff: 0.0016079082477235551 Rank(Z):  19\n",
      "Zk in border!\n",
      "thres: 0.014310984215496148\n",
      "rank 19\n",
      "Do a regular FW step\n",
      "Iteration: 22 Loss: 0.18881656077500075 Loss diff: 0.0006599850664749662 Rank(Z):  20\n",
      "Zk in border!\n",
      "thres: 0.013073559033158766\n",
      "rank 20\n",
      "Do a regular FW step\n",
      "Iteration: 23 Loss: 0.18873215404116314 Loss diff:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmart\\AppData\\Local\\Temp\\ipykernel_40660\\1582850291.py:93: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  inv_s_thin = np.diag(1/s_thin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8.440673383761088e-05 Rank(Z):  21\n",
      "Zk in border!\n",
      "thres: 0.012009391137420167\n",
      "rank 21\n",
      "Do a regular FW step\n",
      "Iteration: 24 Loss: 0.18810156818201917 Loss diff: 0.0006305858591439673 Rank(Z):  22\n",
      "Zk in border!\n",
      "thres: 0.011076182261253065\n",
      "rank 22\n",
      "Do a regular FW step\n",
      "Iteration: 25 Loss: 0.18780443371849193 Loss diff: 0.0002971344635272488 Rank(Z):  23\n",
      "Zk in border!\n",
      "thres: 0.010223049600439404\n",
      "rank 23\n",
      "Do a regular FW step\n",
      "Iteration: 26 Loss: 0.18873422562309913 Loss diff: 0.0009297919046072078 Rank(Z):  24\n",
      "Zk in border!\n",
      "thres: 0.009483631743186227\n",
      "rank 24\n",
      "Do a regular FW step\n",
      "Iteration: 27 Loss: 0.18797688342823582 Loss diff: 0.0007573421948633141 Rank(Z):  25\n",
      "Zk in border!\n",
      "thres: 0.008812637836896231\n",
      "rank 25\n",
      "Do a regular FW step\n",
      "Iteration: 28 Loss: 0.1875407524201473 Loss diff: 0.00043613100808850835 Rank(Z):  26\n",
      "Zk in border!\n",
      "thres: 0.008217785797088428\n",
      "rank 26\n",
      "Do a regular FW step\n",
      "Iteration: 29 Loss: 0.18719976953192463 Loss diff: 0.0003409828882226795 Rank(Z):  27\n",
      "Zk in border!\n",
      "thres: 0.007685856596839402\n",
      "rank 27\n",
      "Do a regular FW step\n",
      "Iteration: 30 Loss: 0.18696194884004957 Loss diff: 0.00023782069187505694 Rank(Z):  28\n",
      "Zk in border!\n",
      "thres: 0.007200989820274728\n",
      "rank 28\n",
      "Do a regular FW step\n",
      "Iteration: 31 Loss: 0.1868251452668485 Loss diff: 0.00013680357320106085 Rank(Z):  29\n",
      "Zk in border!\n",
      "thres: 0.006768757396449665\n",
      "rank 29\n",
      "Do a regular FW step\n",
      "Iteration: 32 Loss: 0.18685719172787435 Loss diff: 3.2046461025836415e-05 Rank(Z):  30\n",
      "Zk in border!\n",
      "thres: 0.006363176576484442\n",
      "rank 30\n",
      "Do a regular FW step\n",
      "Iteration: 33 Loss: 0.18692251718350483 Loss diff: 6.532545563048275e-05 Rank(Z):  31\n",
      "Zk in border!\n",
      "thres: 0.00599756724380085\n",
      "rank 31\n",
      "Do a regular FW step\n",
      "Iteration: 34 Loss: 0.1866468901918159 Loss diff: 0.00027562699168892646 Rank(Z):  32\n",
      "Zk in border!\n",
      "thres: 0.005659235823107678\n",
      "rank 32\n",
      "Do a regular FW step\n",
      "Iteration: 35 Loss: 0.18643731380111514 Loss diff: 0.0002095763907007664 Rank(Z):  33\n",
      "Zk in border!\n",
      "thres: 0.005351735489599108\n",
      "rank 33\n",
      "Do a regular FW step\n",
      "Iteration: 36 Loss: 0.18626305529317363 Loss diff: 0.00017425850794150488 Rank(Z):  34\n",
      "Zk in border!\n",
      "thres: 0.005067576332777879\n",
      "rank 34\n",
      "Do a regular FW step\n",
      "Iteration: 37 Loss: 0.18655854307214437 Loss diff: 0.0002954877789707322 Rank(Z):  35\n",
      "Zk in border!\n",
      "thres: 0.0048057939295738095\n",
      "rank 35\n",
      "Do a regular FW step\n",
      "Iteration: 38 Loss: 0.186275163778917 Loss diff: 0.00028337929322735933 Rank(Z):  36\n",
      "Zk in border!\n",
      "thres: 0.004563118487303397\n",
      "rank 36\n",
      "Do a regular FW step\n",
      "Iteration: 39 Loss: 0.1861355627975701 Loss diff: 0.00013960098134691012 Rank(Z):  37\n",
      "Zk in border!\n",
      "thres: 0.004339336631692392\n",
      "rank 37\n",
      "Do a regular FW step\n",
      "Iteration: 40 Loss: 0.18602655067004975 Loss diff: 0.00010901212752034706 Rank(Z):  38\n",
      "Zk in border!\n",
      "thres: 0.004129444222086942\n",
      "rank 38\n",
      "Do a regular FW step\n",
      "Iteration: 41 Loss: 0.18615486524593142 Loss diff: 0.00012831457588166617 Rank(Z):  39\n",
      "Zk in border!\n",
      "thres: 0.003935921636245343\n",
      "rank 39\n",
      "Do a regular FW step\n",
      "Iteration: 42 Loss: 0.18594801700132269 Loss diff: 0.00020684824460873164 Rank(Z):  40\n",
      "Zk in border!\n",
      "thres: 0.0037544547690646635\n",
      "rank 40\n",
      "Do a regular FW step\n",
      "Iteration: 43 Loss: 0.18585990376849154 Loss diff: 8.811323283114403e-05 Rank(Z):  41\n",
      "Zk in border!\n",
      "thres: 0.003598375964028011\n",
      "rank 41\n",
      "Do a regular FW step\n",
      "Iteration: 44 Loss: 0.1858063799887591 Loss diff: 5.352377973244504e-05 Rank(Z):  42\n",
      "Zk in border!\n",
      "thres: 0.003441682489191411\n",
      "rank 42\n",
      "Do a regular FW step\n",
      "Iteration: 45 Loss: 0.18614458825249683 Loss diff: 0.0003382082637377326 Rank(Z):  43\n",
      "Zk in border!\n",
      "thres: 0.003295035435072924\n",
      "rank 43\n",
      "Do a regular FW step\n",
      "Iteration: 46 Loss: 0.18582583836047392 Loss diff: 0.00031874989202290704 Rank(Z):  44\n",
      "Zk in border!\n",
      "thres: 0.0031560480520711476\n",
      "rank 44\n",
      "Do a regular FW step\n",
      "Iteration: 47 Loss: 0.1856962510014573 Loss diff: 0.00012958735901660856 Rank(Z):  45\n",
      "Zk in border!\n",
      "thres: 0.0030253632620166337\n",
      "rank 45\n",
      "Do a regular FW step\n",
      "Iteration: 48 Loss: 0.1856137863782314 Loss diff: 8.246462322591963e-05 Rank(Z):  46\n",
      "Zk in border!\n",
      "thres: 0.0029033320211436786\n",
      "rank 46\n",
      "Do a regular FW step\n",
      "Iteration: 49 Loss: 0.185615830095779 Loss diff: 2.0437175476117897e-06 Rank(Z):  47\n",
      "Zk in border!\n",
      "thres: 0.002799860488999606\n",
      "rank 47\n",
      "Do a regular FW step\n",
      "Iteration: 50 Loss: 0.18570721425186415 Loss diff: 9.138415608514472e-05 Rank(Z):  48\n",
      "Zk in border!\n",
      "thres: 0.00269203482858249\n",
      "rank 48\n",
      "Do a regular FW step\n",
      "Iteration: 51 Loss: 0.18557834508560342 Loss diff: 0.00012886916626073108 Rank(Z):  49\n",
      "Zk in border!\n",
      "thres: 0.0025938572922481473\n",
      "rank 49\n",
      "Do a regular FW step\n",
      "Iteration: 52 Loss: 0.18552265431305168 Loss diff: 5.569077255174193e-05 Rank(Z):  50\n",
      "Zk in border!\n",
      "thres: 0.002497219134739437\n",
      "rank 50\n",
      "Do a regular FW step\n",
      "Iteration: 53 Loss: 0.18556736134069982 Loss diff: 4.470702764813872e-05 Rank(Z):  51\n",
      "Zk in border!\n",
      "thres: 0.0024056811497759067\n",
      "rank 51\n",
      "Do a regular FW step\n",
      "Iteration: 54 Loss: 0.18542621108980273 Loss diff: 0.00014115025089708744 Rank(Z):  52\n",
      "Zk in border!\n",
      "thres: 0.0023199985514225085\n",
      "rank 52\n",
      "Do a regular FW step\n",
      "Iteration: 55 Loss: 0.18537614225474497 Loss diff: 5.0068835057759387e-05 Rank(Z):  53\n",
      "Zk in border!\n",
      "thres: 0.002238315381932976\n",
      "rank 53\n",
      "Do a regular FW step\n",
      "Iteration: 56 Loss: 0.18540277730970472 Loss diff: 2.6635054959756044e-05 Rank(Z):  54\n",
      "Zk in border!\n",
      "thres: 0.0021728330419653608\n",
      "rank 54\n",
      "Do a regular FW step\n",
      "Iteration: 57 Loss: 0.1853633341426904 Loss diff: 3.944316701431916e-05 Rank(Z):  55\n",
      "Zk in border!\n",
      "thres: 0.0020989794983599097\n",
      "rank 55\n",
      "Do a regular FW step\n",
      "Iteration: 58 Loss: 0.1853795988145463 Loss diff: 1.626467185589231e-05 Rank(Z):  56\n",
      "Zk in border!\n",
      "thres: 0.002029273043298363\n",
      "rank 56\n",
      "Do a regular FW step\n",
      "Iteration: 59 Loss: 0.1852974722229363 Loss diff: 8.212659160999891e-05 Rank(Z):  57\n",
      "Zk in border!\n",
      "thres: 0.0019633439106527817\n",
      "rank 57\n",
      "Do a regular FW step\n",
      "Iteration: 60 Loss: 0.18527604333309847 Loss diff: 2.142888983783231e-05 Rank(Z):  58\n",
      "Zk in border!\n",
      "thres: 0.0019008813462504381\n",
      "rank 58\n",
      "Do a regular FW step\n",
      "Iteration: 61 Loss: 0.18525593786098404 Loss diff: 2.0105472114423373e-05 Rank(Z):  59\n",
      "Zk in border!\n",
      "thres: 0.0018420422442466666\n",
      "rank 59\n",
      "Do a regular FW step\n",
      "Iteration: 62 Loss: 0.1852737604097571 Loss diff: 1.7822548773055447e-05 Rank(Z):  60\n",
      "Zk in border!\n",
      "thres: 0.001785857415572023\n",
      "rank 60\n",
      "Do a regular FW step\n",
      "Iteration: 63 Loss: 0.18538623130508924 Loss diff: 0.00011247089533214516 Rank(Z):  61\n",
      "Zk in border!\n",
      "thres: 0.0017320556260765896\n",
      "rank 61\n",
      "Do a regular FW step\n",
      "Iteration: 64 Loss: 0.1852796511189166 Loss diff: 0.00010658018617265563 Rank(Z):  62\n",
      "Zk in border!\n",
      "thres: 0.0016801385863094431\n",
      "rank 62\n",
      "Do a regular FW step\n",
      "Iteration: 65 Loss: 0.1852191174458776 Loss diff: 6.053367303898716e-05 Rank(Z):  63\n",
      "Zk in border!\n",
      "thres: 0.0016301365983859561\n",
      "rank 63\n",
      "Do a regular FW step\n",
      "Iteration: 66 Loss: 0.18518347426665616 Loss diff: 3.564317922144511e-05 Rank(Z):  64\n",
      "Zk in border!\n",
      "thres: 0.0015821373831000907\n",
      "rank 64\n",
      "Do a regular FW step\n",
      "Iteration: 67 Loss: 0.1851848645244068 Loss diff: 1.3902577506441638e-06 Rank(Z):  65\n",
      "Zk in border!\n",
      "thres: 0.001536193506347372\n",
      "rank 65\n",
      "Do a regular FW step\n",
      "Iteration: 68 Loss: 0.18516450457725336 Loss diff: 2.0359947153436586e-05 Rank(Z):  66\n",
      "Zk in border!\n",
      "thres: 0.0015034995183840216\n",
      "rank 66\n",
      "Do a regular FW step\n",
      "Iteration: 69 Loss: 0.18521498524215177 Loss diff: 5.048066489840575e-05 Rank(Z):  67\n",
      "Zk in border!\n",
      "thres: 0.0014612609344497596\n",
      "rank 67\n",
      "Do a regular FW step\n",
      "Iteration: 70 Loss: 0.18513831368028105 Loss diff: 7.6671561870717e-05 Rank(Z):  68\n",
      "Zk in border!\n",
      "thres: 0.0014208227389735928\n",
      "rank 68\n",
      "Do a regular FW step\n",
      "Iteration: 71 Loss: 0.18517914788030806 Loss diff: 4.0834200027012324e-05 Rank(Z):  69\n",
      "Zk in border!\n",
      "thres: 0.0013822401348966284\n",
      "rank 69\n",
      "Do a regular FW step\n",
      "Iteration: 72 Loss: 0.18511786278982967 Loss diff: 6.128509047839681e-05 Rank(Z):  70\n",
      "Zk in border!\n",
      "thres: 0.0013449374460470231\n",
      "rank 70\n",
      "Do a regular FW step\n",
      "Iteration: 73 Loss: 0.1850978586000019 Loss diff: 2.000418982775476e-05 Rank(Z):  71\n",
      "Zk in border!\n",
      "thres: 0.001308996563717968\n",
      "rank 71\n",
      "Do a regular FW step\n",
      "Iteration: 74 Loss: 0.18508141550560575 Loss diff: 1.6443094396162428e-05 Rank(Z):  72\n",
      "Zk in border!\n",
      "thres: 0.0012744506690905988\n",
      "rank 72\n",
      "Do a regular FW step\n",
      "Iteration: 75 Loss: 0.18507222312981259 Loss diff: 9.192375793165564e-06 Rank(Z):  73\n",
      "Zk in border!\n",
      "thres: 0.0012411848862701191\n",
      "rank 73\n",
      "Do a regular FW step\n",
      "Iteration: 76 Loss: 0.18504160441248146 Loss diff: 3.0618717331121204e-05 Rank(Z):  74\n",
      "Zk in border!\n",
      "thres: 0.0012094366885193608\n",
      "rank 74\n",
      "Do a regular FW step\n",
      "Iteration: 77 Loss: 0.1850716561447778 Loss diff: 3.005173229633784e-05 Rank(Z):  75\n",
      "Zk in border!\n",
      "thres: 0.0011793385829993452\n",
      "rank 75\n",
      "Do a regular FW step\n",
      "Iteration: 78 Loss: 0.18507092440376932 Loss diff: 7.317410084806131e-07 Rank(Z):  76\n",
      "Zk in border!\n",
      "thres: 0.0011502362962890755\n",
      "rank 76\n",
      "Do a regular FW step\n",
      "Iteration: 79 Loss: 0.1850565079950679 Loss diff: 1.4416408701423133e-05 Rank(Z):  77\n",
      "Zk in border!\n",
      "thres: 0.0011225007744792848\n",
      "rank 77\n",
      "Do a regular FW step\n",
      "Iteration: 80 Loss: 0.18504735279904436 Loss diff: 9.155196023541423e-06 Rank(Z):  78\n",
      "Zk in border!\n",
      "thres: 0.0010953069239366675\n",
      "rank 78\n",
      "Do a regular FW step\n",
      "Iteration: 81 Loss: 0.18501707297822023 Loss diff: 3.027982082412506e-05 Rank(Z):  79\n",
      "Zk in border!\n",
      "thres: 0.0010691048388999924\n",
      "rank 79\n",
      "Do a regular FW step\n",
      "Iteration: 82 Loss: 0.18503242983307647 Loss diff: 1.5356854856235502e-05 Rank(Z):  80\n",
      "Zk in border!\n",
      "thres: 0.0010441177546876013\n",
      "rank 80\n",
      "Do a regular FW step\n",
      "Iteration: 83 Loss: 0.1849985242733947 Loss diff: 3.390555968177389e-05 Rank(Z):  81\n",
      "Zk in border!\n",
      "thres: 0.0010205011741146741\n",
      "rank 81\n",
      "Do a regular FW step\n",
      "Iteration: 84 Loss: 0.18497750596382168 Loss diff: 2.1018309573017646e-05 Rank(Z):  82\n",
      "Zk in border!\n",
      "thres: 0.0009970723236005785\n",
      "rank 82\n",
      "Do a regular FW step\n",
      "Iteration: 85 Loss: 0.18497269385844173 Loss diff: 4.812105379947695e-06 Rank(Z):  83\n",
      "Zk in border!\n",
      "thres: 0.0009741979177606552\n",
      "rank 83\n",
      "Do a regular FW step\n",
      "Iteration: 86 Loss: 0.18503380787404777 Loss diff: 6.111401560604524e-05 Rank(Z):  84\n",
      "Zk in border!\n",
      "thres: 0.0009522942843436244\n",
      "rank 84\n",
      "Do a regular FW step\n",
      "Iteration: 87 Loss: 0.18498118231484464 Loss diff: 5.2625559203128836e-05 Rank(Z):  85\n",
      "Zk in border!\n",
      "thres: 0.00093085217005906\n",
      "rank 85\n",
      "Do a regular FW step\n",
      "Iteration: 88 Loss: 0.18497534979335245 Loss diff: 5.832521492193932e-06 Rank(Z):  86\n",
      "Zk in border!\n",
      "thres: 0.0009104430776238903\n",
      "rank 86\n",
      "Do a regular FW step\n",
      "Iteration: 89 Loss: 0.18497268063352845 Loss diff: 2.6691598239980685e-06 Rank(Z):  87\n",
      "Zk in border!\n",
      "thres: 0.0008905438010582412\n",
      "rank 87\n",
      "Do a regular FW step\n",
      "Iteration: 90 Loss: 0.1849500215708604 Loss diff: 2.2659062668045626e-05 Rank(Z):  88\n",
      "Zk in border!\n",
      "thres: 0.0008713103729247607\n",
      "rank 88\n",
      "Do a regular FW step\n",
      "Iteration: 91 Loss: 0.18499548965773377 Loss diff: 4.546808687336257e-05 Rank(Z):  89\n",
      "Zk in border!\n",
      "thres: 0.0008534522220717822\n",
      "rank 89\n",
      "Do a regular FW step\n",
      "Iteration: 92 Loss: 0.1849455755260339 Loss diff: 4.991413169985748e-05 Rank(Z):  90\n",
      "Zk in border!\n",
      "thres: 0.0008356522076660156\n",
      "rank 90\n",
      "Do a regular FW step\n",
      "Iteration: 93 Loss: 0.18492966428476323 Loss diff: 1.5911241270677445e-05 Rank(Z):  91\n",
      "Zk in border!\n",
      "thres: 0.000818384835550301\n",
      "rank 91\n",
      "Do a regular FW step\n",
      "Iteration: 94 Loss: 0.1849734604536628 Loss diff: 4.3796168899573606e-05 Rank(Z):  92\n",
      "Zk in border!\n",
      "thres: 0.0008016544592149843\n",
      "rank 92\n",
      "Do a regular FW step\n",
      "Iteration: 95 Loss: 0.18492077507954657 Loss diff: 5.268537411623475e-05 Rank(Z):  93\n",
      "Zk in border!\n",
      "thres: 0.0007851797067461863\n",
      "rank 93\n",
      "Do a regular FW step\n",
      "Iteration: 96 Loss: 0.1849137028990226 Loss diff: 7.072180523981508e-06 Rank(Z):  94\n",
      "Zk in border!\n",
      "thres: 0.0007691961369593603\n",
      "rank 94\n",
      "Do a regular FW step\n",
      "Iteration: 97 Loss: 0.18491254081798553 Loss diff: 1.1620810370649437e-06 Rank(Z):  95\n",
      "Zk in border!\n",
      "thres: 0.0007537664074525052\n",
      "rank 95\n",
      "Do a regular FW step\n",
      "Iteration: 98 Loss: 0.18490695393405235 Loss diff: 5.586883933178521e-06 Rank(Z):  96\n",
      "Zk in border!\n",
      "thres: 0.0007387302256083306\n",
      "rank 96\n",
      "Do a regular FW step\n",
      "Iteration: 99 Loss: 0.18490331268751506 Loss diff: 3.6412465372892555e-06 Rank(Z):  97\n",
      "Zk in border!\n",
      "thres: 0.0007243205530418351\n",
      "rank 97\n",
      "Do a regular FW step\n",
      "Iteration: 100 Loss: 0.18492410320188146 Loss diff: 2.0790514366403334e-05 Rank(Z):  98\n",
      "Went to lower dim face: 1\n",
      "Stayed: 0\n",
      "Regular: 97\n",
      "Not entered if: 2\n"
     ]
    }
   ],
   "source": [
    "pred_ratings, loss, it, res_listInFW = FW_inface_mix(X_test, FW_objective_function, gamma1 = 0.25, gamma2 = 0.5, delta = 1, THRES = 10, max_iter = 100, patience = 1e-7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAh4klEQVR4nO3dfbRddX3n8ffnnHNPnkOeLhGTQAIGK4gFuZOiVqUWJNUOcWrXiLYdnNqytLK0Y9e0MLbUiXWmaodqZ1Ety9LSB5oq2nrbpqXIiJ3aBeRGKEJoIAQlyYBcSCCQp3vO2d/5Y+9z77439+EkOTc37P15rXXXPfvpnN++O/ns3/nt3/5tRQRmZlZclZkugJmZTS8HvZlZwTnozcwKzkFvZlZwDnozs4KrzXQBxlq2bFmsXr16pothZvaysnXr1mcjone8ZR0FvaT1wOeBKvCliPjtcdb5j8AngAD+NSLel82/Gvj1bLXfiohbJ/us1atXMzAw0EmxzMwsI+n7Ey2bMuglVYGbgMuB3cAWSf0RsS23zlrgeuBNEbFP0unZ/CXAbwJ9pCeArdm2+05kh8zMrHOdtNGvA3ZExM6IGAI2ARvGrPOLwE3tAI+IZ7L5VwB3RsTebNmdwPruFN3MzDrRSdCvAHblpndn8/LOBc6V9G1J92RNPZ1ua2Zm06hbF2NrwFrgUmAl8E+SLuh0Y0nXANcAnHnmmV0qkpmZQWc1+j3Aqtz0ymxe3m6gPyIaEfEE8Chp8HeyLRFxc0T0RURfb++4F43NzOw4dRL0W4C1ktZIqgNXAf1j1vlr0to8kpaRNuXsBO4A3i5psaTFwNuzeWZmdpJM2XQTEU1J15IGdBW4JSIelrQRGIiIfkYCfRvQAv5rRDwHIOmTpCcLgI0RsXc6dsTMzManU22Y4r6+vuhGP/p/eOgp+lYvYdn8WV0olZnZqU3S1ojoG29ZIYdAODTU4kN//h2+9p3dM10UM7MZV8igH2olRMDhRjLTRTEzm3GFDPpmKw34RstBb2ZWyKBvJel1h6Gmg97MrJBB32gHvWv0ZmbFDPpWKw16N92YmRU06BtJ1kbfPLW6jpqZzYRCBn27jd41ejOzggZ9O+DdRm9mVtCgd43ezGxEIYO+MXwx1m30ZmaFDHrX6M3MRhQy6Nt3xh7xDVNmZgUNetfozcyGFTLo3XRjZjaikEHfDnjfMGVm1mHQS1ovabukHZKuG2f5+yUNSnog+/mF3LJWbv7YRxBOC9fozcxGTPkoQUlV4CbgctKHgG+R1B8R28as+pcRce04b3EoIi484ZIeAw9qZmY2opMa/TpgR0TsjIghYBOwYXqLdWJaicejNzNr6yToVwC7ctO7s3ljvVvSg5Jul7QqN3+2pAFJ90h613gfIOmabJ2BwcHBjgs/Ed8wZWY2olsXY/8GWB0RrwPuBG7NLTsre2Dt+4DPSTpn7MYRcXNE9EVEX29v7wkXZriN3v3ozcw6Cvo9QL6GvjKbNywinouII9nkl4CLc8v2ZL93AncDF51AeTvS9KBmZmbDOgn6LcBaSWsk1YGrgFG9ZySdkZu8Engkm79Y0qzs9TLgTcDYi7hd18xdjI1w842ZlduUvW4ioinpWuAOoArcEhEPS9oIDEREP/ARSVcCTWAv8P5s89cAfyApIT2p/PY4vXW6rpm1zUekzTi1qqb7I83MTllTBj1ARGwGNo+Zd0Pu9fXA9eNs9y/ABSdYxmPWrtFDekG2Vj3ZJTAzO3UU8s7YZq5t3u30ZlZ2xQz6UTV6B72ZlVtBg34k3B30ZlZ2BQ36XI3eA5uZWckVM+hzd8S6jd7Myq6QQd9yG72Z2bBCBn2+jX7IwyCYWckVM+hbrtGbmbUVM+gTt9GbmbUVM+hb+e6V7nVjZuVWzKAf1b3SNXozK7diBr3b6M3MhhUz6JNgdk+6a26jN7OyK2jQJ8zpSYesdBu9mZVdIYO+lQRz6+kIzG66MbOy6yjoJa2XtF3SDknXjbP8/ZIGJT2Q/fxCbtnVkh7Lfq7uZuEn0mglzKmnNXrfMGVmZTflg0ckVYGbgMuB3cAWSf3jPCnqLyPi2jHbLgF+E+gDAtiabbuvK6WfQCuJXNONg97Myq2TGv06YEdE7IyIIWATsKHD978CuDMi9mbhfiew/viK2rlGK0Zq9A56Myu5ToJ+BbArN707mzfWuyU9KOl2SauOcduuGlWj9zDFZlZy3boY+zfA6oh4HWmt/dZj2VjSNZIGJA0MDg6ecGEarYR6rUK1IjfdmFnpdRL0e4BVuemV2bxhEfFcRBzJJr8EXNzpttn2N0dEX0T09fb2dlr2CbWSoFYRPVUHvZlZJ0G/BVgraY2kOnAV0J9fQdIZuckrgUey13cAb5e0WNJi4O3ZvGnVSoJatUJPteI2ejMrvSl73UREU9K1pAFdBW6JiIclbQQGIqIf+IikK4EmsBd4f7btXkmfJD1ZAGyMiL3TsB+jNJKEWkXUqxXX6M2s9KYMeoCI2AxsHjPvhtzr64HrJ9j2FuCWEyjjMWu12k03FV+MNbPSK+SdsY0kqFVFveamGzOzQgZ9ejG2Qk9VDnozK71CBn2jlVAdbrpx0JtZuRUy6FtJ0JM13fhirJmVXSGDvtkKqpW0e6WHKTazsitm0GfdK91Gb2ZWwKBPkiAJqFWzNnoHvZmVXOGCvv1gcN8wZWaWKmDQp8HeHgLBN0yZWdkVMOhzNXr3ujEzK2DQt0aCvqda4Yj70ZtZyRUv6LOmm2q1Qr3mYYrNzAoX9K2s6aan4l43ZmZQwKBvN90MD4HgG6bMrOSKF/TtGr0fPGJmBhQx6LNgr1ZEPXuUYIRr9WZWXh0FvaT1krZL2iHpuknWe7ekkNSXTa+WdEjSA9nPF7tV8ImM1OjTppuIkXZ7M7MymvIJU5KqwE3A5cBuYIuk/ojYNma9BcBHgXvHvMXjEXFhd4o7tZE2+go9tfQ81mgFterJKoGZ2amlkxr9OmBHROyMiCFgE7BhnPU+CXwaONzF8h2zkTtj0yEQALfTm1mpdRL0K4Bduend2bxhkl4PrIqIvxtn+zWS7pf0LUlvHu8DJF0jaUDSwODgYKdlH1f+zth2jX7IN02ZWYmd8MVYSRXgRuBXxln8FHBmRFwEfAy4TdLCsStFxM0R0RcRfb29vSdUnpE7YyvUqwJwX3ozK7VOgn4PsCo3vTKb17YAeC1wt6TvAZcA/ZL6IuJIRDwHEBFbgceBc7tR8Inkm256qu02ege9mZVXJ0G/BVgraY2kOnAV0N9eGBEvRMSyiFgdEauBe4ArI2JAUm92MRdJZwNrgZ1d34ucdtNN+4YpcNCbWblN2esmIpqSrgXuAKrALRHxsKSNwEBE9E+y+VuAjZIaQAJ8MCL2dqPgE2k33fRkjxIEGPJQxWZWYlMGPUBEbAY2j5l3wwTrXpp7/VXgqydQvmPWSnI3TNXcRm9mVrg7Y9tj2/S4jd7MDChg0LfGaaN3P3ozK7PCBX1+ULN67s5YM7OyKl7QjxrUzDdMmZkVL+jbd8a6jd7MDChi0GehXqtU6PGdsWZmBQz6cWr0broxszIrbtBX5IuxZmYUMOhbycigZm6jNzMrYNA3htvo5TZ6MzMKGPStJKgIKr5hyswMKGDQN1pBrZLuVrsffcODmplZiRUu6FtJQrWSNtlUKqJWkZtuzKzUChf06YPANTzdU6246cbMSq1wQd9KglolH/RyP3ozK7WOgl7SeknbJe2QdN0k671bUkjqy827Pttuu6QrulHoyTSThFp1ZLfqtYqbbsys1KZ88Ej2KMCbgMuB3cAWSf0RsW3MeguAjwL35uadR/rowfOBVwLfkHRuRLS6twujNVtja/QOejMrt05q9OuAHRGxMyKGgE3AhnHW+yTwaeBwbt4GYFP2kPAngB3Z+02bVnJ0G73vjDWzMusk6FcAu3LTu7N5wyS9HlgVEX93rNt2WyMZ6V4JWRu9a/RmVmInfDFWUgW4EfiVE3iPayQNSBoYHBw8ofK0kuTophtfjDWzEusk6PcAq3LTK7N5bQuA1wJ3S/oecAnQn12QnWpbACLi5ojoi4i+3t7eY9uDMRqtGO5HDzDLF2PNrOQ6CfotwFpJayTVSS+u9rcXRsQLEbEsIlZHxGrgHuDKiBjI1rtK0ixJa4C1wH1d34ucVhLDQx+A2+jNzKbsdRMRTUnXAncAVeCWiHhY0kZgICL6J9n2YUlfBrYBTeDD09njBtIBzKpjmm7cj97MymzKoAeIiM3A5jHzbphg3UvHTH8K+NRxlu+YpTX6XNDXKhw61DhZH29mdsop3J2xzTFt9PWqx7oxs3IrXtAnyTht9A56MyuvAgZ9HNVG74uxZlZmxQv61tgbpnwx1szKrXhBP+aGKQ9qZmZlV8CgD6pVX4w1M2srXtC3gh630ZuZDStc0LeSoJpvo6+5jd7Myq1wQd9oJaNvmMoeJRjhWr2ZlVPhgr6VHH3DFKRt92ZmZVS4oG+OM6gZ4AuyZlZaxQv6cQY1A2g0XaM3s3IqXtCPfZRgLd1FP2XKzMqqmEGff/CIm27MrOQKFfQRkT4cfFT3yjT0HfRmVlaFCvp2z5qxz4wFB72ZlVdHQS9pvaTtknZIum6c5R+U9F1JD0j6Z0nnZfNXSzqUzX9A0he7vQN5rXbQj9Pr5ohvmjKzkpryCVOSqsBNwOXAbmCLpP6I2JZb7baI+GK2/pXAjcD6bNnjEXFhV0s9gXatfdSgZsM1eve6MbNy6qRGvw7YERE7I2II2ARsyK8QEftzk/OAGUnVkRq9m27MzNo6CfoVwK7c9O5s3iiSPizpceAzwEdyi9ZIul/StyS9ebwPkHSNpAFJA4ODg8dQ/NHatfbRbfTZxVg33ZhZSXXtYmxE3BQR5wC/Bvx6Nvsp4MyIuAj4GHCbpIXjbHtzRPRFRF9vb+9xl6Fdox87qBm4H72ZlVcnQb8HWJWbXpnNm8gm4F0AEXEkIp7LXm8FHgfOPa6SdmC4jb7qNnozs7ZOgn4LsFbSGkl14CqgP7+CpLW5yXcCj2Xze7OLuUg6G1gL7OxGwcfTGqd7Zb3mNnozK7cpe91ERFPStcAdQBW4JSIelrQRGIiIfuBaSZcBDWAfcHW2+VuAjZIaQAJ8MCL2TseOQPoYQRi/e6WD3szKasqgB4iIzcDmMfNuyL3+6ATbfRX46okU8FiMf8NU+toPHzGzsirWnbHj9LpxG72ZlV2xgn6SfvRDzdaMlMnMbKYVKuhb7Tb6cbpXukZvZmVVqKCf7IYp96M3s7IqVNCPO6hZpd1046A3s3IqVNC3u1DmHyVYqYgFs2rsP9yYqWKZmc2oQgV9u0bfk7sYC7B0fp3nXhqaiSKZmc24QgV9u40+X6MHWDp/Fs8dODITRTIzm3GFCvqRGv3o3VoyzzV6MyuvQgV9ewiEsTX6ZfPrPHfAQW9m5VSsoM+abnoqo3dr6bxZ7D0wRJK4L72ZlU+xgr5dox9zMXbJvDqtJNzzxsxKqWBBf/QNU5D2ugF41u30ZlZCxQr6ce6MBVg2fxYAz73knjdmVj7FCvrhGv3RvW4A9vqCrJmVUEdBL2m9pO2Sdki6bpzlH5T0XUkPSPpnSeflll2fbbdd0hXdLPxYw4OajXPDFMCzDnozK6Epgz57FOBNwE8A5wHvzQd55raIuCAiLgQ+A9yYbXse6aMHzwfWA7/ffrTgdJjohqnFc9Ogd9ONmZVRJzX6dcCOiNgZEUOkD//ekF8hIvbnJucB7X6MG4BN2UPCnwB2ZO83LSa6YaqnWmHR3B433ZhZKXXyKMEVwK7c9G7gR8auJOnDwMeAOvC23Lb3jNl2xXGVtAPNbFCzMRV6AJb67lgzK6muXYyNiJsi4hzg14BfP5ZtJV0jaUDSwODg4HGXoZkEPVUhHZ30S+fN4lk33ZhZCXUS9HuAVbnpldm8iWwC3nUs20bEzRHRFxF9vb29HRRpfM0kjmqfb1s6v+6mGzMrpU6CfguwVtIaSXXSi6v9+RUkrc1NvhN4LHvdD1wlaZakNcBa4L4TL/b4mq04aviDtqUe78bMSmrKNvqIaEq6FrgDqAK3RMTDkjYCAxHRD1wr6TKgAewDrs62fVjSl4FtQBP4cERM21O6m0ly1PAHbUvmzWLfwSFak9T6zcyKqJOLsUTEZmDzmHk35F5/dJJtPwV86ngLeCyaSRx1s1Tbsvl1ImDfwaHhO2XNzMqgWHfGtpKjhj9oWzqvPQyCm2/MrFyKFfRJHHVXbFt7GATfNGVmZVOsoG/FhDX6ZdkwCL4ga2ZlU6ign+xC61KPYGlmJVWooG+0kqOGP2hbNKeHilyjN7PyKVTQT1ajr1SUPiTcQW9mJVOooE8vxk68S0vm1d10Y2alU7Cgn7h7JaRdLN290szKplhBP0mvG/B4N2ZWTsUK+kn60UM6VLFHsDSzsile0E8wBAKkXSz3H24y1ExOYqnMzGZWsYJ+kiEQYOTZsfsOuvnGzMqjUEHf6qDpBnDzjZmVSqGCvtFKpmy6AQ9sZmblUqig77RGv/fAEM8fHOJ373yUb+949mQVz8xsRnQ0Hv3LRaM1+UNF2kMV33bfk9zw9YfYf7jJrFqF237xR7j4rCUnq5hmZidVRzV6SeslbZe0Q9J14yz/mKRtkh6UdJeks3LLWpIeyH76x27bTa1k4kcJAiycU6NerXDfE3u5+KzFbLrmEl65aA4fuHWAHc+8NJ1FMzObMVPW6CVVgZuAy4HdwBZJ/RGxLbfa/UBfRByU9CHgM8B7smWHIuLC7hZ7fJM9ShBAEv/7fRexcHYPbzhnKQC3/ud1/NQXvs3Vt9zH137pjSxfOPtkFNXM7KTppEa/DtgRETsjYgjYBGzIrxAR34yIg9nkPcDK7hazM80k6JniebBXnP+K4ZAHOHPpXP7o/evYd3CID/3ZVpot97E3s2LpJOhXALty07uzeRP5APD3uenZkgYk3SPpXeNtIOmabJ2BwcHBDoo0vmYrqE7SdDORC1aexv/8qQv4zpPP8/t3P37cn29mdirqaq8bST8L9AGfzc0+KyL6gPcBn5N0ztjtIuLmiOiLiL7e3t7j/vxmkkza62YyGy5cwYYLX8nn73qMB3Y9f9xlMDM71XQS9HuAVbnpldm8USRdBnwcuDIihu9Iiog92e+dwN3ARSdQ3klNNajZVDZueC3LF8zilzfdz4EjzS6WzMxs5nQS9FuAtZLWSKoDVwGjes9Iugj4A9KQfyY3f7GkWdnrZcCbgPxF3K6JiGysm+MP+tPm9HDjey7k+3sPcuOdj3axdGZmM2fKoI+IJnAtcAfwCPDliHhY0kZJV2arfRaYD3xlTDfK1wADkv4V+Cbw22N663RNEunvyR480olLzl7KFee9gr998P8REV0omZnZzOrohqmI2AxsHjPvhtzryybY7l+AC06kgJ1qZL1lJrthqlNv+6HT+YeHn+aRp17kvFcuPOH3MzObSYUZAqGVVel7jvNibN5bX51eEL770WemWNPM7NRXmKBvttKgP57ulWMtXzib15yxkLu3H39XTzOzU0Vxgj5Jm266UaMH+LFX97L1+/vYf7jRlfczM5sphQn6+bNr/MnPr+PHX7O8K+936atPp5UE337Mo1ua2ctbYYJ+Vq3KW87tZcWiOV15v9efuYgFs2tuvjGzl73CBH231aoV3rx2Gd96dNDdLM3sZc1BP4lLzz2dp/cf5t+efnGmi2Jmdtwc9JMY7mbp5hszexlz0E9i+cLZ/PDK0/jdOx/l43/1XXbtPTj1RmZmpxgH/RS++HMX89N9K/nKwG5+7Hfu5vPfeGymi2Rmdkwc9FM447Q5/I//cAHf+tVLefv5y/ncXY9y/5P7ZrpYZmYdc9B36IzT5vCZn/5hTl8wi9/4+kPDQy6YmZ3qHPTHYP6sGh9/53k8tGc/t937/ZkujplZRxz0x+jfv+4M3njOUj57x3aefenI1BuYmc0wB/0xksTGDedzqNHiQ3+2lS8P7GL3PvfGMbNTV0dBL2m9pO2Sdki6bpzlH5O0TdKDku6SdFZu2dWSHst+ru5m4WfKq05fwG/85Hk88ewBfvX2B/nRT3+Td3/hX3jqhUMzXTQzs6Noqtv7JVWBR4HLgd2kjxZ8b/5JUZJ+DLg3Ig5K+hBwaUS8R9ISYID0geEBbAUujogJu6309fXFwMDACe7WyRERPPbMS/zTo4N87huPMbunyh/83MVcfNbimS6amZWMpK0R0Tfesk5q9OuAHRGxMyKGgE3AhvwKEfHNiGi3X9xD+gBxgCuAOyNibxbudwLrj2cnTkWSOHf5An7hzWfzV7/0RubNqvLem+/hj7/9BAeH/HBxMzs1dPIowRXArtz0buBHJln/A8DfT7LtimMp4MvF2uUL+PqH38S1t93PJ/5mG5+9YzvvfN0ZvPXc04H0UYdL59f50VctQ+rOmPlmZp3o6JmxnZL0s6TNNG89xu2uAa4BOPPMM7tZpJNq0dw6f/qBdWz53j5u37qLv3vwKb48sHvUOm84eymfuPJ8Xv2KBTNUSjMrm06Cfg+wKje9Mps3iqTLgI8Db42II7ltLx2z7d1jt42Im4GbIW2j76BMpyxJrFuzhHVrlvCJK89n5+AB6rUKPdUK/7zjWf7XP27nHb/3f3nPv1vFZa85nYvPWsJpc3pmuthmVmCdXIytkV6M/XHS4N4CvC8iHs6tcxFwO7A+Ih7LzV9CegH29dms75BejN070ee9nC7GHo99B4b4nX/czlcGdjPUSpBg7enzOX3BbJbOr7NoTg/VSoVqBebUa1y0ahEXr17Mwtk+GZjZxCa7GDtl0Gdv8A7gc0AVuCUiPiVpIzAQEf2SvgFcADyVbfJkRFyZbfvzwH/L5n8qIv5oss8qetC3HW60+M6T+7jvib08tGc/zx04wnMvDfH8wSFaSdCKYKiZkARUBOcuX8CqJXN5xcLZLJ5X55n9h3ly70GefekI552xkDe+ahlvOHspKxbNoVLxNQCzsjnhoD+ZyhL0nTg01OL+XenJ4IFdz/PU84d5ev9hXjjUYNn8OisXz2XJvDoP7n6eZ18aAtKHo5++YDanL5zF7FqVWlVUK2LJvDrLF87m9AWzWDi7h3mzasypV/nB/sM88ewBvv/cAWbXqqxYPIcVi+bQu2AWi+b2cNqcOgtn11gwu4fZPRVfSDY7RU0W9F29GGvdNade5Y3nLOON5ywbNb+VBNVcrb3dn/++J/ay5/lD/OCFw/zgxcMMNROONINmEjz2g5d45sXDNFpHn9hrFbFqyVyGmglPPXCIicZrq1bE3HqVOT1V5tSr1KvptYeeqqhUlG4XQRJpGZMIKsq2qVeZW68yu6fKrFqFWbX0d71WQYKDQy0OHmnRaCXMrleZV68yp16jXhW17HNqFaUnLolGEjRbCa0k0vfL3reWNXtVKxUqgooESk+aLx1ucmCoSb1WYf6sGvPqteHrJz3ZCbFaERWlPwBBEAHNJGglQUWkZamKWkUot24SQUQgafhvk75fWg4JItIbSiD9pibS8pH7m1cqUKukfxdl62S7MXyibVfQAmi2gkaSkGT/LtK/gYY/2ydnc9C/DFXHNM20+/Ofu3zynjxJEjx/qMGLhxu8dKTJwaEWy+bPYtXiOdSq6S0VjVbC0y8cZu+BIV441GDfwSFePNzkpSNNXjzc4NBQwqFGi0NDTYZaCUPNoNFKSLKAE2mAtQMzieDgUIv9h5s8s/8Ih5stjjQSjjRb2YkoIYC59Srz6jVqVXG40UqDf6g1TX/BcmmfJNonm/Qo5VfIzjWRnqzaJ+hKJT2pQnYSS1cdPrbt80f+BNT+rJHPVm55jDrRjSnCqPK254yc4NLy5depjHrv/LapJEkrHcP7k5308uuMPQe2T6rtv0X7MyuVkc9r71dklZogEBpVsRhvv0Z9Tu6DI/vbEvCaMxZy08+8foKtjp+DvkQqWRPOknn1CdfpqVZYtWQuq5bMPYklG19E+m2k0UpoNINmkgzXrGtV0VOpUKkoO2G0ONxITzjNVrpOMPIffV69xvzZNeb2VBlqJbx4uMmBI00arYShVpJuE0Er+50PlbR2nP4nDqDRHClHOxiHa91ZAAw1ExqtoJWkJ7J26ORzICANitxJsr1uK2J4KOyI0QEZY96n1v5WlZ1Y25/bSkaCO7JAGvttrT2fIPt2MhKqrQiSZKRs7QImEbSSkW87+eMV47x3GpqjQzt/sslvNXY/h08O7e3b34py+9Muw6hPD4ZPVPngbsXodUb9LXL7MPakGLlt22WsjPmWleSO2ThvP+7ntk8S7ZPtmmXzJtrqhDjo7ZQliZ6q6KlWYOJz03FZvrC772d2KvPolWZmBeegNzMrOAe9mVnBOejNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgTrlBzSQNAt8/gbdYBjzbpeK8XJRtn8u2v+B9LosT2eezIqJ3vAWnXNCfKEkDE43gVlRl2+ey7S94n8tiuvbZTTdmZgXnoDczK7giBv3NM12AGVC2fS7b/oL3uSymZZ8L10ZvZmajFbFGb2ZmOQ56M7OCK0zQS1ovabukHZKum+nyTAdJqyR9U9I2SQ9L+mg2f4mkOyU9lv1ePNNl7TZJVUn3S/rbbHqNpHuz4/2Xkrr8aJKZJWmRpNsl/ZukRyS9oejHWdJ/yf5dPyTpLyTNLtpxlnSLpGckPZSbN+5xVer3sn1/UNJxP2OwEEEvqQrcBPwEcB7wXknnzWyppkUT+JWIOA+4BPhwtp/XAXdFxFrgrmy6aD4KPJKb/jTwuxHxKmAf8IEZKdX0+TzwDxHxQ8APk+57YY+zpBXAR4C+iHgtUAWuonjH+Y+B9WPmTXRcfwJYm/1cA3zheD+0EEEPrAN2RMTOiBgCNgEbZrhMXRcRT0XEd7LXL5L+519Buq+3ZqvdCrxrRgo4TSStBN4JfCmbFvA24PZslULts6TTgLcAfwgQEUMR8TwFP86kjzadI6kGzAWeomDHOSL+Cdg7ZvZEx3UD8CeRugdYJOmM4/ncogT9CmBXbnp3Nq+wJK0GLgLuBZZHxFPZoqeB5TNVrmnyOeBXgSSbXgo8HxHNbLpox3sNMAj8UdZc9SVJ8yjwcY6IPcDvAE+SBvwLwFaKfZzbJjquXcu1ogR9qUiaD3wV+OWI2J9fFml/2cL0mZX0k8AzEbF1pstyEtWA1wNfiIiLgAOMaaYp4HFeTFqDXQO8EpjH0U0chTddx7UoQb8HWJWbXpnNKxxJPaQh/+cR8bVs9g/aX+my38/MVPmmwZuAKyV9j7RJ7m2k7deLsq/4ULzjvRvYHRH3ZtO3kwZ/kY/zZcATETEYEQ3ga6THvsjHuW2i49q1XCtK0G8B1mZX6OukF3H6Z7hMXZe1Tf8h8EhE3Jhb1A9cnb2+Gvj6yS7bdImI6yNiZUSsJj2u/ycifgb4JvDT2WpF2+engV2SXp3N+nFgGwU+zqRNNpdImpv9O2/vc2GPc85Ex7Uf+E9Z75tLgBdyTTzHJiIK8QO8A3gUeBz4+EyXZ5r28UdJv9Y9CDyQ/byDtM36LuAx4BvAkpku6zTt/6XA32avzwbuA3YAXwFmzXT5uryvFwID2bH+a2Bx0Y8z8N+BfwMeAv4UmFW04wz8Bek1iAbpN7cPTHRcAZH2Jnwc+C5pj6Tj+lwPgWBmVnBFaboxM7MJOOjNzArOQW9mVnAOejOzgnPQm5kVnIPezKzgHPRmZgX3/wEUIn7BZZlKoAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(res_listInFW)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuclear norm: 0.5632052043683061\n",
      "Data shape: (200, 400)\n",
      "Number of observed values: 14942\n",
      "Rank of the matrix: 1\n",
      "Minimum and maximum values: 0.05306863232623509 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n = 400\n",
    "m = 200\n",
    "r = 10\n",
    "rho = 0.10\n",
    "SNR = 5\n",
    "delta = 3.75\n",
    "\n",
    "\n",
    "\n",
    "# taking data\n",
    "U = sparse.random(m, r, density=0.1, format='csr', data_rvs=None)\n",
    "V = sparse.random(r, n, density=0.1, format='csr', data_rvs=None)\n",
    "E = sparse.random(m, n, density=0.1, format='csr', data_rvs=None)\n",
    "\n",
    "\n",
    "\n",
    "VT = V.transpose(copy=True)\n",
    "\n",
    "UVT = U*V\n",
    "#print(UVT. shape)\n",
    "\n",
    "w1 = 1/(sparse.linalg.norm(UVT, ord='fro'))\n",
    "\n",
    "w2 = 1/(SNR*sparse.linalg.norm(E, ord='fro'))\n",
    "\n",
    "#Finally observed data matrix is:\n",
    "X_test = w1*UVT + w2*E\n",
    "\n",
    "# Non zero values\n",
    "idx_ratings = np.argwhere(X_test != 0.)\n",
    "idx_rows = idx_ratings[:,0]\n",
    "idx_cols = idx_ratings[:,1]\n",
    "\n",
    "# Nuclear norm of the test set\n",
    "rank = np.linalg.matrix_rank(X_test)\n",
    "U_thin, s_thin, Vh_thin = sparse.linalg.svds(X_test, k = rank, which='LM')\n",
    "nuc_norm = s_thin.sum()\n",
    "\n",
    "# Print some info about the generated data\n",
    "print('Nuclear norm:', nuc_norm)\n",
    "print('Data shape:', np.shape(X_test))\n",
    "print('Number of observed values:', len(idx_rows))\n",
    "print('Rank of the matrix:', rank)\n",
    "print('Minimum and maximum values:', X_test.max(), X_test.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 20.710085461897688 Loss diff: 20.178013507822605 Rank(Z):  1\n",
      "Iteration: 1 Loss: 8.136005495734203 Loss diff: 12.574079966163485 Rank(Z):  2\n",
      "Iteration: 10 Loss: 0.5841890868123718 Loss diff: 0.5057625973947553 Rank(Z):  11\n",
      "Iteration: 20 Loss: 0.38644839266838615 Loss diff: 0.12540854574466803 Rank(Z):  21\n",
      "Iteration: 30 Loss: 0.27455173635972413 Loss diff: 0.039281464643407704 Rank(Z):  31\n",
      "Iteration: 40 Loss: 0.14399512309815454 Loss diff: 0.02499449833787848 Rank(Z):  41\n",
      "Iteration: 50 Loss: 0.09198025468452456 Loss diff: 0.0028837977552780697 Rank(Z):  51\n",
      "Iteration: 60 Loss: 0.07665822248490584 Loss diff: 0.013324781893819443 Rank(Z):  61\n",
      "Iteration: 70 Loss: 0.07333132593891507 Loss diff: 0.0006173570934196876 Rank(Z):  71\n",
      "Iteration: 80 Loss: 0.052422250273545315 Loss diff: 0.0017050065465505915 Rank(Z):  81\n",
      "Iteration: 90 Loss: 0.04357177196057917 Loss diff: 0.0007945935581365055 Rank(Z):  91\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 750 ms\n"
     ]
    }
   ],
   "source": [
    "%time pred_ratings, loss, loss_track = FrankWolfe(X_test, FW_objective_function, delta = 10, max_iter=100, patience=1e-7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_track)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1\n",
      "Do regular FW step\n",
      "Iteration: 1 Loss: 0.5853282793478831 Loss diff: 49.525331316145206 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 2 Loss: 0.3537519534474485 Loss diff: 0.23157632590043464 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 3 Loss: 0.45288475571449016 Loss diff: 0.09913280226704169 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 4 Loss: 0.4304950862546344 Loss diff: 0.022389669459855766 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 5 Loss: 0.4321174627365453 Loss diff: 0.001622376481910881 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 6 Loss: 0.43225871872437643 Loss diff: 0.000141255987831157 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 7 Loss: 0.43228699437825624 Loss diff: 2.8275653879805684e-05 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 8 Loss: 0.4322951641290038 Loss diff: 8.169750747577531e-06 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 9 Loss: 0.43229810535310204 Loss diff: 2.941224098218509e-06 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 10 Loss: 0.43229933533377773 Loss diff: 1.229980675698794e-06 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 11 Loss: 0.4322999093266066 Loss diff: 5.73992828878378e-07 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 12 Loss: 0.43230020073842723 Loss diff: 2.914118206187588e-07 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 13 Loss: 0.43230035893328034 Loss diff: 1.5819485310597514e-07 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 14 Loss: 0.4323004496315471 Loss diff: 9.069826678587134e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 15 Loss: 0.4323005040504303 Loss diff: 5.441888317747612e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 16 Loss: 0.4323005379821563 Loss diff: 3.393172598453731e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 17 Loss: 0.4323005598492383 Loss diff: 2.1867082033999452e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 18 Loss: 0.43230057435054714 Loss diff: 1.4501308820413783e-08 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 19 Loss: 0.4323005842114248 Loss diff: 9.860877669964907e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 20 Loss: 0.4323005910670745 Loss diff: 6.8556497145877415e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 21 Loss: 0.4323005959283478 Loss diff: 4.861273272815936e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 22 Loss: 0.43230059943691534 Loss diff: 3.5085675476054234e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 23 Loss: 0.43230060200986214 Loss diff: 2.57294680006126e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 24 Loss: 0.4323006039241327 Loss diff: 1.914270575387178e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 25 Loss: 0.432300605367197 Loss diff: 1.4430642791829484e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 26 Loss: 0.4323006064682006 Loss diff: 1.1010036216063668e-09 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 27 Loss: 0.4323006073175455 Loss diff: 8.493448722113328e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 28 Loss: 0.43230060797944825 Loss diff: 6.619027548282475e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 29 Loss: 0.4323006085001447 Loss diff: 5.20696430417189e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 30 Loss: 0.4323006089133422 Loss diff: 4.131975317456238e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 31 Loss: 0.4323006092439 Loss diff: 3.305578033518941e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 32 Loss: 0.4323006095103494 Loss diff: 2.664493625736952e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 33 Loss: 0.4323006097266435 Loss diff: 2.1629414925783408e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 34 Loss: 0.4323006099033866 Loss diff: 1.7674306462822642e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 35 Loss: 0.43230061004870857 Loss diff: 1.453219766744951e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 36 Loss: 0.43230061016889376 Loss diff: 1.20185195129352e-10 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 37 Loss: 0.43230061026883704 Loss diff: 9.994327587747875e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 38 Loss: 0.4323006103523794 Loss diff: 8.354233971274994e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 39 Loss: 0.43230061042255497 Loss diff: 7.017558756317044e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 40 Loss: 0.43230061048177626 Loss diff: 5.922129453495018e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 41 Loss: 0.43230061053197333 Loss diff: 5.0197068723889515e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 42 Loss: 0.43230061057469915 Loss diff: 4.2725822879674524e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 43 Loss: 0.43230061061121033 Loss diff: 3.651118296588152e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 44 Loss: 0.4323006106425288 Loss diff: 3.1318447835104735e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 45 Loss: 0.4323006106694899 Loss diff: 2.696110001920715e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 46 Loss: 0.43230061069277964 Loss diff: 2.3289759010225453e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 47 Loss: 0.4323006107129641 Loss diff: 2.018446521034889e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 48 Loss: 0.4323006107305122 Loss diff: 1.7548074104922762e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 49 Loss: 0.4323006107458143 Loss diff: 1.530209292610607e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 50 Loss: 0.4323006107591959 Loss diff: 1.3381629138109474e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 51 Loss: 0.4323006107709306 Loss diff: 1.1734724303380517e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 52 Loss: 0.4323006107812483 Loss diff: 1.0317691145900199e-11 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 53 Loss: 0.4323006107903432 Loss diff: 9.094891506578051e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 54 Loss: 0.4323006107983797 Loss diff: 8.03651589720289e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 55 Loss: 0.43230061080549775 Loss diff: 7.118028388930497e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 56 Loss: 0.43230061081181664 Loss diff: 6.31888985580531e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 57 Loss: 0.4323006108174382 Loss diff: 5.621558774038249e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 58 Loss: 0.43230061082245 Loss diff: 5.011824288914113e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 59 Loss: 0.4323006108269273 Loss diff: 4.4772519025571e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 60 Loss: 0.43230061083093474 Loss diff: 4.007461029686965e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 61 Loss: 0.4323006108345284 Loss diff: 3.5936809084091692e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 62 Loss: 0.43230061083775717 Loss diff: 3.2287506002148803e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 63 Loss: 0.43230061084066296 Loss diff: 2.9057867223514222e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 64 Loss: 0.43230061084328275 Loss diff: 2.619793271207982e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 65 Loss: 0.4323006108456484 Loss diff: 2.3656632208712836e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 66 Loss: 0.43230061084778815 Loss diff: 2.139732835360064e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 67 Loss: 0.43230061084972643 Loss diff: 1.9382828675418295e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 68 Loss: 0.43230061085148497 Loss diff: 1.7585377598550167e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 69 Loss: 0.43230061085308275 Loss diff: 1.597777465889294e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 70 Loss: 0.4323006108545365 Loss diff: 1.4537815395954112e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 71 Loss: 0.432300610855861 Loss diff: 1.3244960683778118e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 72 Loss: 0.4323006108570694 Loss diff: 1.2083667400020204e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 73 Loss: 0.43230061085817323 Loss diff: 1.1038392422335619e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 74 Loss: 0.4323006108591829 Loss diff: 1.0096923297453486e-12 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 75 Loss: 0.43230061086010757 Loss diff: 9.246492460590616e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 76 Loss: 0.4323006108609554 Loss diff: 8.478218127550008e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 77 Loss: 0.43230061086173355 Loss diff: 7.781553179597722e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 78 Loss: 0.4323006108624488 Loss diff: 7.152611836147571e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 79 Loss: 0.4323006108631068 Loss diff: 6.57973675544099e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 80 Loss: 0.43230061086371274 Loss diff: 6.059597268404104e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 81 Loss: 0.43230061086427146 Loss diff: 5.5871973714261e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 82 Loss: 0.432300610864787 Loss diff: 5.155320614846914e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 83 Loss: 0.43230061086526334 Loss diff: 4.763411887154234e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 84 Loss: 0.43230061086570387 Loss diff: 4.405364961712621e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 85 Loss: 0.43230061086611155 Loss diff: 4.076738946423575e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 86 Loss: 0.4323006108664892 Loss diff: 3.77642361826247e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 87 Loss: 0.43230061086683946 Loss diff: 3.502753642692369e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 88 Loss: 0.43230061086716454 Loss diff: 3.2507330161024584e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 89 Loss: 0.43230061086746646 Loss diff: 3.019251515468113e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 90 Loss: 0.43230061086774707 Loss diff: 2.806088694740083e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 91 Loss: 0.4323006108680082 Loss diff: 2.611244553918368e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 92 Loss: 0.43230061086825144 Loss diff: 2.432498646953718e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 93 Loss: 0.4323006108684781 Loss diff: 2.266520304772257e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 94 Loss: 0.43230061086868943 Loss diff: 2.1133095273739855e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 95 Loss: 0.4323006108688867 Loss diff: 1.9728663147589032e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 96 Loss: 0.432300610869071 Loss diff: 1.8429702208777599e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 97 Loss: 0.4323006108692432 Loss diff: 1.7219559111936178e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 98 Loss: 0.4323006108694043 Loss diff: 1.6109336087311021e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 99 Loss: 0.43230061086955507 Loss diff: 1.5076828674409626e-13 Rank(Z):  1\n",
      "Do regular FW step\n",
      "Iteration: 100 Loss: 0.4323006108696963 Loss diff: 1.412203687323199e-13 Rank(Z):  1\n",
      "CPU times: total: 5.05 s\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%time pred_ratings, loss, loss_track = FW_inface(X_test, FW_objective_function, delta = 15, THRES=10000, max_iter=100, patience=1e-7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(res_listInFW)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwQyN3Cy_uF5"
   },
   "source": [
    "# Tuned Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def FW_inface_gabri(X, objective_function, delta, gamma1=0, gamma2=1, THRES=0.001, Z_init=None, max_iter=150,\n",
    "                    patience=1e-3, printing=True):\n",
    "    '''\n",
    "    :param X: sparse matrix with ratings and 'empty values', rows - users, columns - books.\n",
    "    :param objective_function: objective function that we would like to minimize with FW.\n",
    "    :param Z_init: In case we want to initialize Z with a known matrix, if not given Z_init will be a zeros matrix.\n",
    "    :param max_iter: max number of iterations for the method.\n",
    "    :param patience: once reached this tolerance provide the result.\n",
    "    :return: Z: matrix of predicted ratings - it should be like X but with no 'empty values'\n",
    "            loss: difference between original values (X) and predicted ones (Z).\n",
    "    '''\n",
    "\n",
    "    # Get X indexes for not empty values\n",
    "    idx_ratings = np.argwhere(X != 0)\n",
    "    #idx_ratings = np.argwhere(~np.isnan(X))\n",
    "    idx_rows = idx_ratings[:, 0]\n",
    "    idx_cols = idx_ratings[:, 1]\n",
    "\n",
    "    # choose an appropriate delta\n",
    "\n",
    "    # Initialize Z_{-1}\n",
    "    if Z_init is not None:\n",
    "        Z = Z_init\n",
    "    else:\n",
    "        Z = np.zeros(X.shape)\n",
    "\n",
    "    # Create vectors with the not empty features of the sparse matrix\n",
    "    X_rated = X[idx_rows, idx_cols]\n",
    "    Z_rated = Z[idx_rows, idx_cols]\n",
    "    diff_vec = Z_rated - X_rated\n",
    "    diff_vec = np.array(diff_vec)[0]\n",
    "\n",
    "    # Initial gradient and Z0\n",
    "    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "    u_max, s_max, v_max = sparse.linalg.svds(grad, k=1, which='LM')\n",
    "    Zk = -delta * np.outer(u_max, v_max)\n",
    "    Z_rated = Zk[idx_rows, idx_cols]\n",
    "\n",
    "    print('Initial Zk Rank: ', LA.matrix_rank(Zk))\n",
    "\n",
    "    # Initialize lower bound on the optimal objective function (f*)\n",
    "    diff_vec = Z_rated - X_rated\n",
    "    diff_vec = np.array(diff_vec)[0]\n",
    "    new_low_bound = np.max((objective_function(diff_vec) + np.multiply(diff_vec, Z_rated)), 0).sum()\n",
    "    #new_low_bound = 0 #used 0 otherwise the other new_low_bound was too high!\n",
    "\n",
    "    # Set L and D constants\n",
    "    L = 1\n",
    "    D = 2 * delta\n",
    "\n",
    "    # Compute first iteration thin SVD\n",
    "    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "    r_grad = LA.matrix_rank(grad)  # Compute rank of the gradient sparse matrix to find thin SVD size\n",
    "    U_thin, D_thin, Vh_thin = sparse.linalg.svds(grad, k=r_grad,\n",
    "                                                 which='LM')  # Compute k = rank singular values # replaced r_grad with 1\n",
    "    # U_app, D_app, Vh_app = sparse.linalg.svds(grad, k = 1, which='SM')\n",
    "    #U_thin, D_thin, Vh_thin = LA.svd(grad.toarray())\n",
    "    #D_thin = D_thin.T\n",
    "\n",
    "    # Additional needed parameters\n",
    "    diff_objective = patience + 1\n",
    "    objective = objective_function(diff_vec)\n",
    "    it = 0\n",
    "    res_list = [objective]\n",
    "\n",
    "    while (diff_objective > patience) and (it < max_iter):\n",
    "\n",
    "        # Lower bound update\n",
    "        low_bound = new_low_bound\n",
    "\n",
    "        # In-face direction with the away step strategy: two calculations depending of where Z lies within the feasible set\n",
    "        if D_thin.sum() <= delta:  # Z in border (sum of singular values == radious of feasible set)\n",
    "            G = 0.5 * (Vh_thin.dot(grad.T.dot(U_thin)) + U_thin.T.dot(grad.dot(Vh_thin.T)))\n",
    "\n",
    "            ''' WE NEEDED NOT ONLY THE SMALLEST EIGENVALUE, BUT THE EIGENVECOTR ASSOCIATED WITH THE SMALLEST VALUE\n",
    "            u = sparse.linalg.eigs(G, k = 1, which = 'SM')#unitary eigenvector corresponding to smallest eigenvalue of G\n",
    "            '''\n",
    "\n",
    "            print('Zk in border!')\n",
    "            eigvalues, eigvectors = LA.eig(G)  #find the eigenvalues\n",
    "            min_eig = np.argmin(eigvalues)  #find the index of the smallest eigenvalue\n",
    "            u = eigvectors[:, min_eig]  #take the eigenvector corresponding to the smallest eigenvalue\n",
    "            M = np.outer(u, u.T)\n",
    "            Zk_tilde = delta * U_thin.dot(M).dot(Vh_thin)\n",
    "            Dk = Zk - Zk_tilde\n",
    "            alpha_B = 0.5\n",
    "\n",
    "            ''' DOESN'T WORK <-- To solve!\n",
    "            alpha_B = scipy.linalg.inv(delta*u.T.dot(scipy.linalg.inv(Dk)).dot(u)-1)\n",
    "            '''\n",
    "\n",
    "        else:  #inside\n",
    "\n",
    "            idx_max_s = np.argmax(D_thin)\n",
    "            Zk_tilde = delta * np.outer(U_thin[idx_max_s, :], Vh_thin[idx_max_s, :])\n",
    "            Dk = Zk - Zk_tilde\n",
    "            #BINARY SEARCH (xd)\n",
    "            '''\n",
    "            alpha_B = alpha_binary_search(Zk,\n",
    "                                          Dk,\n",
    "                                          delta)\n",
    "            '''\n",
    "            alpha_B = 0.5\n",
    "\n",
    "        nuclear_norm = D_thin.sum()\n",
    "\n",
    "        U = nuclear_norm * D_thin  # standardize the simplex\n",
    "        r = D_thin.shape[0]  # added the index so it could be compared to a number\n",
    "        no_obs = idx_rows.shape[0]  # <-- is this thing useless? - Gabri\n",
    "        # THRES = 0.001\n",
    "\n",
    "        #print('Nuclear Norm:  ' + str(nuclear_norm))\n",
    "\n",
    "        #if abs(delta - nuclear_norm) < THRES and r > 1:\n",
    "        Z_B = Zk + alpha_B * Dk\n",
    "        diff_vec_B = Z_B[idx_rows, idx_cols] - X_rated\n",
    "        diff_vec_B = np.array(diff_vec_B)[0].sum()\n",
    "        beta = alpha_B / 5  # FIND A GOOD VALUE -- a binary search is also suggested by the paper xdd\n",
    "\n",
    "        Z_A = Zk + beta * Dk\n",
    "        diff_vec_A = Z_A[idx_rows, idx_cols] - X_rated\n",
    "        diff_vec_A = np.array(diff_vec_A)[0].sum()\n",
    "\n",
    "        # if abs(delta - nuclear_norm) < THRES and r > 1:\n",
    "        if 1 / (objective_function(diff_vec_B) - low_bound) >= (\n",
    "                1 / (objective - low_bound) + gamma1 / (2 * L * D ** 2)):\n",
    "\n",
    "            # 1. Move to a lower dimensional face\n",
    "            print('Went to a lower-dimensional face')\n",
    "            Zk = Z_B\n",
    "\n",
    "        elif 1 / (objective_function(diff_vec_A) - low_bound) >= (\n",
    "                1 / (objective - low_bound) + gamma2 / (2 * L * D ** 2)):\n",
    "\n",
    "            # 2. Stay in the current face\n",
    "            print('Stay in the current face')\n",
    "            Zk = Z_A\n",
    "\n",
    "            # else:\n",
    "            #   raise 'Error'\n",
    "\n",
    "        else:\n",
    "\n",
    "            # 3. Do a regular FW step and update the lower bound\n",
    "            print('Do regular FW step')\n",
    "\n",
    "            #Zk update\n",
    "            idx_max_s = np.argmax(D_thin)\n",
    "            Zk_tilde = -delta * np.outer(U_thin[idx_max_s, :],\n",
    "                                         Vh_thin[:, idx_max_s])  # Am i selecting right the vectors??\n",
    "            alpha_k = 2 / (it + 2)\n",
    "            Zk = (1 - alpha_k) * Z + alpha_k * Zk_tilde\n",
    "\n",
    "            #print('Zk FW step: ', Zk[1,:10])\n",
    "\n",
    "            # Lower bound update\n",
    "            direction_vec = Zk_tilde.flatten() - Zk.flatten()\n",
    "\n",
    "            grad = grad.toarray()  # this method converts the sparse matrix into a numpy array!\n",
    "\n",
    "            wolfe_gap = grad.T.flatten() * direction_vec  #added the flatten otherwise you can't do the operation\n",
    "            B_w = objective + wolfe_gap.sum()\n",
    "            #new_low_bound = np.max(low_bound, B_w)   # gave problems during the execution: wanted both numbers as integers??\n",
    "\n",
    "            ''' TRIED THIS INSTEAD '''\n",
    "            if low_bound.sum() >= B_w:\n",
    "                new_low_bound = low_bound\n",
    "            else:\n",
    "                new_low_bound = B_w\n",
    "\n",
    "        # Loss\n",
    "        diff_vec = Zk[idx_rows, idx_cols] - X_rated\n",
    "        diff_vec = np.array(diff_vec)[0]\n",
    "        new_objective = objective_function(diff_vec)\n",
    "\n",
    "        # Improvement at this iteration\n",
    "        diff_objective = np.abs(objective - new_objective)\n",
    "        objective = new_objective\n",
    "\n",
    "        # Gradient\n",
    "        grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "\n",
    "        # Thin SVD\n",
    "        r_grad = LA.matrix_rank(grad)  # Compute rank of the gradient sparse matrix to find thin SVD size\n",
    "        U_thin, D_thin, Vh_thin = sparse.linalg.svds(grad, k=r_grad,\n",
    "                                                     which='LM')  # Compute k = rank singular values # replaced r_grad with 1\n",
    "        #U_thin, D_thin, Vh_thin = LA.svd(grad.toarray())\n",
    "        #D_thin = D_thin.T\n",
    "\n",
    "        # Count iteration\n",
    "        it += 1\n",
    "\n",
    "        res_list.append(objective)\n",
    "\n",
    "        if printing == True:\n",
    "            if it % 5 == 0 or it == 1:\n",
    "                print(D_thin.sum())\n",
    "                print('Iteration: ', it, 'f(Z_k): ', objective, 'f(Z_{k-1}) - f(Z_k): ', diff_objective,\n",
    "                      ' Rank of Zk: ', LA.matrix_rank(Zk))\n",
    "                print(''' ''')\n",
    "\n",
    "    return Z, objective, it, res_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(res_listFW, label = 'Frank Wolfe')\n",
    "plt.plot(res_listInFW, label = 'In-Face Frank Wolfe')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0manG1buepT"
   },
   "source": [
    "# Grid Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_MUb9nrwJyA"
   },
   "source": [
    "##Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUZGgSUsudah",
    "outputId": "bd7fb5f1-3a5b-4467-e5cc-23f3784569fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 0.001    ------     Final Loss: 1995299.6906607011   at iteration 2\n",
      "Delta: 0.01    ------     Final Loss: 1995296.9066038157   at iteration 2\n",
      "Delta: 0.1    ------     Final Loss: 1995269.065718345   at iteration 2\n",
      "Delta: 1    ------     Final Loss: 1994990.6592616318   at iteration 4\n",
      "Delta: 10    ------     Final Loss: 1992207.8429168023   at iteration 19\n",
      "Delta: 100    ------     Final Loss: 1964496.978598407   at iteration 84\n",
      "Delta: 1000    ------     Final Loss: 1699162.6623283497   at iteration 201\n",
      "Delta: 10000    ------     Final Loss: 302150.77467486466   at iteration 64\n",
      "Delta: 100000    ------     Final Loss: 175474.63085276185   at iteration 70\n",
      "Delta: 1000000    ------     Final Loss: 146671.28662048402   at iteration 201\n",
      "Delta: 10000000    ------     Final Loss: 672843.1813172555   at iteration 201\n",
      "Delta: 100000000    ------     Final Loss: 1757112480.4694064   at iteration 201\n"
     ]
    }
   ],
   "source": [
    "deltas = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter=201, patience = 0.01, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--BkRQqIxmSw",
    "outputId": "f9c17983-9fb7-40c8-dc83-c9d8c099cf3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 250000    ------     Final Loss: 184966.58801373874   at iteration 142\n",
      "Delta: 500000    ------     Final Loss: 169552.51866399613   at iteration 201\n",
      "Delta: 750000    ------     Final Loss: 420615.6132565984   at iteration 201\n",
      "Delta: 1250000    ------     Final Loss: 216412.31880627264   at iteration 201\n",
      "Delta: 1500000    ------     Final Loss: 273680.42343568715   at iteration 201\n",
      "Delta: 2500000    ------     Final Loss: 417712.4612960932   at iteration 201\n",
      "Delta: 5000000    ------     Final Loss: 571545.4612269908   at iteration 201\n",
      "Delta: 7500000    ------     Final Loss: 632556.1542439449   at iteration 201\n"
     ]
    }
   ],
   "source": [
    "deltas = [250000,\n",
    "          500000,\n",
    "          750000,\n",
    "          1250000,\n",
    "          1500000,\n",
    "          2500000,\n",
    "          5000000,\n",
    "          7500000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter=201, patience = 0.01, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5Xei-9Gy3hh",
    "outputId": "47a9fd59-b0a5-4bca-ab08-992677a90c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 150000    ------     Final Loss: 180502.69121015765   at iteration 95\n",
      "Delta: 300000    ------     Final Loss: 186151.14181067253   at iteration 165\n",
      "Delta: 450000    ------     Final Loss: 188191.3045520871   at iteration 231\n",
      "Delta: 600000    ------     Final Loss: 189243.6150128576   at iteration 297\n",
      "Delta: 900000    ------     Final Loss: 190318.32554160611   at iteration 425\n",
      "Delta: 1200000    ------     Final Loss: 189679.94593613478   at iteration 501\n",
      "Delta: 1500000    ------     Final Loss: 149600.4326080835   at iteration 501\n"
     ]
    }
   ],
   "source": [
    "deltas = [150000,\n",
    "          300000,\n",
    "          450000,\n",
    "          600000,\n",
    "          900000,\n",
    "          1200000,\n",
    "          1500000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter=501, patience = 0.01, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2W5PqtaH0eZu",
    "outputId": "c519e67f-c98e-4186-a5a2-8d0d6a031872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 1750000    ------     Final Loss: 191384.43819455765   at iteration 786\n",
      "Delta: 2000000    ------     Final Loss: 191527.2106760225   at iteration 889\n",
      "Delta: 2250000    ------     Final Loss: 191638.53051798942   at iteration 991\n"
     ]
    }
   ],
   "source": [
    "deltas = [1750000,\n",
    "          2000000,\n",
    "          2250000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter = 1000, patience = 0.001, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkpvJqQi2YxJ",
    "outputId": "c4eeadcf-f4a5-4873-94aa-33a40b766bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 1375000    ------     Final Loss: 191074.29727334835   at iteration 630\n",
      "Delta: 1500000    ------     Final Loss: 191194.6863524153   at iteration 682\n",
      "Delta: 1625000    ------     Final Loss: 191296.77299220837   at iteration 734\n"
     ]
    }
   ],
   "source": [
    "deltas = [1375000,\n",
    "          1500000,\n",
    "          1625000]\n",
    "\n",
    "for delta in deltas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, delta = delta, max_iter = 1000, patience = 0.001, printing = False)\n",
    "  print('Delta: ' + str(delta) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO5uv6UZ33wJ"
   },
   "source": [
    "## Gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45FR0xoX323p",
    "outputId": "51369768-d4dc-4455-f573-83822546ef54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma1: 0   Gamma2: 0.1    ------     Final Loss: 335449.4737096828   at iteration 500\n",
      "Gamma1: 0   Gamma2: 1    ------     Final Loss: 335449.4737096833   at iteration 500\n",
      "Gamma1: 0.1   Gamma2: 1    ------     Final Loss: 149460.45148459333   at iteration 500\n",
      "Gamma1: 1   Gamma2: 1    ------     Final Loss: 149460.451484515   at iteration 500\n",
      "Gamma1: 1   Gamma2: 10    ------     Final Loss: 335449.47370968235   at iteration 500\n",
      "Gamma1: 1   Gamma2: 100    ------     Final Loss: 149460.4514845148   at iteration 500\n",
      "Gamma1: 10   Gamma2: 100    ------     Final Loss: 149460.451484515   at iteration 500\n"
     ]
    }
   ],
   "source": [
    "gammas = [[0,    0.1],\n",
    "          [0,    1],\n",
    "          [0.1,  1],\n",
    "          [1,    1],\n",
    "          [1,   10],\n",
    "          [1,  100],\n",
    "          [10, 100]]\n",
    "\n",
    "for gamma1, gamma2 in gammas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, gamma1 = gamma1, gamma2 = gamma2, delta = 1500000, max_iter = 500, patience = 0.001, printing = False)\n",
    "  print('Gamma1: ' + str(gamma1) + '   Gamma2: ' + str(gamma2) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOD4CGHO884Z",
    "outputId": "f4ddeb2d-b605-4c9c-fab8-d096648bc3e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma1: 1   Gamma2: 100    ------     Final Loss: 149460.45148459318   at iteration 500\n",
      "Gamma1: 100   Gamma2: 100    ------     Final Loss: 149460.45148451466   at iteration 500\n",
      "Gamma1: 0.1   Gamma2: 100    ------     Final Loss: 149460.4514845149   at iteration 500\n",
      "Gamma1: 1   Gamma2: 1000    ------     Final Loss: 335449.4737115356   at iteration 500\n"
     ]
    }
   ],
   "source": [
    "gammas = [[1,  100],\n",
    "          [100, 100],\n",
    "          [0.1,  100],\n",
    "          [1,    1000]]\n",
    "\n",
    "for gamma1, gamma2 in gammas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, gamma1 = gamma1, gamma2 = gamma2, delta = 1500000, max_iter = 500, patience = 0.001, printing = False)\n",
    "  print('Gamma1: ' + str(gamma1) + '   Gamma2: ' + str(gamma2) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMpXvq5S-ZvP",
    "outputId": "ac623233-b516-433e-a7f7-041981b1ed94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma1: 0   Gamma2: 100    ------     Final Loss: 335449.47371153446   at iteration 500\n",
      "Gamma1: 0   Gamma2: 1000    ------     Final Loss: 149460.4514845933   at iteration 500\n",
      "Gamma1: 0.1   Gamma2: 0.1    ------     Final Loss: 149460.45148451495   at iteration 500\n"
     ]
    }
   ],
   "source": [
    "gammas = [[0,  100],\n",
    "          [0, 1000],\n",
    "          [0.1,  0.1]]\n",
    "\n",
    "for gamma1, gamma2 in gammas:\n",
    "  pred_ratings, loss, it = FW_inface(new_data, FW_objective_function, gamma1 = gamma1, gamma2 = gamma2, delta = 1500000, max_iter = 500, patience = 0.001, printing = False)\n",
    "  print('Gamma1: ' + str(gamma1) + '   Gamma2: ' + str(gamma2) + '    ------     Final Loss: ' + str(loss) + '   at iteration ' + str(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpS5jRz4vpHa"
   },
   "source": [
    "## Sub-Chapter"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FW_GoodReads_recommender - Copia.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}