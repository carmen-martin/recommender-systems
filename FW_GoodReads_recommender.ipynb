{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9fwkajk0JFu",
    "outputId": "4f2c9a27-2a9e-4f0e-d207-834d493497b0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import scipy.linalg\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65ARbMD3vBTq"
   },
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Vm7PYy7xNGZ"
   },
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ECj6WKmZ0XzI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "0Yt6f1OK0n0b"
   },
   "outputs": [],
   "source": [
    "#path = '/content/drive/MyDrive/O4DS - Project Work/goodreads_cleaned.csv'\n",
    "path = 'DATA/goodreads_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "XPZR6p810sja",
    "outputId": "fd62773d-42bc-4d82-8d5b-98d8b239d281"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                 user_id   book_id  rating\n0       8842281e1d1347389f2ab93d60773d4d  18245960       5\n1       8842281e1d1347389f2ab93d60773d4d     16981       3\n2       8842281e1d1347389f2ab93d60773d4d  28684704       3\n3       8842281e1d1347389f2ab93d60773d4d  27161156       0\n4       8842281e1d1347389f2ab93d60773d4d  25884323       4\n...                                  ...       ...     ...\n899995  b9450d1c1f97f891c392b1105959b56e  11832081       3\n899996  b9450d1c1f97f891c392b1105959b56e  16095092       3\n899997  b9450d1c1f97f891c392b1105959b56e   8430896       4\n899998  b9450d1c1f97f891c392b1105959b56e  12275680       4\n899999  b9450d1c1f97f891c392b1105959b56e     17005       3\n\n[900000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>18245960</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>16981</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>28684704</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>27161156</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>25884323</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>899995</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>11832081</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>899996</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>16095092</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>899997</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>8430896</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>899998</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>12275680</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>899999</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>17005</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>900000 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path, sep = \";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPE9r1Q4xT1Z"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBrJsOa1nmZl",
    "outputId": "ecc94afc-d4a0-43b4-cb47-5ba63254ae1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "count    12188.000000\nmean        73.843124\nstd        103.860677\nmin          1.000000\n25%         14.000000\n50%         37.000000\n75%         92.000000\nmax       1815.000000\nName: user_id, dtype: float64"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user_id.value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH79waI4pO83",
    "outputId": "8b94587b-1d8a-4d9e-e553-c4dc1129dcb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "count    25474.000000\nmean        35.330141\nstd         67.222413\nmin          1.000000\n25%         10.000000\n50%         17.000000\n75%         34.000000\nmax       1734.000000\nName: book_id, dtype: float64"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.book_id.value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRNXYn4cxeG5"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Vbw2KbXRpvEK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "22ae7d43-feab-44e7-a7e0-650aa9430472"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "457.0"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['book_id_count'] = df.groupby('book_id')['book_id'].transform('count')\n",
    "df['user_id_count'] = df.groupby('user_id')['user_id'].transform('count')\n",
    "df.book_id_count.quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "NTP9FUQmsJeX"
   },
   "outputs": [],
   "source": [
    "book_quantile = 0.9\n",
    "user_quantile = 0.25\n",
    "\n",
    "df = df.loc[(df.book_id_count >= df.book_id.value_counts().quantile(book_quantile)) & (df.user_id_count >= df.user_id.value_counts().quantile(user_quantile)),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwiSPjEwmqL7",
    "outputId": "e3d093cf-eada-443d-800b-6b44e5ce2b36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(425794, 5)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMoKEdpWxnWc"
   },
   "source": [
    "## Data Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "p0BWaRDjApkr",
    "outputId": "910b163d-7f81-4e11-dfc9-9452eaf2e087"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "book_id                           1         2         3         5         \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           6         11        34        295       \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           320       343       350       662       \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           667       830       865       890       \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       1.0       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           902       930       960       968       \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           1103      1232      1420      1617      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       5.0       4.0       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           1618      1622      1845      1852      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       5.0       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           1885      1934      1953      2052      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           2156      2165      2187      2493      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           2526      2612      2623      2657      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           2744      2839      2956      2998      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           3008      3431      3473      3636      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       4.0   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           3682      3758      ...  30269126  30312891  \\\nuser_id                                               ...                       \n000a1016fda6008d1edbba720ca00851       NaN       NaN  ...       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN  ...       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       3.0       NaN  ...       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN  ...       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN  ...       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN  ...       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN  ...       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN  ...       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN  ...       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN  ...       NaN       NaN   \n\nbook_id                           30325011  30415154  30555488  30633337  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           30653853  30687916  30688435  30724132  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           30731416  30747137  30809689  30821598  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           30831912  30839185  30969741  31140847  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           31145133  31145148  31176886  31423196  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           31450752  31450852  31450908  31451174  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           31538614  31538635  31538647  31931941  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           31952703  32075662  32075671  32078787  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           32571395  32796253  32848471  33140405  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           33151805  33232571  33280872  33288638  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       4.0   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           33385229  33643994  34044126  34076952  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN   \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN   \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN   \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN   \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN   \n\nbook_id                           34273458  35247769  35404657  35504431  \nuser_id                                                                   \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN  \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN  \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN  \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN  \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN  \n00238d8a4c276c47f5d5e242f54a8f28       NaN       NaN       NaN       NaN  \n002a023d3de233b4bd3ec4fc3e9c581a       NaN       NaN       NaN       NaN  \n002e063d40ae0107a59d8f9c1aa7a423       NaN       NaN       NaN       NaN  \n005238c5743d61b58e49d5da089e43df       NaN       NaN       NaN       NaN  \n005a572f0bfe510fa796e6eadc6a2eb4       NaN       NaN       NaN       NaN  \n\n[10 rows x 2575 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>book_id</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>5</th>\n      <th>6</th>\n      <th>11</th>\n      <th>34</th>\n      <th>295</th>\n      <th>320</th>\n      <th>343</th>\n      <th>350</th>\n      <th>662</th>\n      <th>667</th>\n      <th>830</th>\n      <th>865</th>\n      <th>890</th>\n      <th>902</th>\n      <th>930</th>\n      <th>960</th>\n      <th>968</th>\n      <th>1103</th>\n      <th>1232</th>\n      <th>1420</th>\n      <th>1617</th>\n      <th>1618</th>\n      <th>1622</th>\n      <th>1845</th>\n      <th>1852</th>\n      <th>1885</th>\n      <th>1934</th>\n      <th>1953</th>\n      <th>2052</th>\n      <th>2156</th>\n      <th>2165</th>\n      <th>2187</th>\n      <th>2493</th>\n      <th>2526</th>\n      <th>2612</th>\n      <th>2623</th>\n      <th>2657</th>\n      <th>2744</th>\n      <th>2839</th>\n      <th>2956</th>\n      <th>2998</th>\n      <th>3008</th>\n      <th>3431</th>\n      <th>3473</th>\n      <th>3636</th>\n      <th>3682</th>\n      <th>3758</th>\n      <th>...</th>\n      <th>30269126</th>\n      <th>30312891</th>\n      <th>30325011</th>\n      <th>30415154</th>\n      <th>30555488</th>\n      <th>30633337</th>\n      <th>30653853</th>\n      <th>30687916</th>\n      <th>30688435</th>\n      <th>30724132</th>\n      <th>30731416</th>\n      <th>30747137</th>\n      <th>30809689</th>\n      <th>30821598</th>\n      <th>30831912</th>\n      <th>30839185</th>\n      <th>30969741</th>\n      <th>31140847</th>\n      <th>31145133</th>\n      <th>31145148</th>\n      <th>31176886</th>\n      <th>31423196</th>\n      <th>31450752</th>\n      <th>31450852</th>\n      <th>31450908</th>\n      <th>31451174</th>\n      <th>31538614</th>\n      <th>31538635</th>\n      <th>31538647</th>\n      <th>31931941</th>\n      <th>31952703</th>\n      <th>32075662</th>\n      <th>32075671</th>\n      <th>32078787</th>\n      <th>32571395</th>\n      <th>32796253</th>\n      <th>32848471</th>\n      <th>33140405</th>\n      <th>33151805</th>\n      <th>33232571</th>\n      <th>33280872</th>\n      <th>33288638</th>\n      <th>33385229</th>\n      <th>33643994</th>\n      <th>34044126</th>\n      <th>34076952</th>\n      <th>34273458</th>\n      <th>35247769</th>\n      <th>35404657</th>\n      <th>35504431</th>\n    </tr>\n    <tr>\n      <th>user_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>000a1016fda6008d1edbba720ca00851</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0011e1a9112b3d798702ef5b20bbf35b</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0019de4561419b7543238e0979f2f33e</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>00204424763e8233c5f53f0729f2304f</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>00214d8b0a020837cccf5f41eb563037</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>00238d8a4c276c47f5d5e242f54a8f28</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>002a023d3de233b4bd3ec4fc3e9c581a</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>002e063d40ae0107a59d8f9c1aa7a423</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>005238c5743d61b58e49d5da089e43df</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>005a572f0bfe510fa796e6eadc6a2eb4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 2575 columns</p>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.pivot_table(df, columns=\"book_id\", index=\"user_id\", values=\"rating\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "WHSkO2aLmqL9"
   },
   "source": [
    "## Convert to an array to work with the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "idzyac1lmqL9",
    "outputId": "c8942a07-f99a-4bfc-d623-ac580f8414ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "book_id                           1         2         3         5         \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       5.0       NaN       NaN   \n\nbook_id                           6         11        34        295       \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           320       343       350       662       \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           667       830       865       890       \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           902       930       960       968       \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           1103      1232      1420      1617      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           1618      1622      1845      1852      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           1885      1934      1953      2052      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           2156      2165      2187      2493      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           2526      2612      2623      2657      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           2744      2839      2956      2998      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           3008      3431      3473      3636      \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       2.0   \n\nbook_id                           3682      3758      ...  30269126  30312891  \\\nuser_id                                               ...                       \n000a1016fda6008d1edbba720ca00851       NaN       NaN  ...       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN  ...       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       3.0       NaN  ...       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN  ...       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN  ...       NaN       NaN   \n...                                    ...       ...  ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN  ...       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN  ...       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN  ...       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN  ...       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN  ...       NaN       NaN   \n\nbook_id                           30325011  30415154  30555488  30633337  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           30653853  30687916  30688435  30724132  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       4.0       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           30731416  30747137  30809689  30821598  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           30831912  30839185  30969741  31140847  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           31145133  31145148  31176886  31423196  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           31450752  31450852  31450908  31451174  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           31538614  31538635  31538647  31931941  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           31952703  32075662  32075671  32078787  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           32571395  32796253  32848471  33140405  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       3.0       NaN       NaN       NaN   \n\nbook_id                           33151805  33232571  33280872  33288638  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           33385229  33643994  34044126  34076952  \\\nuser_id                                                                    \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN   \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN   \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN   \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN   \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN   \n...                                    ...       ...       ...       ...   \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN   \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN   \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN   \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN   \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN   \n\nbook_id                           34273458  35247769  35404657  35504431  \nuser_id                                                                   \n000a1016fda6008d1edbba720ca00851       NaN       NaN       NaN       NaN  \n0011e1a9112b3d798702ef5b20bbf35b       NaN       NaN       NaN       NaN  \n0019de4561419b7543238e0979f2f33e       NaN       NaN       NaN       NaN  \n00204424763e8233c5f53f0729f2304f       NaN       NaN       NaN       NaN  \n00214d8b0a020837cccf5f41eb563037       NaN       NaN       NaN       NaN  \n...                                    ...       ...       ...       ...  \nfff3a250fbc018ad2c2c2d45c86734da       NaN       NaN       NaN       NaN  \nfff7bfd82b89fa347edfe9a82ac0c61b       NaN       NaN       NaN       NaN  \nfffc34d137f5c5c5e1ca1d6f325a4dcf       NaN       NaN       NaN       NaN  \nfffce7dae5ac5e8fb6288d81658ececc       NaN       NaN       NaN       NaN  \nffff7cafdaf5196383cb2efca08fb6fe       NaN       NaN       NaN       NaN  \n\n[9153 rows x 2575 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>book_id</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>5</th>\n      <th>6</th>\n      <th>11</th>\n      <th>34</th>\n      <th>295</th>\n      <th>320</th>\n      <th>343</th>\n      <th>350</th>\n      <th>662</th>\n      <th>667</th>\n      <th>830</th>\n      <th>865</th>\n      <th>890</th>\n      <th>902</th>\n      <th>930</th>\n      <th>960</th>\n      <th>968</th>\n      <th>1103</th>\n      <th>1232</th>\n      <th>1420</th>\n      <th>1617</th>\n      <th>1618</th>\n      <th>1622</th>\n      <th>1845</th>\n      <th>1852</th>\n      <th>1885</th>\n      <th>1934</th>\n      <th>1953</th>\n      <th>2052</th>\n      <th>2156</th>\n      <th>2165</th>\n      <th>2187</th>\n      <th>2493</th>\n      <th>2526</th>\n      <th>2612</th>\n      <th>2623</th>\n      <th>2657</th>\n      <th>2744</th>\n      <th>2839</th>\n      <th>2956</th>\n      <th>2998</th>\n      <th>3008</th>\n      <th>3431</th>\n      <th>3473</th>\n      <th>3636</th>\n      <th>3682</th>\n      <th>3758</th>\n      <th>...</th>\n      <th>30269126</th>\n      <th>30312891</th>\n      <th>30325011</th>\n      <th>30415154</th>\n      <th>30555488</th>\n      <th>30633337</th>\n      <th>30653853</th>\n      <th>30687916</th>\n      <th>30688435</th>\n      <th>30724132</th>\n      <th>30731416</th>\n      <th>30747137</th>\n      <th>30809689</th>\n      <th>30821598</th>\n      <th>30831912</th>\n      <th>30839185</th>\n      <th>30969741</th>\n      <th>31140847</th>\n      <th>31145133</th>\n      <th>31145148</th>\n      <th>31176886</th>\n      <th>31423196</th>\n      <th>31450752</th>\n      <th>31450852</th>\n      <th>31450908</th>\n      <th>31451174</th>\n      <th>31538614</th>\n      <th>31538635</th>\n      <th>31538647</th>\n      <th>31931941</th>\n      <th>31952703</th>\n      <th>32075662</th>\n      <th>32075671</th>\n      <th>32078787</th>\n      <th>32571395</th>\n      <th>32796253</th>\n      <th>32848471</th>\n      <th>33140405</th>\n      <th>33151805</th>\n      <th>33232571</th>\n      <th>33280872</th>\n      <th>33288638</th>\n      <th>33385229</th>\n      <th>33643994</th>\n      <th>34044126</th>\n      <th>34076952</th>\n      <th>34273458</th>\n      <th>35247769</th>\n      <th>35404657</th>\n      <th>35504431</th>\n    </tr>\n    <tr>\n      <th>user_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>000a1016fda6008d1edbba720ca00851</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0011e1a9112b3d798702ef5b20bbf35b</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0019de4561419b7543238e0979f2f33e</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>00204424763e8233c5f53f0729f2304f</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>00214d8b0a020837cccf5f41eb563037</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>fff3a250fbc018ad2c2c2d45c86734da</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>fff7bfd82b89fa347edfe9a82ac0c61b</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>fffc34d137f5c5c5e1ca1d6f325a4dcf</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>fffce7dae5ac5e8fb6288d81658ececc</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>ffff7cafdaf5196383cb2efca08fb6fe</th>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>9153 rows Ã— 2575 columns</p>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "va_I-ShImqL9",
    "outputId": "e9e5d405-d9f3-409b-ca8f-7305e6ce6564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan  5. nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "data_matrix = df.to_numpy(na_value=np.nan)\n",
    "print(data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khOoAYeMmqL9",
    "outputId": "7ad7670d-b53e-4fe6-9391-29a97572df1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  419]\n",
      " [   0  427]\n",
      " [   0  495]\n",
      " ...\n",
      " [9152 2452]\n",
      " [9152 2475]\n",
      " [9152 2559]]\n"
     ]
    }
   ],
   "source": [
    "# Check how to get the index of not empty values\n",
    "idx = np.argwhere(~np.isnan(data_matrix))\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4i6vLKb_mqL-",
    "outputId": "87d81808-11b9-4139-f3bc-ce718b1e5415"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1., 2., 3., 4., 5.])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data_matrix[idx[:,0], idx[:,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKX1GoN3vcYY"
   },
   "source": [
    "# Frank-Wolfe - standard algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "sK9kPumvmqL-"
   },
   "source": [
    "- Should we feed $\\delta$ to the FW algorithm or should it be defined based on the dimensions of the data?\n",
    "- Which is the correct objective function?\n",
    "- Initialize with random matrix of integers from 1 to 5 or with zeros matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "igJ95M7EmqL-"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kNN3FZyamqL_"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'propack'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [103]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpropack\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m svdg\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'propack'"
     ]
    }
   ],
   "source": [
    "%run propack"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kobsOmgimqL_"
   },
   "outputs": [],
   "source": [
    "def FW_objective_function(diff_vec):\n",
    "    return 0.5*(np.power(diff_vec,2).sum())\n",
    "    #return 0.5 * np.linalg.norm(diff_vec, 2)**2\n",
    "\n",
    "def FrankWolfe(X, objective_function, delta, printing_res = True, Z_init = None, max_iter = 150, patience = 1e-3):\n",
    "    '''\n",
    "    :param X: sparse matrix with ratings and 'empty values', rows - users, columns - books.\n",
    "    :param objective_function: objective function that we would like to minimize with FW\n",
    "    :param delta: feasible set ball radius\n",
    "    :param printing_res:\n",
    "    :param Z_init: In case we want to initialize Z with a known matrix, if not given Z_init will be a zeros matrix\n",
    "    :param max_iter: max number of iterations for the method\n",
    "    :param patience: once reached this tolerance provide the result\n",
    "    :return: Z: matrix of predicted ratings - it should be like X but with no 'empty values'\n",
    "            accuracy: difference between original values (X) and predicted ones (Z)\n",
    "    '''\n",
    "\n",
    "    # Get X indexes for not empty values\n",
    "    idx_ratings = np.argwhere(X != 0)\n",
    "    #idx_ratings = np.argwhere(~np.isnan(X))\n",
    "    idx_rows = idx_ratings[:,0]\n",
    "    idx_cols = idx_ratings[:,1]\n",
    "\n",
    "    # Initialize Z -- think about a good init\n",
    "    if Z_init is not None:\n",
    "        Z = Z_init\n",
    "    else:\n",
    "        #Z = np.random.randint(1, 6, size=X.shape)\n",
    "        #Z = Z.astype(float)\n",
    "        Z = np.zeros(X.shape)\n",
    "\n",
    "    # Create vectors with the not empty features of the sparse matrix\n",
    "    X_rated = X[idx_rows, idx_cols]\n",
    "    Z_rated = Z[idx_rows, idx_cols]\n",
    "    diff_vec = Z_rated - X_rated\n",
    "\n",
    "    diff_err = patience + 1\n",
    "    err = objective_function(diff_vec)\n",
    "    it = 0\n",
    "    while (diff_err > patience) and (it < max_iter):\n",
    "\n",
    "        # Gradient\n",
    "        grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "\n",
    "        # SVD\n",
    "        u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')   # Compute k = 1 singular values, starting from the largest (which = 'LM')\n",
    "\n",
    "        # Update\n",
    "        update_Z = -delta*np.outer(u_max,v_max)     # Zk_tilde in the theory\n",
    "\n",
    "        #alpha - as studied in class\n",
    "        alpha_k = 2/(it+2)\n",
    "        Z = (1-alpha_k)*Z + alpha_k*update_Z\n",
    "\n",
    "        # Error\n",
    "        diff_vec = Z[idx_rows, idx_cols] - X_rated\n",
    "        new_err = objective_function(diff_vec)\n",
    "\n",
    "        # Improvement at this iteration\n",
    "        diff_err = np.abs(err - new_err)\n",
    "        err = new_err\n",
    "\n",
    "        if printing_res == True:\n",
    "          print('Iteration:', it, 'Loss:', err, 'Diff loss:', diff_err)\n",
    "\n",
    "        # Count iteration\n",
    "        it += 1\n",
    "    return Z, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "WXTeGA9emqMA"
   },
   "source": [
    "We build a smaller matrix for testing the FW alg, then we will apply it to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8_5BaHGfmqMA"
   },
   "outputs": [],
   "source": [
    "# Create a random sparse matrix for testing\n",
    "rvs = stats.randint(1,6).rvs\n",
    "X_test = sparse.random(1500, 2000,              # shape of the sparse matrix\n",
    "            density = 0.05,             # density of the sparse matrix\n",
    "            dtype = np.int32,           # data type\n",
    "            data_rvs=rvs).toarray()     # distribution\n",
    "\n",
    "#Normalize the values\n",
    "X_test_norm = X_test/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HiCoSkpMmqMA",
    "outputId": "ad67adec-11c5-4d56-de6b-1573ccaca27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 1025452.8619112275 Diff loss: 992320.0619112274\n",
      "Iteration: 1 Loss: 340112.4198101729 Diff loss: 685340.4421010546\n",
      "Iteration: 2 Loss: 95807.38873195121 Diff loss: 244305.0310782217\n",
      "Iteration: 3 Loss: 267157.37486659497 Diff loss: 171349.98613464375\n",
      "Iteration: 4 Loss: 1890987.6340689366 Diff loss: 1623830.2592023415\n",
      "Iteration: 5 Loss: 318147.1635612207 Diff loss: 1572840.4705077158\n",
      "Iteration: 6 Loss: 845443.5646209167 Diff loss: 527296.401059696\n",
      "Iteration: 7 Loss: 237989.2211579262 Diff loss: 607454.3434629906\n",
      "Iteration: 8 Loss: 485970.7363074082 Diff loss: 247981.515149482\n",
      "Iteration: 9 Loss: 185027.57060305812 Diff loss: 300943.1657043501\n",
      "Iteration: 10 Loss: 319448.83534347964 Diff loss: 134421.26474042152\n",
      "Iteration: 11 Loss: 149448.60393385522 Diff loss: 170000.23140962442\n",
      "Iteration: 12 Loss: 229526.5567915776 Diff loss: 80077.95285772238\n",
      "Iteration: 13 Loss: 124739.95460787958 Diff loss: 104786.60218369802\n",
      "Iteration: 14 Loss: 175729.2419455971 Diff loss: 50989.28733771753\n",
      "Iteration: 15 Loss: 106997.90716394353 Diff loss: 68731.33478165358\n",
      "Iteration: 16 Loss: 141090.55194633777 Diff loss: 34092.64478239424\n",
      "Iteration: 17 Loss: 93872.00180998712 Diff loss: 47218.55013635065\n",
      "Iteration: 18 Loss: 117521.05669934391 Diff loss: 23649.05488935679\n",
      "Iteration: 19 Loss: 83906.49463664099 Diff loss: 33614.56206270293\n",
      "Iteration: 20 Loss: 100778.45628195938 Diff loss: 16871.9616453184\n",
      "Iteration: 21 Loss: 76169.78056714222 Diff loss: 24608.67571481716\n",
      "Iteration: 22 Loss: 88468.5336009041 Diff loss: 12298.753033761881\n",
      "Iteration: 23 Loss: 70046.55701213317 Diff loss: 18421.976588770936\n",
      "Iteration: 24 Loss: 79158.86118866061 Diff loss: 9112.30417652744\n",
      "Iteration: 25 Loss: 65118.54647984039 Diff loss: 14040.314708820217\n",
      "Iteration: 26 Loss: 71950.71363861821 Diff loss: 6832.167158777818\n",
      "Iteration: 27 Loss: 61093.96686935354 Diff loss: 10856.746769264668\n",
      "Iteration: 28 Loss: 66257.51218588835 Diff loss: 5163.545316534808\n",
      "Iteration: 29 Loss: 57764.53683406168 Diff loss: 8492.975351826673\n",
      "Iteration: 30 Loss: 61683.44124743515 Diff loss: 3918.9044133734715\n",
      "Iteration: 31 Loss: 54978.498162858734 Diff loss: 6704.943084576415\n",
      "Iteration: 32 Loss: 57953.69130470963 Diff loss: 2975.1931418508975\n",
      "Iteration: 33 Loss: 52623.230955198334 Diff loss: 5330.460349511297\n",
      "Iteration: 34 Loss: 54872.70562130666 Diff loss: 2249.4746661083263\n",
      "Iteration: 35 Loss: 50613.77467576047 Diff loss: 4258.930945546192\n",
      "Iteration: 36 Loss: 52298.296302626586 Diff loss: 1684.521626866117\n",
      "Iteration: 37 Loss: 48885.079295047784 Diff loss: 3413.2170075788017\n",
      "Iteration: 38 Loss: 50125.10005817464 Diff loss: 1240.0207631268568\n",
      "Iteration: 39 Loss: 47386.66824282878 Diff loss: 2738.431815345859\n",
      "Iteration: 40 Loss: 48273.71553480379 Diff loss: 887.04729197501\n",
      "Iteration: 41 Loss: 46078.89466776935 Diff loss: 2194.8208670344393\n",
      "Iteration: 42 Loss: 46683.3991382904 Diff loss: 604.5044705210457\n",
      "Iteration: 43 Loss: 44930.271181087184 Diff loss: 1753.1279572032145\n",
      "Iteration: 44 Loss: 45307.048096448154 Diff loss: 376.7769153609697\n",
      "Iteration: 45 Loss: 43915.53600720136 Diff loss: 1391.5120892467967\n",
      "Iteration: 46 Loss: 44107.688121906176 Diff loss: 192.1521147048188\n",
      "Iteration: 47 Loss: 43014.23273387023 Diff loss: 1093.4553880359454\n",
      "Iteration: 48 Loss: 43055.9716880586 Diff loss: 41.73895418836764\n",
      "Iteration: 49 Loss: 42209.65376069911 Diff loss: 846.3179273594869\n",
      "Iteration: 50 Loss: 42128.368042704045 Diff loss: 81.28571799506608\n",
      "Iteration: 51 Loss: 41488.04493643403 Diff loss: 640.3231062700143\n",
      "Iteration: 52 Loss: 41305.83488761785 Diff loss: 182.21004881618137\n",
      "Iteration: 53 Loss: 40838.00021833566 Diff loss: 467.83466928218695\n",
      "Iteration: 54 Loss: 40572.830741698766 Diff loss: 265.16947663689643\n",
      "Iteration: 55 Loss: 40249.99625071671 Diff loss: 322.8344909820589\n",
      "Iteration: 56 Loss: 39916.571754497745 Diff loss: 333.4244962189623\n",
      "Iteration: 57 Loss: 39716.0311278304 Diff loss: 200.54062666734535\n",
      "Iteration: 58 Loss: 39326.46625167973 Diff loss: 389.56487615066726\n",
      "Iteration: 59 Loss: 39229.3415438547 Diff loss: 97.12470782503078\n",
      "Iteration: 60 Loss: 38793.680088119385 Diff loss: 435.66145573531685\n",
      "Iteration: 61 Loss: 38784.17949529497 Diff loss: 9.500592824413616\n",
      "Iteration: 62 Loss: 38310.799365143255 Diff loss: 473.380130151716\n",
      "Iteration: 63 Loss: 38375.634638555435 Diff loss: 64.83527341217996\n",
      "Iteration: 64 Loss: 37871.56638118081 Diff loss: 504.0682573746235\n",
      "Iteration: 65 Loss: 37999.49194650611 Diff loss: 127.92556532529852\n",
      "Iteration: 66 Loss: 37470.67120405056 Diff loss: 528.8207424555512\n",
      "Iteration: 67 Loss: 37652.1168745288 Diff loss: 181.4456704782424\n",
      "Iteration: 68 Loss: 37103.58587288531 Diff loss: 548.5310016434887\n",
      "Iteration: 69 Loss: 37330.36212549071 Diff loss: 226.77625260539935\n",
      "Iteration: 70 Loss: 36766.43154940432 Diff loss: 563.9305760863936\n",
      "Iteration: 71 Loss: 37031.491491568195 Diff loss: 265.05994216387626\n",
      "Iteration: 72 Loss: 36455.87133792291 Diff loss: 575.6201536452863\n",
      "Iteration: 73 Loss: 36753.117285970846 Diff loss: 297.2459480479374\n",
      "Iteration: 74 Loss: 36169.02324998386 Diff loss: 584.0940359869855\n",
      "Iteration: 75 Loss: 36493.14865581598 Diff loss: 324.12540583212103\n",
      "Iteration: 76 Loss: 35903.38908752508 Diff loss: 589.7595682909014\n",
      "Iteration: 77 Loss: 36249.74865712566 Diff loss: 346.35956960057956\n",
      "Iteration: 78 Loss: 35656.795986444406 Diff loss: 592.952670681254\n",
      "Iteration: 79 Loss: 36021.298423238244 Diff loss: 364.50243679383857\n",
      "Iteration: 80 Loss: 35427.34809048257 Diff loss: 593.950332755674\n",
      "Iteration: 81 Loss: 35806.36710433567 Diff loss: 379.0190138531034\n",
      "Iteration: 82 Loss: 35213.38637751102 Diff loss: 592.9807268246514\n",
      "Iteration: 83 Loss: 35603.6865241808 Diff loss: 390.30014666977513\n",
      "Iteration: 84 Loss: 35013.45508243148 Diff loss: 590.231441749318\n",
      "Iteration: 85 Loss: 35412.12970956809 Diff loss: 398.6746271366137\n",
      "Iteration: 86 Loss: 34826.27348608313 Diff loss: 585.8562234849596\n",
      "Iteration: 87 Loss: 35230.69261251952 Diff loss: 404.4191264363835\n",
      "Iteration: 88 Loss: 34650.71209193227 Diff loss: 579.9805205872472\n",
      "Iteration: 89 Loss: 35058.47847541138 Diff loss: 407.7663834791092\n",
      "Iteration: 90 Loss: 34485.77240957719 Diff loss: 572.706065834187\n",
      "Iteration: 91 Loss: 34894.68439288353 Diff loss: 408.9119833063378\n",
      "Iteration: 92 Loss: 34330.56971929728 Diff loss: 564.1146735862494\n",
      "Iteration: 93 Loss: 34738.58970749169 Diff loss: 408.0199881944063\n",
      "Iteration: 94 Loss: 34184.31831470711 Diff loss: 554.2713927845762\n",
      "Iteration: 95 Loss: 34589.5459431128 Diff loss: 405.2276284056861\n",
      "Iteration: 96 Loss: 34046.31881828438 Diff loss: 543.227124828416\n",
      "Iteration: 97 Loss: 34446.9680345028 Diff loss: 400.6492162184222\n",
      "Iteration: 98 Loss: 33915.94724257075 Diff loss: 531.0207919320528\n",
      "Iteration: 99 Loss: 34310.32665572008 Diff loss: 394.37941314932687\n",
      "Iteration: 100 Loss: 33792.645532317045 Diff loss: 517.681123403032\n",
      "Iteration: 101 Loss: 34179.14148631264 Diff loss: 386.4959539955962\n",
      "Iteration: 102 Loss: 33675.91337292636 Diff loss: 503.228113386278\n",
      "Iteration: 103 Loss: 34052.975283675056 Diff loss: 377.06191074869275\n",
      "Iteration: 104 Loss: 33565.3010907034 Diff loss: 487.67419297165907\n",
      "Iteration: 105 Loss: 33931.4286538412 Diff loss: 366.1275631378012\n",
      "Iteration: 106 Loss: 33460.40350262851 Diff loss: 471.02515121268516\n",
      "Iteration: 107 Loss: 33814.1354318291 Diff loss: 353.731929200585\n",
      "Iteration: 108 Loss: 33360.85459929392 Diff loss: 453.2808325351798\n",
      "Iteration: 109 Loss: 33700.75859669923 Diff loss: 339.9039974053085\n",
      "Iteration: 110 Loss: 33266.32296573549 Diff loss: 434.4356309637369\n",
      "Iteration: 111 Loss: 33590.986655344605 Diff loss: 324.66368960911495\n",
      "Iteration: 112 Loss: 33176.50786258618 Diff loss: 414.4787927584257\n",
      "Iteration: 113 Loss: 33484.5304314291 Diff loss: 308.02256884292\n",
      "Iteration: 114 Loss: 33091.135905808194 Diff loss: 393.39452562090446\n",
      "Iteration: 115 Loss: 33381.12018903777 Diff loss: 289.98428322957625\n",
      "Iteration: 116 Loss: 33009.95829916028 Diff loss: 371.16188987749047\n",
      "Iteration: 117 Loss: 33280.502998996235 Diff loss: 270.5446998359548\n",
      "Iteration: 118 Loss: 32932.74859232961 Diff loss: 347.75440666662325\n",
      "Iteration: 119 Loss: 33182.44020882658 Diff loss: 249.69161649696616\n",
      "Iteration: 120 Loss: 32859.30096398268 Diff loss: 323.1392448439001\n",
      "Iteration: 121 Loss: 33086.70478411217 Diff loss: 227.40382012948976\n",
      "Iteration: 122 Loss: 32789.42907155694 Diff loss: 297.2757125552307\n",
      "Iteration: 123 Loss: 32993.07810713456 Diff loss: 203.6490355776259\n",
      "Iteration: 124 Loss: 32722.965586136776 Diff loss: 270.1125209977872\n",
      "Iteration: 125 Loss: 32901.34545548758 Diff loss: 178.37986935080335\n",
      "Iteration: 126 Loss: 32659.762680452885 Diff loss: 241.58277503469435\n",
      "Iteration: 127 Loss: 32811.288619128005 Diff loss: 151.52593867512041\n",
      "Iteration: 128 Loss: 32599.694055879438 Diff loss: 211.59456324856728\n",
      "Iteration: 129 Loss: 32722.672365608872 Diff loss: 122.97830972943484\n",
      "Iteration: 130 Loss: 32542.659835604525 Diff loss: 180.01253000434735\n",
      "Iteration: 131 Loss: 32635.21695558919 Diff loss: 92.55711998466359\n",
      "Iteration: 132 Loss: 32488.59759743126 Diff loss: 146.61935815792822\n",
      "Iteration: 133 Loss: 32548.53524076872 Diff loss: 59.93764333746003\n",
      "Iteration: 134 Loss: 32437.508790653752 Diff loss: 111.02645011496861\n",
      "Iteration: 135 Loss: 32461.96105663651 Diff loss: 24.452265982759855\n",
      "Iteration: 136 Loss: 32389.53225266912 Diff loss: 72.42880396739201\n",
      "Iteration: 137 Loss: 32373.923042120332 Diff loss: 15.609210548787814\n",
      "Iteration: 138 Loss: 32345.19721950396 Diff loss: 28.72582261637217\n",
      "Iteration: 139 Loss: 32277.999111700563 Diff loss: 67.19810780339685\n",
      "Iteration: 140 Loss: 32300.40402591231 Diff loss: 22.404914211747382\n",
      "Iteration: 141 Loss: 32014.460024137115 Diff loss: 285.9440017751949\n",
      "Iteration: 142 Loss: 27167.877929514747 Diff loss: 4846.5820946223685\n",
      "Iteration: 143 Loss: 27466.109517776993 Diff loss: 298.2315882622461\n",
      "Iteration: 144 Loss: 27280.493412907632 Diff loss: 185.61610486936115\n",
      "Iteration: 145 Loss: 27511.989830543047 Diff loss: 231.49641763541513\n",
      "Iteration: 146 Loss: 27387.365792560176 Diff loss: 124.6240379828705\n",
      "Iteration: 147 Loss: 27546.808789497347 Diff loss: 159.4429969371704\n",
      "Iteration: 148 Loss: 27489.515844656955 Diff loss: 57.29294484039201\n",
      "Iteration: 149 Loss: 27587.992100089825 Diff loss: 98.47625543286995\n",
      "Iteration: 150 Loss: 27585.712180553655 Diff loss: 2.279919536169473\n",
      "Iteration: 151 Loss: 27630.97677338426 Diff loss: 45.26459283060467\n",
      "Iteration: 152 Loss: 27674.016495631462 Diff loss: 43.03972224720201\n",
      "Iteration: 153 Loss: 27683.305995591676 Diff loss: 9.289499960214016\n",
      "Iteration: 154 Loss: 27720.92069190092 Diff loss: 37.6146963092433\n",
      "Iteration: 155 Loss: 27742.541880425873 Diff loss: 21.62118852495405\n",
      "Iteration: 156 Loss: 24841.430688420634 Diff loss: 2901.111192005239\n",
      "Iteration: 157 Loss: 21112.94027080055 Diff loss: 3728.4904176200835\n",
      "Iteration: 158 Loss: 17859.479374977633 Diff loss: 3253.4608958229182\n",
      "Iteration: 159 Loss: 15054.006956899226 Diff loss: 2805.472418078407\n",
      "Iteration: 160 Loss: 12670.75471267587 Diff loss: 2383.252244223355\n",
      "Iteration: 161 Loss: 10685.120265670503 Diff loss: 1985.6344470053682\n",
      "Iteration: 162 Loss: 9073.548468362418 Diff loss: 1611.5717973080846\n",
      "Iteration: 163 Loss: 7813.384138082759 Diff loss: 1260.1643302796592\n",
      "Iteration: 164 Loss: 6882.651972270187 Diff loss: 930.732165812572\n",
      "Iteration: 165 Loss: 6259.598557579983 Diff loss: 623.0534146902037\n",
      "Iteration: 166 Loss: 5920.701435554381 Diff loss: 338.8971220256026\n",
      "Iteration: 167 Loss: 5712.408109418411 Diff loss: 208.29332613596944\n",
      "Iteration: 168 Loss: 5521.7914740381375 Diff loss: 190.6166353802737\n",
      "Iteration: 169 Loss: 5342.454879270792 Diff loss: 179.33659476734556\n",
      "Iteration: 170 Loss: 5177.538476857649 Diff loss: 164.91640241314326\n",
      "Iteration: 171 Loss: 5018.770786028814 Diff loss: 158.76769082883493\n",
      "Iteration: 172 Loss: 4790.058479017777 Diff loss: 228.7123070110365\n",
      "Iteration: 173 Loss: 4615.953285874094 Diff loss: 174.1051931436832\n",
      "Iteration: 174 Loss: 4453.25573053562 Diff loss: 162.69755533847365\n",
      "Iteration: 175 Loss: 4295.6100214023345 Diff loss: 157.6457091332859\n",
      "Iteration: 176 Loss: 4149.335610708636 Diff loss: 146.27441069369888\n",
      "Iteration: 177 Loss: 4015.297594007902 Diff loss: 134.0380167007338\n",
      "Iteration: 178 Loss: 3876.043809200284 Diff loss: 139.25378480761765\n",
      "Iteration: 179 Loss: 3738.1404417790086 Diff loss: 137.90336742127556\n",
      "Iteration: 180 Loss: 3608.3146180581557 Diff loss: 129.8258237208529\n",
      "Iteration: 181 Loss: 3487.0610542515187 Diff loss: 121.25356380663698\n",
      "Iteration: 182 Loss: 3366.4663639477108 Diff loss: 120.59469030380797\n",
      "Iteration: 183 Loss: 3249.261721402481 Diff loss: 117.20464254522994\n",
      "Iteration: 184 Loss: 3138.9192051144937 Diff loss: 110.34251628798711\n",
      "Iteration: 185 Loss: 3034.606323209723 Diff loss: 104.31288190477062\n",
      "Iteration: 186 Loss: 2936.4899324687144 Diff loss: 98.11639074100867\n",
      "Iteration: 187 Loss: 2848.2541034897927 Diff loss: 88.23582897892175\n",
      "Iteration: 188 Loss: 3507.8226077857053 Diff loss: 659.5685042959126\n",
      "Iteration: 189 Loss: 3602.5068904048653 Diff loss: 94.68428261916006\n",
      "Iteration: 190 Loss: 3537.193064208045 Diff loss: 65.31382619682017\n",
      "Iteration: 191 Loss: 3655.2248410087745 Diff loss: 118.0317768007294\n",
      "Iteration: 192 Loss: 3611.691670777924 Diff loss: 43.533170230850374\n",
      "Iteration: 193 Loss: 3735.905220679851 Diff loss: 124.21354990192685\n",
      "Iteration: 194 Loss: 3707.9143198029333 Diff loss: 27.99090087691775\n",
      "Iteration: 195 Loss: 3836.1930675671865 Diff loss: 128.27874776425324\n",
      "Iteration: 196 Loss: 3821.877178784911 Diff loss: 14.315888782275579\n",
      "Iteration: 197 Loss: 3953.289616819413 Diff loss: 131.41243803450197\n",
      "Iteration: 198 Loss: 3951.384555663402 Diff loss: 1.9050611560110156\n",
      "Iteration: 199 Loss: 4085.253410287256 Diff loss: 133.86885462385408\n",
      "Iteration: 200 Loss: 4094.711709131316 Diff loss: 9.45829884406021\n",
      "Iteration: 201 Loss: 4230.475154658527 Diff loss: 135.76344552721048\n",
      "Iteration: 202 Loss: 4250.375709337664 Diff loss: 19.900554679137713\n",
      "Iteration: 203 Loss: 4387.545831745074 Diff loss: 137.1701224074095\n",
      "Iteration: 204 Loss: 4417.059138389731 Diff loss: 29.51330664465695\n",
      "Iteration: 205 Loss: 4555.203663203091 Diff loss: 138.14452481336048\n",
      "Iteration: 206 Loss: 4593.574330616229 Diff loss: 38.37066741313811\n",
      "Iteration: 207 Loss: 4732.306482406434 Diff loss: 138.73215179020463\n",
      "Iteration: 208 Loss: 4778.842664344703 Diff loss: 46.53618193826878\n",
      "Iteration: 209 Loss: 4917.8145142976755 Diff loss: 138.97184995297266\n",
      "Iteration: 210 Loss: 4971.880583396224 Diff loss: 54.066069098548724\n",
      "Iteration: 211 Loss: 5110.778126462551 Diff loss: 138.89754306632676\n",
      "Iteration: 212 Loss: 5171.789072206249 Diff loss: 61.01094574369836\n",
      "Iteration: 213 Loss: 5310.328273320176 Diff loss: 138.53920111392654\n",
      "Iteration: 214 Loss: 5377.745128132167 Diff loss: 67.41685481199147\n",
      "Iteration: 215 Loss: 5515.668571319577 Diff loss: 137.92344318740925\n",
      "Iteration: 216 Loss: 5588.994515394823 Diff loss: 73.3259440752463\n",
      "Iteration: 217 Loss: 5726.068460499341 Diff loss: 137.07394510451832\n",
      "Iteration: 218 Loss: 5804.845413629992 Diff loss: 78.77695313065124\n",
      "Iteration: 219 Loss: 5940.857144764381 Diff loss: 136.01173113438836\n",
      "Iteration: 220 Loss: 6024.662729013789 Diff loss: 83.80558424940864\n",
      "Iteration: 221 Loss: 6159.418117489803 Diff loss: 134.75538847601365\n",
      "Iteration: 222 Loss: 6247.862911513964 Diff loss: 88.44479402416073\n",
      "Iteration: 223 Loss: 6381.1841350527 Diff loss: 133.32122353873638\n",
      "Iteration: 224 Loss: 6473.9091575069915 Diff loss: 92.72502245429132\n",
      "Iteration: 225 Loss: 6605.632526289531 Diff loss: 131.72336878253918\n",
      "Iteration: 226 Loss: 6702.306889577638 Diff loss: 96.67436328810709\n",
      "Iteration: 227 Loss: 6832.280732419759 Diff loss: 129.97384284212148\n",
      "Iteration: 228 Loss: 6932.599400930958 Diff loss: 100.31866851119867\n",
      "Iteration: 229 Loss: 7060.681963571699 Diff loss: 128.08256264074134\n",
      "Iteration: 230 Loss: 7164.363530431003 Diff loss: 103.68156685930353\n",
      "Iteration: 231 Loss: 7290.4208337141945 Diff loss: 126.05730328319169\n",
      "Iteration: 232 Loss: 7397.205190426519 Diff loss: 106.78435671232455\n",
      "Iteration: 233 Loss: 7521.108790092328 Diff loss: 123.90359966580854\n",
      "Iteration: 234 Loss: 7630.754490859149 Diff loss: 109.64570076682139\n",
      "Iteration: 235 Loss: 7752.379074845566 Diff loss: 121.62458398641684\n",
      "Iteration: 236 Loss: 7864.660065660043 Diff loss: 112.28099081447726\n",
      "Iteration: 237 Loss: 7983.880824391744 Diff loss: 119.2207587317007\n",
      "Iteration: 238 Loss: 8098.581965356397 Diff loss: 114.70114096465295\n",
      "Iteration: 239 Loss: 8215.271688148296 Diff loss: 116.68972279189893\n",
      "Iteration: 240 Loss: 8332.182044342517 Diff loss: 116.9103561942211\n",
      "Iteration: 241 Loss: 8446.20796119498 Diff loss: 114.02591685246261\n",
      "Iteration: 242 Loss: 8565.10996503465 Diff loss: 118.90200383967021\n",
      "Iteration: 243 Loss: 8676.330539248944 Diff loss: 111.22057421429417\n",
      "Iteration: 244 Loss: 8796.981397480453 Diff loss: 120.65085823150912\n",
      "Iteration: 245 Loss: 8905.243745048701 Diff loss: 108.26234756824852\n",
      "Iteration: 246 Loss: 9027.341927071462 Diff loss: 122.09818202276074\n",
      "Iteration: 247 Loss: 9132.481656542035 Diff loss: 105.139729470573\n",
      "Iteration: 248 Loss: 9255.603860507368 Diff loss: 123.12220396533303\n",
      "Iteration: 249 Loss: 9357.451610572107 Diff loss: 101.84775006473865\n",
      "Iteration: 250 Loss: 9480.929634760942 Diff loss: 123.47802418883475\n",
      "Iteration: 251 Loss: 9579.333358338672 Diff loss: 98.40372357773049\n",
      "Iteration: 252 Loss: 9702.006236627773 Diff loss: 122.67287828910048\n",
      "Iteration: 253 Loss: 9796.883049205193 Diff loss: 94.87681257742042\n",
      "Iteration: 254 Loss: 9916.593031903341 Diff loss: 119.70998269814845\n",
      "Iteration: 255 Loss: 10007.99709006173 Diff loss: 91.40405815838858\n",
      "Iteration: 256 Loss: 10120.611383977195 Diff loss: 112.61429391546517\n",
      "Iteration: 257 Loss: 10208.501283027368 Diff loss: 87.88989905017297\n",
      "Iteration: 258 Loss: 10306.355072620936 Diff loss: 97.85378959356785\n",
      "Iteration: 259 Loss: 10387.430038624567 Diff loss: 81.07496600363083\n",
      "Iteration: 260 Loss: 10456.516808740362 Diff loss: 69.08677011579493\n",
      "Iteration: 261 Loss: 10491.425702555309 Diff loss: 34.90889381494708\n",
      "Iteration: 262 Loss: 10137.61430399043 Diff loss: 353.8113985648797\n",
      "Iteration: 263 Loss: 8910.180762523167 Diff loss: 1227.4335414672623\n",
      "Iteration: 264 Loss: 7828.970276706167 Diff loss: 1081.2104858169996\n",
      "Iteration: 265 Loss: 6888.187428768658 Diff loss: 940.7828479375094\n",
      "Iteration: 266 Loss: 6098.839977098692 Diff loss: 789.3474516699662\n",
      "Iteration: 267 Loss: 5403.666947929588 Diff loss: 695.1730291691038\n",
      "Iteration: 268 Loss: 4834.5842639918765 Diff loss: 569.0826839377114\n",
      "Iteration: 269 Loss: 4386.005945284514 Diff loss: 448.5783187073621\n",
      "Iteration: 270 Loss: 5330.956748158056 Diff loss: 944.9508028735418\n",
      "Iteration: 271 Loss: 4516.222815069816 Diff loss: 814.7339330882405\n",
      "Iteration: 272 Loss: 5418.443540787483 Diff loss: 902.2207257176669\n",
      "Iteration: 273 Loss: 4652.2813121286445 Diff loss: 766.1622286588381\n",
      "Iteration: 274 Loss: 5512.987462522587 Diff loss: 860.7061503939422\n",
      "Iteration: 275 Loss: 4793.083598614767 Diff loss: 719.9038639078199\n",
      "Iteration: 276 Loss: 5615.203068239867 Diff loss: 822.1194696251005\n",
      "Iteration: 277 Loss: 4937.992047765758 Diff loss: 677.2110204741093\n",
      "Iteration: 278 Loss: 5722.093114578524 Diff loss: 784.1010668127665\n",
      "Iteration: 279 Loss: 5086.525830653418 Diff loss: 635.5672839251065\n",
      "Iteration: 280 Loss: 5834.564970679782 Diff loss: 748.0391400263643\n",
      "Iteration: 281 Loss: 5238.282803865617 Diff loss: 596.2821668141651\n",
      "Iteration: 282 Loss: 5950.386976663501 Diff loss: 712.1041727978836\n",
      "Iteration: 283 Loss: 5392.9085850740585 Diff loss: 557.4783915894423\n",
      "Iteration: 284 Loss: 6071.012990691342 Diff loss: 678.1044056172832\n",
      "Iteration: 285 Loss: 5550.084261570134 Diff loss: 520.9287291212077\n",
      "Iteration: 286 Loss: 6193.3735088355115 Diff loss: 643.2892472653775\n",
      "Iteration: 287 Loss: 5709.517291745182 Diff loss: 483.85621709032966\n",
      "Iteration: 288 Loss: 6321.592758559813 Diff loss: 612.075466814631\n",
      "Iteration: 289 Loss: 5870.941415808029 Diff loss: 450.6513427517839\n",
      "Iteration: 290 Loss: 6445.969207060654 Diff loss: 575.0277912526253\n",
      "Iteration: 291 Loss: 6034.1079157687955 Diff loss: 411.86129129185883\n",
      "Iteration: 292 Loss: 6586.944751283118 Diff loss: 552.836835514323\n",
      "Iteration: 293 Loss: 6198.8215297987535 Diff loss: 388.1232214843649\n",
      "Iteration: 294 Loss: 6648.550747284703 Diff loss: 449.72921748594945\n",
      "Iteration: 295 Loss: 6366.175950939611 Diff loss: 282.37479634509236\n",
      "Iteration: 296 Loss: 6799.759487697637 Diff loss: 433.58353675802664\n",
      "Iteration: 297 Loss: 6532.7493671242955 Diff loss: 267.01012057334174\n",
      "Iteration: 298 Loss: 6949.96761622104 Diff loss: 417.21824909674433\n",
      "Iteration: 299 Loss: 6700.539119616117 Diff loss: 249.42849660492266\n",
      "Iteration: 300 Loss: 7091.4573552071215 Diff loss: 390.9182355910043\n",
      "Iteration: 301 Loss: 6869.11344267696 Diff loss: 222.3439125301611\n",
      "Iteration: 302 Loss: 7236.608118118716 Diff loss: 367.49467544175604\n",
      "Iteration: 303 Loss: 7038.295410674284 Diff loss: 198.31270744443282\n",
      "Iteration: 304 Loss: 7380.1761827674145 Diff loss: 341.8807720931309\n",
      "Iteration: 305 Loss: 7207.937399888681 Diff loss: 172.23878287873322\n",
      "Iteration: 306 Loss: 7526.643443880974 Diff loss: 318.7060439922925\n",
      "Iteration: 307 Loss: 7377.895718080656 Diff loss: 148.74772580031822\n",
      "Iteration: 308 Loss: 7670.968207659037 Diff loss: 293.07248957838146\n",
      "Iteration: 309 Loss: 7548.0430482185 Diff loss: 122.92515944053685\n",
      "Iteration: 310 Loss: 7820.060746562022 Diff loss: 272.01769834352217\n",
      "Iteration: 311 Loss: 7718.258058223801 Diff loss: 101.80268833822174\n",
      "Iteration: 312 Loss: 7960.834714464371 Diff loss: 242.57665624057063\n",
      "Iteration: 313 Loss: 7888.426059452698 Diff loss: 72.40865501167355\n",
      "Iteration: 314 Loss: 8118.16808336042 Diff loss: 229.7420239077219\n",
      "Iteration: 315 Loss: 8058.476509530039 Diff loss: 59.69157383038055\n",
      "Iteration: 316 Loss: 8197.735561694575 Diff loss: 139.25905216453612\n",
      "Iteration: 317 Loss: 8228.402945436374 Diff loss: 30.667383741798403\n",
      "Iteration: 318 Loss: 8360.906784117396 Diff loss: 132.5038386810229\n",
      "Iteration: 319 Loss: 8397.230363022445 Diff loss: 36.323578905048635\n",
      "Iteration: 320 Loss: 8521.442236611356 Diff loss: 124.21187358891075\n",
      "Iteration: 321 Loss: 8565.402491770674 Diff loss: 43.96025515931797\n",
      "Iteration: 322 Loss: 8675.357528168892 Diff loss: 109.955036398218\n",
      "Iteration: 323 Loss: 8733.16843305664 Diff loss: 57.81090488774862\n",
      "Iteration: 324 Loss: 8829.148910191037 Diff loss: 95.980477134397\n",
      "Iteration: 325 Loss: 8900.37390874129 Diff loss: 71.22499855025308\n",
      "Iteration: 326 Loss: 8982.343579103375 Diff loss: 81.9696703620848\n",
      "Iteration: 327 Loss: 9066.930685943767 Diff loss: 84.58710684039215\n",
      "Iteration: 328 Loss: 9135.727386732202 Diff loss: 68.79670078843446\n",
      "Iteration: 329 Loss: 8300.460018734984 Diff loss: 835.2673679972177\n",
      "Iteration: 330 Loss: 8398.8361611905 Diff loss: 98.37614245551595\n",
      "Iteration: 331 Loss: 8457.930433856647 Diff loss: 59.094272666146935\n",
      "Iteration: 332 Loss: 8553.781771483908 Diff loss: 95.85133762726036\n",
      "Iteration: 333 Loss: 8615.118142172278 Diff loss: 61.33637068837015\n",
      "Iteration: 334 Loss: 8703.575178346788 Diff loss: 88.45703617451\n",
      "Iteration: 335 Loss: 8772.85129726803 Diff loss: 69.2761189212415\n",
      "Iteration: 336 Loss: 8848.634204928176 Diff loss: 75.78290766014652\n",
      "Iteration: 337 Loss: 8928.141944482571 Diff loss: 79.50773955439581\n",
      "Iteration: 338 Loss: 8884.458509182457 Diff loss: 43.68343530011407\n",
      "Iteration: 339 Loss: 8100.1217179889545 Diff loss: 784.3367911935029\n",
      "Iteration: 340 Loss: 7400.312644523402 Diff loss: 699.8090734655525\n",
      "Iteration: 341 Loss: 7601.476309342914 Diff loss: 201.16366481951172\n",
      "Iteration: 342 Loss: 7548.782046100872 Diff loss: 52.694263242041416\n",
      "Iteration: 343 Loss: 7727.550488449709 Diff loss: 178.7684423488363\n",
      "Iteration: 344 Loss: 7697.641845778058 Diff loss: 29.90864267165034\n",
      "Iteration: 345 Loss: 7867.203288291274 Diff loss: 169.5614425132153\n",
      "Iteration: 346 Loss: 7846.732177241805 Diff loss: 20.471111049468163\n",
      "Iteration: 347 Loss: 7967.821852400196 Diff loss: 121.08967515839049\n",
      "Iteration: 348 Loss: 7995.589871050018 Diff loss: 27.768018649821897\n",
      "Iteration: 349 Loss: 8112.595601475374 Diff loss: 117.00573042535598\n",
      "Iteration: 350 Loss: 8144.480481744352 Diff loss: 31.884880268978122\n",
      "Iteration: 351 Loss: 8242.672127105447 Diff loss: 98.19164536109474\n",
      "Iteration: 352 Loss: 8293.658693062052 Diff loss: 50.98656595660577\n",
      "Iteration: 353 Loss: 8382.966801960187 Diff loss: 89.30810889813438\n",
      "Iteration: 354 Loss: 8442.563903816725 Diff loss: 59.59710185653785\n",
      "Iteration: 355 Loss: 8509.19997070486 Diff loss: 66.63606688813525\n",
      "Iteration: 356 Loss: 8565.318586829151 Diff loss: 56.11861612429129\n",
      "Iteration: 357 Loss: 7890.2464781678555 Diff loss: 675.0721086612957\n",
      "Iteration: 358 Loss: 7248.206186315481 Diff loss: 642.0402918523741\n",
      "Iteration: 359 Loss: 6679.525275519308 Diff loss: 568.6809107961735\n",
      "Iteration: 360 Loss: 6182.00560603659 Diff loss: 497.5196694827182\n",
      "Iteration: 361 Loss: 5753.4646213929245 Diff loss: 428.54098464366507\n",
      "Iteration: 362 Loss: 5391.564377648202 Diff loss: 361.9002437447225\n",
      "Iteration: 363 Loss: 5092.425030429576 Diff loss: 299.1393472186264\n",
      "Iteration: 364 Loss: 4848.144180320294 Diff loss: 244.28085010928135\n",
      "Iteration: 365 Loss: 4663.589929370449 Diff loss: 184.55425094984548\n",
      "Iteration: 366 Loss: 5203.278611972407 Diff loss: 539.6886826019581\n",
      "Iteration: 367 Loss: 4728.527234543443 Diff loss: 474.7513774289637\n",
      "Iteration: 368 Loss: 5243.443653740166 Diff loss: 514.9164191967229\n",
      "Iteration: 369 Loss: 4793.989996330883 Diff loss: 449.45365740928355\n",
      "Iteration: 370 Loss: 5293.848460111779 Diff loss: 499.8584637808963\n",
      "Iteration: 371 Loss: 4863.229984787548 Diff loss: 430.61847532423053\n",
      "Iteration: 372 Loss: 5348.192006284001 Diff loss: 484.9620214964525\n",
      "Iteration: 373 Loss: 4935.942288106772 Diff loss: 412.2497181772287\n",
      "Iteration: 374 Loss: 5405.773658707495 Diff loss: 469.83137060072295\n",
      "Iteration: 375 Loss: 5011.905532809045 Diff loss: 393.86812589844976\n",
      "Iteration: 376 Loss: 5466.390050825308 Diff loss: 454.4845180162629\n",
      "Iteration: 377 Loss: 5090.922312434242 Diff loss: 375.46773839106663\n",
      "Iteration: 378 Loss: 5529.927562881192 Diff loss: 439.0052504469504\n",
      "Iteration: 379 Loss: 5172.8090133884425 Diff loss: 357.1185494927495\n",
      "Iteration: 380 Loss: 5596.268152659686 Diff loss: 423.45913927124366\n",
      "Iteration: 381 Loss: 5257.392565268644 Diff loss: 338.87558739104225\n",
      "Iteration: 382 Loss: 5665.284376742798 Diff loss: 407.89181147415366\n",
      "Iteration: 383 Loss: 5344.5089330661895 Diff loss: 320.7754436766081\n",
      "Iteration: 384 Loss: 5736.846444254624 Diff loss: 392.3375111884343\n",
      "Iteration: 385 Loss: 5434.002205518314 Diff loss: 302.8442387363093\n",
      "Iteration: 386 Loss: 5810.823163595904 Diff loss: 376.82095807758924\n",
      "Iteration: 387 Loss: 5525.723942299821 Diff loss: 285.09922129608276\n",
      "Iteration: 388 Loss: 5887.086095868251 Diff loss: 361.3621535684297\n",
      "Iteration: 389 Loss: 5619.532647691779 Diff loss: 267.55344817647165\n",
      "Iteration: 390 Loss: 5965.5015928603525 Diff loss: 345.96894516857355\n",
      "Iteration: 391 Loss: 5715.293310488435 Diff loss: 250.20828237191745\n",
      "Iteration: 392 Loss: 6046.019890345766 Diff loss: 330.72657985733076\n",
      "Iteration: 393 Loss: 5812.8770923153825 Diff loss: 233.14279803038335\n",
      "Iteration: 394 Loss: 5492.502994369662 Diff loss: 320.3740979457207\n",
      "Iteration: 395 Loss: 5810.796043891565 Diff loss: 318.2930495219034\n",
      "Iteration: 396 Loss: 5581.694779665746 Diff loss: 229.10126422581925\n",
      "Iteration: 397 Loss: 5888.319894268213 Diff loss: 306.62511460246697\n",
      "Iteration: 398 Loss: 5672.919002600677 Diff loss: 215.4008916675357\n",
      "Iteration: 399 Loss: 5967.571907774825 Diff loss: 294.65290517414815\n",
      "Iteration: 400 Loss: 5766.039451055556 Diff loss: 201.53245671926925\n",
      "Iteration: 401 Loss: 6048.50012012002 Diff loss: 282.4606690644641\n",
      "Iteration: 402 Loss: 5860.930440033667 Diff loss: 187.5696800863534\n",
      "Iteration: 403 Loss: 6131.058799801778 Diff loss: 270.1283597681113\n",
      "Iteration: 404 Loss: 5957.473559849173 Diff loss: 173.58523995260475\n",
      "Iteration: 405 Loss: 6215.170249082021 Diff loss: 257.69668923284735\n",
      "Iteration: 406 Loss: 6055.556384600649 Diff loss: 159.61386448137182\n",
      "Iteration: 407 Loss: 6300.754090618553 Diff loss: 245.19770601790424\n",
      "Iteration: 408 Loss: 6155.071694269206 Diff loss: 145.68239634934707\n",
      "Iteration: 409 Loss: 6387.716826073507 Diff loss: 232.64513180430095\n",
      "Iteration: 410 Loss: 6255.916916419834 Diff loss: 131.79990965367324\n",
      "Iteration: 411 Loss: 6475.983046813047 Diff loss: 220.06613039321292\n",
      "Iteration: 412 Loss: 6357.9936301938 Diff loss: 117.9894166192471\n",
      "Iteration: 413 Loss: 6565.357821946345 Diff loss: 207.3641917525456\n",
      "Iteration: 414 Loss: 6461.206713923469 Diff loss: 104.15110802287654\n",
      "Iteration: 415 Loss: 6657.861554083284 Diff loss: 196.65484015981565\n",
      "Iteration: 416 Loss: 6565.477410662599 Diff loss: 92.38414342068518\n",
      "Iteration: 417 Loss: 6179.540359355216 Diff loss: 385.9370513073827\n",
      "Iteration: 418 Loss: 6369.639475527021 Diff loss: 190.09911617180478\n",
      "Iteration: 419 Loss: 6277.176076010641 Diff loss: 92.46339951638038\n",
      "Iteration: 420 Loss: 6460.6159310340845 Diff loss: 183.43985502344367\n",
      "Iteration: 421 Loss: 6376.101552621406 Diff loss: 84.51437841267852\n",
      "Iteration: 422 Loss: 6549.359285960329 Diff loss: 173.25773333892266\n",
      "Iteration: 423 Loss: 6476.181618258195 Diff loss: 73.17766770213348\n",
      "Iteration: 424 Loss: 6639.804763407418 Diff loss: 163.6231451492231\n",
      "Iteration: 425 Loss: 6577.32428811705 Diff loss: 62.480475290368304\n",
      "Iteration: 426 Loss: 6730.499327671543 Diff loss: 153.1750395544932\n",
      "Iteration: 427 Loss: 6679.447770965962 Diff loss: 51.05155670558088\n",
      "Iteration: 428 Loss: 6822.798149116498 Diff loss: 143.35037815053602\n",
      "Iteration: 429 Loss: 6782.469105330482 Diff loss: 40.32904378601597\n",
      "Iteration: 430 Loss: 6913.725634003564 Diff loss: 131.25652867308145\n",
      "Iteration: 431 Loss: 6886.30055324718 Diff loss: 27.425080756383977\n",
      "Iteration: 432 Loss: 7011.793773485732 Diff loss: 125.49322023855257\n",
      "Iteration: 433 Loss: 6990.921302915213 Diff loss: 20.872470570519\n",
      "Iteration: 434 Loss: 6630.621163730887 Diff loss: 360.3001391843263\n",
      "Iteration: 435 Loss: 6252.017102692887 Diff loss: 378.6040610379996\n",
      "Iteration: 436 Loss: 5920.849193503977 Diff loss: 331.16790918891\n",
      "Iteration: 437 Loss: 5635.818455624168 Diff loss: 285.03073787980975\n",
      "Iteration: 438 Loss: 5395.625150510573 Diff loss: 240.19330511359476\n",
      "Iteration: 439 Loss: 5198.9569362441225 Diff loss: 196.66821426645038\n",
      "Iteration: 440 Loss: 5044.463796731585 Diff loss: 154.4931395125377\n",
      "Iteration: 441 Loss: 4930.669184359531 Diff loss: 113.79461237205396\n",
      "Iteration: 442 Loss: 4826.6287516019365 Diff loss: 104.04043275759432\n",
      "Iteration: 443 Loss: 4725.812551705065 Diff loss: 100.8161998968717\n",
      "Iteration: 444 Loss: 4627.581272808407 Diff loss: 98.23127889665739\n",
      "Iteration: 445 Loss: 4532.623155890884 Diff loss: 94.95811691752351\n",
      "Iteration: 446 Loss: 4440.3565134184455 Diff loss: 92.26664247243843\n",
      "Iteration: 447 Loss: 4348.5906336680855 Diff loss: 91.76587975036\n",
      "Iteration: 448 Loss: 4250.66292831169 Diff loss: 97.92770535639556\n",
      "Iteration: 449 Loss: 4160.050887085959 Diff loss: 90.61204122573054\n",
      "Iteration: 450 Loss: 4071.1394156795564 Diff loss: 88.91147140640305\n",
      "Iteration: 451 Loss: 3983.382555376502 Diff loss: 87.75686030305451\n",
      "Iteration: 452 Loss: 3898.40024923714 Diff loss: 84.98230613936175\n",
      "Iteration: 453 Loss: 3815.678796775718 Diff loss: 82.7214524614219\n",
      "Iteration: 454 Loss: 3733.4661931274313 Diff loss: 82.21260364828686\n",
      "Iteration: 455 Loss: 3654.8510956876153 Diff loss: 78.61509743981605\n",
      "Iteration: 456 Loss: 3573.914212153815 Diff loss: 80.93688353380048\n",
      "Iteration: 457 Loss: 3495.5133206487812 Diff loss: 78.40089150503354\n",
      "Iteration: 458 Loss: 3419.7246554476405 Diff loss: 75.7886652011407\n",
      "Iteration: 459 Loss: 3345.3955850787493 Diff loss: 74.32907036889128\n",
      "Iteration: 460 Loss: 3272.856958436818 Diff loss: 72.53862664193139\n",
      "Iteration: 461 Loss: 3201.3180115691 Diff loss: 71.5389468677181\n",
      "Iteration: 462 Loss: 3131.261073719188 Diff loss: 70.05693784991172\n",
      "Iteration: 463 Loss: 3061.375607809018 Diff loss: 69.88546591016984\n",
      "Iteration: 464 Loss: 2994.82345360238 Diff loss: 66.55215420663808\n",
      "Iteration: 465 Loss: 2929.275350703254 Diff loss: 65.5481028991262\n",
      "Iteration: 466 Loss: 2862.312786485118 Diff loss: 66.96256421813587\n",
      "Iteration: 467 Loss: 2798.336564286663 Diff loss: 63.97622219845516\n",
      "Iteration: 468 Loss: 2735.845160642697 Diff loss: 62.491403643965896\n",
      "Iteration: 469 Loss: 2674.770425322227 Diff loss: 61.074735320470154\n",
      "Iteration: 470 Loss: 2614.575454768675 Diff loss: 60.194970553551684\n",
      "Iteration: 471 Loss: 2556.0169909228193 Diff loss: 58.55846384585584\n",
      "Iteration: 472 Loss: 2499.5617240702027 Diff loss: 56.45526685261666\n",
      "Iteration: 473 Loss: 2441.420430226081 Diff loss: 58.14129384412172\n",
      "Iteration: 474 Loss: 2387.454075500765 Diff loss: 53.966354725315796\n",
      "Iteration: 475 Loss: 2333.463739039796 Diff loss: 53.99033646096905\n",
      "Iteration: 476 Loss: 2278.493542574366 Diff loss: 54.970196465430035\n",
      "Iteration: 477 Loss: 2226.5219063765935 Diff loss: 51.97163619777257\n",
      "Iteration: 478 Loss: 2174.5263444561992 Diff loss: 51.995561920394266\n",
      "Iteration: 479 Loss: 2125.2356302872763 Diff loss: 49.29071416892293\n",
      "Iteration: 480 Loss: 2075.591877390196 Diff loss: 49.6437528970805\n",
      "Iteration: 481 Loss: 2027.516008284861 Diff loss: 48.07586910533473\n",
      "Iteration: 482 Loss: 1981.103279158488 Diff loss: 46.41272912637305\n",
      "Iteration: 483 Loss: 1935.0291639706934 Diff loss: 46.07411518779463\n",
      "Iteration: 484 Loss: 1887.4435803874435 Diff loss: 47.5855835832499\n",
      "Iteration: 485 Loss: 1844.856590191484 Diff loss: 42.586990195959515\n",
      "Iteration: 486 Loss: 1801.7252948532353 Diff loss: 43.13129533824872\n",
      "Iteration: 487 Loss: 1759.8314645738974 Diff loss: 41.89383027933786\n",
      "Iteration: 488 Loss: 1717.1125293544446 Diff loss: 42.71893521945276\n",
      "Iteration: 489 Loss: 1676.538132018487 Diff loss: 40.574397335957656\n",
      "Iteration: 490 Loss: 1636.9776392148578 Diff loss: 39.56049280362913\n",
      "Iteration: 491 Loss: 1597.229250771922 Diff loss: 39.74838844293595\n",
      "Iteration: 492 Loss: 1559.9509323687546 Diff loss: 37.278318403167304\n",
      "Iteration: 493 Loss: 1522.9543668712834 Diff loss: 36.99656549747124\n",
      "Iteration: 494 Loss: 1485.4044152995762 Diff loss: 37.5499515717072\n",
      "Iteration: 495 Loss: 1449.654624761524 Diff loss: 35.74979053805214\n",
      "Iteration: 496 Loss: 1414.6231524125953 Diff loss: 35.03147234892867\n",
      "Iteration: 497 Loss: 1380.2192840911964 Diff loss: 34.40386832139893\n",
      "Iteration: 498 Loss: 1347.469247199796 Diff loss: 32.75003689140044\n",
      "Iteration: 499 Loss: 1314.3795231076447 Diff loss: 33.0897240921513\n",
      "Iteration: 500 Loss: 1284.1945358389373 Diff loss: 30.184987268707346\n",
      "Iteration: 501 Loss: 1252.5709887413127 Diff loss: 31.623547097624623\n",
      "Iteration: 502 Loss: 1222.5864135609584 Diff loss: 29.98457518035434\n",
      "Iteration: 503 Loss: 1192.9767827385162 Diff loss: 29.609630822442114\n",
      "Iteration: 504 Loss: 1164.2620589970381 Diff loss: 28.71472374147811\n",
      "Iteration: 505 Loss: 1137.4212900221728 Diff loss: 26.84076897486534\n",
      "Iteration: 506 Loss: 1108.6120910924196 Diff loss: 28.809198929753165\n",
      "Iteration: 507 Loss: 1082.407795149052 Diff loss: 26.204295943367697\n",
      "Iteration: 508 Loss: 1054.5747606967147 Diff loss: 27.833034452337188\n",
      "Iteration: 509 Loss: 1029.5517590223822 Diff loss: 25.02300167433259\n",
      "Iteration: 510 Loss: 1005.5083221060329 Diff loss: 24.043436916349265\n",
      "Iteration: 511 Loss: 980.1274024108053 Diff loss: 25.38091969522759\n",
      "Iteration: 512 Loss: 957.3177578032625 Diff loss: 22.80964460754285\n",
      "Iteration: 513 Loss: 933.1336061965793 Diff loss: 24.184151606683145\n",
      "Iteration: 514 Loss: 911.2628379997828 Diff loss: 21.870768196796462\n",
      "Iteration: 515 Loss: 887.472969924792 Diff loss: 23.789868074990864\n",
      "Iteration: 516 Loss: 866.9374546154758 Diff loss: 20.535515309316224\n",
      "Iteration: 517 Loss: 844.6538534005874 Diff loss: 22.283601214888336\n",
      "Iteration: 518 Loss: 824.5310352951689 Diff loss: 20.12281810541856\n",
      "Iteration: 519 Loss: 803.7649633124807 Diff loss: 20.766071982688118\n",
      "Iteration: 520 Loss: 783.3752430237998 Diff loss: 20.389720288680905\n",
      "Iteration: 521 Loss: 764.6756526073154 Diff loss: 18.699590416484398\n",
      "Iteration: 522 Loss: 746.333754272479 Diff loss: 18.3418983348364\n",
      "Iteration: 523 Loss: 727.0131125705898 Diff loss: 19.320641701889258\n",
      "Iteration: 524 Loss: 708.8294060911688 Diff loss: 18.183706479420948\n",
      "Iteration: 525 Loss: 692.0304309193804 Diff loss: 16.79897517178847\n",
      "Iteration: 526 Loss: 674.1949044929289 Diff loss: 17.83552642645145\n",
      "Iteration: 527 Loss: 658.0978756395281 Diff loss: 16.097028853400843\n",
      "Iteration: 528 Loss: 642.1457041338227 Diff loss: 15.952171505705337\n",
      "Iteration: 529 Loss: 625.8149401659632 Diff loss: 16.330763967859525\n",
      "Iteration: 530 Loss: 609.9271740119893 Diff loss: 15.887766153973871\n",
      "Iteration: 531 Loss: 596.4313640347925 Diff loss: 13.495809977196814\n",
      "Iteration: 532 Loss: 673.9014158326157 Diff loss: 77.4700517978232\n",
      "Iteration: 533 Loss: 706.2901667451976 Diff loss: 32.38875091258183\n",
      "Iteration: 534 Loss: 692.7508760159393 Diff loss: 13.539290729258255\n",
      "Iteration: 535 Loss: 728.1488649045145 Diff loss: 35.39798888857524\n",
      "Iteration: 536 Loss: 717.2667847149405 Diff loss: 10.882080189574026\n",
      "Iteration: 537 Loss: 753.4642962880251 Diff loss: 36.197511573084626\n",
      "Iteration: 538 Loss: 744.8702220325171 Diff loss: 8.594074255508076\n",
      "Iteration: 539 Loss: 781.7586246492019 Diff loss: 36.88840261668486\n",
      "Iteration: 540 Loss: 775.3233627655587 Diff loss: 6.435261883643193\n",
      "Iteration: 541 Loss: 812.8423072339209 Diff loss: 37.51894446836218\n",
      "Iteration: 542 Loss: 808.4797022474671 Diff loss: 4.362604986453789\n",
      "Iteration: 543 Loss: 846.5828699627548 Diff loss: 38.10316771528767\n",
      "Iteration: 544 Loss: 844.2230749538769 Diff loss: 2.3597950088778816\n",
      "Iteration: 545 Loss: 882.8694772192513 Diff loss: 38.64640226537438\n",
      "Iteration: 546 Loss: 882.4505304666751 Diff loss: 0.41894675257617564\n",
      "Iteration: 547 Loss: 921.6017021744153 Diff loss: 39.15117170774022\n",
      "Iteration: 548 Loss: 923.0664376766576 Diff loss: 1.4647355022423199\n",
      "Iteration: 549 Loss: 962.685356732699 Diff loss: 39.61891905604136\n",
      "Iteration: 550 Loss: 965.9800965560561 Diff loss: 3.2947398233570766\n",
      "Iteration: 551 Loss: 1006.0306612819819 Diff loss: 40.050564725925824\n",
      "Iteration: 552 Loss: 1011.1045920224493 Diff loss: 5.073930740467404\n",
      "Iteration: 553 Loss: 1051.551290456303 Diff loss: 40.44669843385361\n",
      "Iteration: 554 Loss: 1058.3561427646675 Diff loss: 6.804852308364616\n",
      "Iteration: 555 Loss: 1099.1637822340376 Diff loss: 40.80763946937009\n",
      "Iteration: 556 Loss: 1107.653666211637 Diff loss: 8.489883977599447\n",
      "Iteration: 557 Loss: 1148.787105789435 Diff loss: 41.133439577797844\n",
      "Iteration: 558 Loss: 1158.9184407204136 Diff loss: 10.13133493097871\n",
      "Iteration: 559 Loss: 1200.3422929840706 Diff loss: 41.423852263656954\n",
      "Iteration: 560 Loss: 1212.0738053931386 Diff loss: 11.731512409068046\n",
      "Iteration: 561 Loss: 1253.752079319483 Diff loss: 41.678273926344445\n",
      "Iteration: 562 Loss: 1267.044860471388 Diff loss: 13.29278115190482\n",
      "Iteration: 563 Loss: 1308.9405139848045 Diff loss: 41.89565351341662\n",
      "Iteration: 564 Loss: 1323.7581380652832 Diff loss: 14.817624080478708\n",
      "Iteration: 565 Loss: 1365.832499074312 Diff loss: 42.074361009028735\n",
      "Iteration: 566 Loss: 1382.1412110212339 Diff loss: 16.308711946921903\n",
      "Iteration: 567 Loss: 1424.353209019204 Diff loss: 42.21199799797023\n",
      "Iteration: 568 Loss: 1442.1221984691088 Diff loss: 17.768989449904666\n",
      "Iteration: 569 Loss: 1484.4273221602054 Diff loss: 42.305123691096696\n",
      "Iteration: 570 Loss: 1503.629108361323 Diff loss: 19.201786201117557\n",
      "Iteration: 571 Loss: 1545.9779632461857 Diff loss: 42.3488548848627\n",
      "Iteration: 572 Loss: 1566.5889253344906 Diff loss: 20.61096208830486\n",
      "Iteration: 573 Loss: 1608.9252005512008 Diff loss: 42.33627521671019\n",
      "Iteration: 574 Loss: 1630.926296778953 Diff loss: 22.001096227752214\n",
      "Iteration: 575 Loss: 1673.1838502199023 Diff loss: 42.25755344094932\n",
      "Iteration: 576 Loss: 1696.5615720103435 Diff loss: 23.377721790441228\n",
      "Iteration: 577 Loss: 1738.6601894024363 Diff loss: 42.09861739209282\n",
      "Iteration: 578 Loss: 1763.407770296135 Diff loss: 24.74758089369857\n",
      "Iteration: 579 Loss: 1805.2469289714638 Diff loss: 41.83915867532892\n",
      "Iteration: 580 Loss: 1831.365711067285 Diff loss: 26.11878209582119\n",
      "Iteration: 581 Loss: 1872.815383726737 Diff loss: 41.44967265945206\n",
      "Iteration: 582 Loss: 1900.3158476676035 Diff loss: 27.50046394086644\n",
      "Iteration: 583 Loss: 1941.203115720058 Diff loss: 40.887268052454374\n",
      "Iteration: 584 Loss: 1970.1038488391107 Diff loss: 28.900733119052802\n",
      "Iteration: 585 Loss: 2010.194317995355 Diff loss: 40.09046915624435\n",
      "Iteration: 586 Loss: 2040.513456737257 Diff loss: 30.319138741901952\n",
      "Iteration: 587 Loss: 2079.4887454501663 Diff loss: 38.97528871290933\n",
      "Iteration: 588 Loss: 2111.2110908947475 Diff loss: 31.72234544458115\n",
      "Iteration: 589 Loss: 2148.652294174641 Diff loss: 37.44120327989367\n",
      "Iteration: 590 Loss: 2181.6210167437403 Diff loss: 32.9687225690991\n",
      "Iteration: 591 Loss: 2217.0297802008677 Diff loss: 35.40876345712741\n",
      "Iteration: 592 Loss: 2250.6121400978536 Diff loss: 33.58235989698596\n",
      "Iteration: 593 Loss: 2283.5037349343675 Diff loss: 32.89159483651383\n",
      "Iteration: 594 Loss: 2315.639339117117 Diff loss: 32.135604182749375\n",
      "Iteration: 595 Loss: 2345.0923191282586 Diff loss: 29.45298001114179\n",
      "Iteration: 596 Loss: 2369.289912533317 Diff loss: 24.197593405058342\n",
      "Iteration: 597 Loss: 2364.090648149811 Diff loss: 5.1992643835060335\n",
      "Iteration: 598 Loss: 2138.9975488813484 Diff loss: 225.09309926846254\n",
      "Iteration: 599 Loss: 1941.3653727455476 Diff loss: 197.63217613580082\n",
      "Iteration: 600 Loss: 1762.751321631274 Diff loss: 178.61405111427348\n",
      "Iteration: 601 Loss: 1608.3810231004848 Diff loss: 154.37029853078934\n",
      "Iteration: 602 Loss: 1477.5734963366106 Diff loss: 130.8075267638742\n",
      "Iteration: 603 Loss: 1626.58374338068 Diff loss: 149.0102470440695\n",
      "Iteration: 604 Loss: 1529.7024826999927 Diff loss: 96.88126068068732\n",
      "Iteration: 605 Loss: 1671.2221103296567 Diff loss: 141.519627629664\n",
      "Iteration: 606 Loss: 1583.211148054139 Diff loss: 88.01096227551784\n",
      "Iteration: 607 Loss: 1720.4763963464547 Diff loss: 137.26524829231585\n",
      "Iteration: 608 Loss: 1638.0203815170644 Diff loss: 82.45601482939037\n",
      "Iteration: 609 Loss: 1765.6478383876379 Diff loss: 127.6274568705735\n",
      "Iteration: 610 Loss: 1694.0370652235429 Diff loss: 71.61077316409501\n",
      "Iteration: 611 Loss: 1818.613033368917 Diff loss: 124.57596814537419\n",
      "Iteration: 612 Loss: 1751.228668832713 Diff loss: 67.38436453620398\n",
      "Iteration: 613 Loss: 1863.2167056949495 Diff loss: 111.98803686223641\n",
      "Iteration: 614 Loss: 1809.530271239403 Diff loss: 53.68643445554653\n",
      "Iteration: 615 Loss: 1920.5328621283707 Diff loss: 111.00259088896769\n",
      "Iteration: 616 Loss: 1868.8874151632356 Diff loss: 51.64544696513508\n",
      "Iteration: 617 Loss: 1965.541947385654 Diff loss: 96.65453222241854\n",
      "Iteration: 618 Loss: 1929.2789665812857 Diff loss: 36.26298080436845\n",
      "Iteration: 619 Loss: 2025.629218389943 Diff loss: 96.35025180865728\n",
      "Iteration: 620 Loss: 1990.5889192790248 Diff loss: 35.04029911091811\n",
      "Iteration: 621 Loss: 2076.647736021526 Diff loss: 86.058816742501\n",
      "Iteration: 622 Loss: 2052.8243809414953 Diff loss: 23.823355080030524\n",
      "Iteration: 623 Loss: 2137.84946530232 Diff loss: 85.02508436082462\n",
      "Iteration: 624 Loss: 2115.946167166461 Diff loss: 21.90329813585913\n",
      "Iteration: 625 Loss: 2187.123079175248 Diff loss: 71.17691200878744\n",
      "Iteration: 626 Loss: 2179.946479803818 Diff loss: 7.176599371430257\n",
      "Iteration: 627 Loss: 2251.2883241516424 Diff loss: 71.34184434782446\n",
      "Iteration: 628 Loss: 2244.697863257126 Diff loss: 6.590460894516582\n",
      "Iteration: 629 Loss: 2305.552883435516 Diff loss: 60.85502017839008\n",
      "Iteration: 630 Loss: 2310.241924907209 Diff loss: 4.689041471693145\n",
      "Iteration: 631 Loss: 2370.8062861932717 Diff loss: 60.56436128606265\n",
      "Iteration: 632 Loss: 2376.4967230738685 Diff loss: 5.690436880596735\n",
      "Iteration: 633 Loss: 2422.408783764269 Diff loss: 45.91206069040072\n",
      "Iteration: 634 Loss: 2443.465955848006 Diff loss: 21.057172083736987\n",
      "Iteration: 635 Loss: 2489.98362275952 Diff loss: 46.51766691151397\n",
      "Iteration: 636 Loss: 2510.961822820484 Diff loss: 20.9782000609639\n",
      "Iteration: 637 Loss: 2550.976670176131 Diff loss: 40.01484735564691\n",
      "Iteration: 638 Loss: 2579.2439906794225 Diff loss: 28.267320503291558\n",
      "Iteration: 639 Loss: 2617.7551134122095 Diff loss: 38.511122732787044\n",
      "Iteration: 640 Loss: 2648.0279277291397 Diff loss: 30.27281431693018\n",
      "Iteration: 641 Loss: 2673.366222970791 Diff loss: 25.338295241651394\n",
      "Iteration: 642 Loss: 2470.187630456898 Diff loss: 203.1785925138929\n",
      "Iteration: 643 Loss: 2315.8598318272025 Diff loss: 154.32779862969574\n",
      "Iteration: 644 Loss: 2145.736522793402 Diff loss: 170.12330903380052\n",
      "Iteration: 645 Loss: 1996.5416318045834 Diff loss: 149.19489098881854\n",
      "Iteration: 646 Loss: 1867.8347396994657 Diff loss: 128.70689210511773\n",
      "Iteration: 647 Loss: 1759.1714918744253 Diff loss: 108.66324782504034\n",
      "Iteration: 648 Loss: 1670.099573974473 Diff loss: 89.07191789995227\n",
      "Iteration: 649 Loss: 1600.1516465033542 Diff loss: 69.94792747111887\n",
      "Iteration: 650 Loss: 1548.8294527521352 Diff loss: 51.32219375121895\n",
      "Iteration: 651 Loss: 1515.540507928141 Diff loss: 33.28894482399414\n",
      "Iteration: 652 Loss: 1486.3435543747592 Diff loss: 29.196953553381945\n",
      "Iteration: 653 Loss: 1458.1097723174362 Diff loss: 28.23378205732297\n",
      "Iteration: 654 Loss: 1430.5725039982706 Diff loss: 27.537268319165605\n",
      "Iteration: 655 Loss: 1403.677391848446 Diff loss: 26.895112149824627\n",
      "Iteration: 656 Loss: 1377.7142318140454 Diff loss: 25.963160034400516\n",
      "Iteration: 657 Loss: 1351.5149147542563 Diff loss: 26.19931705978911\n",
      "Iteration: 658 Loss: 1323.6409202048903 Diff loss: 27.87399454936599\n",
      "Iteration: 659 Loss: 1297.0443903771745 Diff loss: 26.596529827715813\n",
      "Iteration: 660 Loss: 1270.9309476661097 Diff loss: 26.113442711064863\n",
      "Iteration: 661 Loss: 1245.5903491905704 Diff loss: 25.34059847553931\n",
      "Iteration: 662 Loss: 1220.700591956671 Diff loss: 24.889757233899445\n",
      "Iteration: 663 Loss: 1196.562878745444 Diff loss: 24.13771321122681\n",
      "Iteration: 664 Loss: 1173.0637708362547 Diff loss: 23.499107909189433\n",
      "Iteration: 665 Loss: 1150.079408905584 Diff loss: 22.98436193067073\n",
      "Iteration: 666 Loss: 1125.7064260786065 Diff loss: 24.372982826977477\n",
      "Iteration: 667 Loss: 1103.4321263634263 Diff loss: 22.274299715180177\n",
      "Iteration: 668 Loss: 1080.4059196585622 Diff loss: 23.02620670486408\n",
      "Iteration: 669 Loss: 1058.6171385161083 Diff loss: 21.7887811424539\n",
      "Iteration: 670 Loss: 1036.939127721679 Diff loss: 21.678010794429383\n",
      "Iteration: 671 Loss: 1015.4291108589357 Diff loss: 21.510016862743214\n",
      "Iteration: 672 Loss: 994.5824941227798 Diff loss: 20.846616736155852\n",
      "Iteration: 673 Loss: 974.8215684448795 Diff loss: 19.76092567790033\n",
      "Iteration: 674 Loss: 953.9915417975294 Diff loss: 20.83002664735011\n",
      "Iteration: 675 Loss: 934.7329241888409 Diff loss: 19.258617608688496\n",
      "Iteration: 676 Loss: 915.6140489380778 Diff loss: 19.118875250763153\n",
      "Iteration: 677 Loss: 896.4174577451786 Diff loss: 19.196591192899177\n",
      "Iteration: 678 Loss: 877.0510325636284 Diff loss: 19.3664251815502\n",
      "Iteration: 679 Loss: 858.8517676580997 Diff loss: 18.199264905528707\n",
      "Iteration: 680 Loss: 840.1948587173575 Diff loss: 18.65690894074214\n",
      "Iteration: 681 Loss: 822.9682478109688 Diff loss: 17.226610906388714\n",
      "Iteration: 682 Loss: 805.0938691067074 Diff loss: 17.874378704261403\n",
      "Iteration: 683 Loss: 788.2015248785599 Diff loss: 16.892344228147522\n",
      "Iteration: 684 Loss: 770.9064002937114 Diff loss: 17.29512458484851\n",
      "Iteration: 685 Loss: 754.300744967602 Diff loss: 16.6056553261094\n",
      "Iteration: 686 Loss: 738.1145536151644 Diff loss: 16.186191352437618\n",
      "Iteration: 687 Loss: 722.4534016383038 Diff loss: 15.661151976860538\n",
      "Iteration: 688 Loss: 707.2228974839171 Diff loss: 15.230504154386722\n",
      "Iteration: 689 Loss: 691.7120358146725 Diff loss: 15.510861669244605\n",
      "Iteration: 690 Loss: 676.6234942569262 Diff loss: 15.088541557746339\n",
      "Iteration: 691 Loss: 661.5156309932133 Diff loss: 15.10786326371283\n",
      "Iteration: 692 Loss: 647.4563433568183 Diff loss: 14.059287636395084\n",
      "Iteration: 693 Loss: 632.7991170807514 Diff loss: 14.657226276066808\n",
      "Iteration: 694 Loss: 618.7259589518666 Diff loss: 14.07315812888487\n",
      "Iteration: 695 Loss: 605.0395909942279 Diff loss: 13.686367957638709\n",
      "Iteration: 696 Loss: 591.5456451051934 Diff loss: 13.493945889034421\n",
      "Iteration: 697 Loss: 578.3901838244961 Diff loss: 13.155461280697295\n",
      "Iteration: 698 Loss: 566.2810516098126 Diff loss: 12.109132214683541\n",
      "Iteration: 699 Loss: 553.118958679702 Diff loss: 13.162092930110589\n",
      "Iteration: 700 Loss: 541.1076662157335 Diff loss: 12.011292463968516\n",
      "Iteration: 701 Loss: 528.7805160151839 Diff loss: 12.327150200549568\n",
      "Iteration: 702 Loss: 517.1189981567226 Diff loss: 11.661517858461366\n",
      "Iteration: 703 Loss: 505.1610213245089 Diff loss: 11.95797683221366\n",
      "Iteration: 704 Loss: 494.2275898436987 Diff loss: 10.933431480810214\n",
      "Iteration: 705 Loss: 483.3361156198443 Diff loss: 10.891474223854402\n",
      "Iteration: 706 Loss: 471.65216007496474 Diff loss: 11.683955544879552\n",
      "Iteration: 707 Loss: 461.28435171530776 Diff loss: 10.367808359656976\n",
      "Iteration: 708 Loss: 450.51639938774105 Diff loss: 10.767952327566718\n",
      "Iteration: 709 Loss: 440.6301169759642 Diff loss: 9.886282411776847\n",
      "Iteration: 710 Loss: 430.2693171531656 Diff loss: 10.360799822798583\n",
      "Iteration: 711 Loss: 420.7284152998512 Diff loss: 9.540901853314438\n",
      "Iteration: 712 Loss: 411.7304526378686 Diff loss: 8.99796266198257\n",
      "Iteration: 713 Loss: 401.1221567747907 Diff loss: 10.608295863077899\n",
      "Iteration: 714 Loss: 392.08812742286653 Diff loss: 9.034029351924175\n",
      "Iteration: 715 Loss: 382.87640726351555 Diff loss: 9.211720159350989\n",
      "Iteration: 716 Loss: 374.3341594255186 Diff loss: 8.542247837996968\n",
      "Iteration: 717 Loss: 366.2951676262411 Diff loss: 8.038991799277483\n",
      "Iteration: 718 Loss: 358.0960527843676 Diff loss: 8.199114841873495\n",
      "Iteration: 719 Loss: 350.216862908199 Diff loss: 7.879189876168596\n",
      "Iteration: 720 Loss: 341.76477689591695 Diff loss: 8.452086012282052\n",
      "Iteration: 721 Loss: 333.7084097871477 Diff loss: 8.05636710876928\n",
      "Iteration: 722 Loss: 326.5757893224428 Diff loss: 7.132620464704871\n",
      "Iteration: 723 Loss: 318.55194893910476 Diff loss: 8.02384038333804\n",
      "Iteration: 724 Loss: 311.5490606053794 Diff loss: 7.002888333725366\n",
      "Iteration: 725 Loss: 304.67699478723506 Diff loss: 6.8720658181443355\n",
      "Iteration: 726 Loss: 296.71955693958444 Diff loss: 7.957437847650624\n",
      "Iteration: 727 Loss: 290.44839079189006 Diff loss: 6.271166147694373\n",
      "Iteration: 728 Loss: 283.7876649899471 Diff loss: 6.660725801942988\n",
      "Iteration: 729 Loss: 277.5409455965731 Diff loss: 6.246719393373951\n",
      "Iteration: 730 Loss: 271.50940596585957 Diff loss: 6.031539630713553\n",
      "Iteration: 731 Loss: 265.73266598567216 Diff loss: 5.776739980187415\n",
      "Iteration: 732 Loss: 261.19326867659197 Diff loss: 4.5393973090801865\n",
      "Iteration: 733 Loss: 292.4227394521704 Diff loss: 31.22947077557842\n",
      "Iteration: 734 Loss: 331.8894052448083 Diff loss: 39.46666579263791\n",
      "Iteration: 735 Loss: 303.60513345782556 Diff loss: 28.284271786982742\n",
      "Iteration: 736 Loss: 343.5535902806922 Diff loss: 39.94845682286666\n",
      "Iteration: 737 Loss: 316.79431206467336 Diff loss: 26.759278216018856\n",
      "Iteration: 738 Loss: 357.16734162547266 Diff loss: 40.373029560799296\n",
      "Iteration: 739 Loss: 331.7275528767615 Diff loss: 25.43978874871118\n",
      "Iteration: 740 Loss: 372.49123843818296 Diff loss: 40.76368556142148\n",
      "Iteration: 741 Loss: 348.3004075232486 Diff loss: 24.190830914934338\n",
      "Iteration: 742 Loss: 389.4344721840714 Diff loss: 41.13406466082279\n",
      "Iteration: 743 Loss: 366.4506341586513 Diff loss: 22.983838025420084\n",
      "Iteration: 744 Loss: 407.9375278397878 Diff loss: 41.48689368113645\n",
      "Iteration: 745 Loss: 386.129125974945 Diff loss: 21.80840186484278\n",
      "Iteration: 746 Loss: 427.95189444499334 Diff loss: 41.822768470048345\n",
      "Iteration: 747 Loss: 407.2922461991662 Diff loss: 20.659648245827157\n",
      "Iteration: 748 Loss: 449.4340023649481 Diff loss: 42.14175616578194\n",
      "Iteration: 749 Loss: 429.8992694389867 Diff loss: 19.534732925961407\n",
      "Iteration: 750 Loss: 472.34302876196483 Diff loss: 42.44375932297811\n",
      "Iteration: 751 Loss: 453.91135983447396 Diff loss: 18.431668927490875\n",
      "Iteration: 752 Loss: 496.6399449761174 Diff loss: 42.728585141643464\n",
      "Iteration: 753 Loss: 479.2910871822215 Diff loss: 17.348857793895945\n",
      "Iteration: 754 Loss: 522.2870206738451 Diff loss: 42.99593349162359\n",
      "Iteration: 755 Loss: 506.0021592759016 Diff loss: 16.284861397943473\n",
      "Iteration: 756 Loss: 549.2475130651048 Diff loss: 43.24535378920325\n",
      "Iteration: 757 Loss: 534.0092511585268 Diff loss: 15.238261906578032\n",
      "Iteration: 758 Loss: 577.4854295893235 Diff loss: 43.47617843079672\n",
      "Iteration: 759 Loss: 563.2778800872861 Diff loss: 14.20754950203741\n",
      "Iteration: 760 Loss: 606.9653060133131 Diff loss: 43.687425926027004\n",
      "Iteration: 761 Loss: 593.7743002781108 Diff loss: 13.191005735202339\n",
      "Iteration: 762 Loss: 637.651957340763 Diff loss: 43.87765706265225\n",
      "Iteration: 763 Loss: 625.4654011110171 Diff loss: 12.186556229745975\n",
      "Iteration: 764 Loss: 669.510155594334 Diff loss: 44.04475448331698\n",
      "Iteration: 765 Loss: 658.3185951477527 Diff loss: 11.19156044658132\n",
      "Iteration: 766 Loss: 702.5041689911832 Diff loss: 44.18557384343046\n",
      "Iteration: 767 Loss: 692.3016806642476 Diff loss: 10.202488326935622\n",
      "Iteration: 768 Loss: 736.5970542220056 Diff loss: 44.29537355775801\n",
      "Iteration: 769 Loss: 727.3826574272932 Diff loss: 9.214396794712343\n",
      "Iteration: 770 Loss: 771.7495081708796 Diff loss: 44.36685074358638\n",
      "Iteration: 771 Loss: 763.5294621255108 Diff loss: 8.220046045368804\n",
      "Iteration: 772 Loss: 807.917914672704 Diff loss: 44.3884525471932\n",
      "Iteration: 773 Loss: 800.7095662714328 Diff loss: 7.208348401271223\n",
      "Iteration: 774 Loss: 845.050872554451 Diff loss: 44.341306283018184\n",
      "Iteration: 775 Loss: 838.8893339411184 Diff loss: 6.161538613332596\n",
      "Iteration: 776 Loss: 883.0827623784212 Diff loss: 44.19342843730283\n",
      "Iteration: 777 Loss: 878.0329467105184 Diff loss: 5.049815667902749\n",
      "Iteration: 778 Loss: 921.9213994101373 Diff loss: 43.888452699618824\n",
      "Iteration: 779 Loss: 918.1005193564903 Diff loss: 3.820880053646988\n",
      "Iteration: 780 Loss: 961.4239764861306 Diff loss: 43.32345712964036\n",
      "Iteration: 781 Loss: 959.0446472585907 Diff loss: 2.3793292275399835\n",
      "Iteration: 782 Loss: 1001.3523615737998 Diff loss: 42.30771431520918\n",
      "Iteration: 783 Loss: 1000.8038390435499 Diff loss: 0.5485225302498975\n",
      "Iteration: 784 Loss: 1041.3100458306615 Diff loss: 40.50620678711152\n",
      "Iteration: 785 Loss: 1043.2897301752853 Diff loss: 1.97968434462382\n",
      "Iteration: 786 Loss: 1080.7570267532776 Diff loss: 37.46729657799233\n",
      "Iteration: 787 Loss: 1086.3611179145096 Diff loss: 5.604091161231963\n",
      "Iteration: 788 Loss: 1119.3802003433968 Diff loss: 33.01908242888726\n",
      "Iteration: 789 Loss: 1129.7578394730394 Diff loss: 10.377639129642603\n",
      "Iteration: 790 Loss: 1157.4517897840826 Diff loss: 27.69395031104318\n",
      "Iteration: 791 Loss: 1172.8623382192347 Diff loss: 15.410548435152123\n",
      "Iteration: 792 Loss: 1195.041970793013 Diff loss: 22.179632573778235\n",
      "Iteration: 793 Loss: 1213.6640930191274 Diff loss: 18.62212222611447\n",
      "Iteration: 794 Loss: 1230.7009653341345 Diff loss: 17.03687231500703\n",
      "Iteration: 795 Loss: 1243.4064580942231 Diff loss: 12.705492760088646\n",
      "Iteration: 796 Loss: 1229.9375480901058 Diff loss: 13.468910004117333\n",
      "Iteration: 797 Loss: 1108.5684798262148 Diff loss: 121.369068263891\n",
      "Iteration: 798 Loss: 1000.965531850957 Diff loss: 107.60294797525785\n",
      "Iteration: 799 Loss: 906.9705129577592 Diff loss: 93.99501889319777\n",
      "Iteration: 800 Loss: 924.6875658800703 Diff loss: 17.717052922311154\n",
      "Iteration: 801 Loss: 949.3148710081939 Diff loss: 24.627305128123567\n",
      "Iteration: 802 Loss: 961.8961375918959 Diff loss: 12.581266583702018\n",
      "Iteration: 803 Loss: 986.532675512412 Diff loss: 24.636537920516048\n",
      "Iteration: 804 Loss: 1000.0008366187328 Diff loss: 13.46816110632085\n",
      "Iteration: 805 Loss: 1024.5244064490716 Diff loss: 24.52356983033883\n",
      "Iteration: 806 Loss: 1038.8688382451126 Diff loss: 14.344431796040908\n",
      "Iteration: 807 Loss: 1063.175940863961 Diff loss: 24.307102618848376\n",
      "Iteration: 808 Loss: 1078.41580568971 Diff loss: 15.23986482574901\n",
      "Iteration: 809 Loss: 1102.3865797246935 Diff loss: 23.970774034983606\n",
      "Iteration: 810 Loss: 1118.5556237609687 Diff loss: 16.16904403627518\n",
      "Iteration: 811 Loss: 1142.0521229379626 Diff loss: 23.496499176993893\n",
      "Iteration: 812 Loss: 1159.1891098927408 Diff loss: 17.136986954778195\n",
      "Iteration: 813 Loss: 1182.0619670159806 Diff loss: 22.872857123239783\n",
      "Iteration: 814 Loss: 1200.192933177988 Diff loss: 18.130966162007326\n",
      "Iteration: 815 Loss: 1222.3027485791188 Diff loss: 22.109815401130845\n",
      "Iteration: 816 Loss: 1241.4042070312867 Diff loss: 19.10145845216789\n",
      "Iteration: 817 Loss: 1262.66720912397 Diff loss: 21.263002092683337\n",
      "Iteration: 818 Loss: 1282.5988501227446 Diff loss: 19.931640998774583\n",
      "Iteration: 819 Loss: 1303.0613556175656 Diff loss: 20.462505494821016\n",
      "Iteration: 820 Loss: 1323.4747575370807 Diff loss: 20.41340191951508\n",
      "Iteration: 821 Loss: 1343.388691724241 Diff loss: 19.913934187160294\n",
      "Iteration: 822 Loss: 1363.7098458754824 Diff loss: 20.321154151241444\n",
      "Iteration: 823 Loss: 1383.476552520991 Diff loss: 19.76670664550852\n",
      "Iteration: 824 Loss: 1403.2678160777868 Diff loss: 19.79126355679591\n",
      "Iteration: 825 Loss: 1423.02373418275 Diff loss: 19.755918104963257\n",
      "Iteration: 826 Loss: 1442.6767830956237 Diff loss: 19.6530489128736\n",
      "Iteration: 827 Loss: 1462.9996743240276 Diff loss: 20.322891228403932\n",
      "Iteration: 828 Loss: 1485.3793919794136 Diff loss: 22.379717655386003\n",
      "Iteration: 829 Loss: 1507.8136344323239 Diff loss: 22.434242452910212\n",
      "Iteration: 830 Loss: 1386.6692589754623 Diff loss: 121.14437545686155\n",
      "Iteration: 831 Loss: 1274.0684662069518 Diff loss: 112.60079276851047\n",
      "Iteration: 832 Loss: 1174.0702311051332 Diff loss: 99.99823510181864\n",
      "Iteration: 833 Loss: 1086.425736074444 Diff loss: 87.64449503068909\n",
      "Iteration: 834 Loss: 1149.29119965301 Diff loss: 62.86546357856582\n",
      "Iteration: 835 Loss: 1122.5773019570252 Diff loss: 26.71389769598477\n",
      "Iteration: 836 Loss: 1183.4832480106184 Diff loss: 60.9059460535932\n",
      "Iteration: 837 Loss: 1159.4679509097082 Diff loss: 24.015297100910175\n",
      "Iteration: 838 Loss: 1195.916345470084 Diff loss: 36.448394560375846\n",
      "Iteration: 839 Loss: 1197.8591748626916 Diff loss: 1.942829392607564\n",
      "Iteration: 840 Loss: 1233.3274310737197 Diff loss: 35.46825621102812\n",
      "Iteration: 841 Loss: 1235.7532781394648 Diff loss: 2.4258470657450744\n",
      "Iteration: 842 Loss: 1271.9400642771775 Diff loss: 36.186786137712716\n",
      "Iteration: 843 Loss: 1274.4559646747894 Diff loss: 2.5159003976118584\n",
      "Iteration: 844 Loss: 1310.922010856576 Diff loss: 36.46604618178662\n",
      "Iteration: 845 Loss: 1313.8439306328555 Diff loss: 2.9219197762795375\n",
      "Iteration: 846 Loss: 1349.7988206460698 Diff loss: 35.95489001321425\n",
      "Iteration: 847 Loss: 1353.875512742062 Diff loss: 4.076692095992257\n",
      "Iteration: 848 Loss: 1388.33496701637 Diff loss: 34.45945427430797\n",
      "Iteration: 849 Loss: 1394.5336386123956 Diff loss: 6.198671596025633\n",
      "Iteration: 850 Loss: 1426.9924704294954 Diff loss: 32.458831817099735\n",
      "Iteration: 851 Loss: 1435.8022234398627 Diff loss: 8.80975301036733\n",
      "Iteration: 852 Loss: 1466.1729295145083 Diff loss: 30.370706074645568\n",
      "Iteration: 853 Loss: 1477.6618426422222 Diff loss: 11.488913127713886\n",
      "Iteration: 854 Loss: 1505.6473801880463 Diff loss: 27.985537545824172\n",
      "Iteration: 855 Loss: 1520.0933984427095 Diff loss: 14.446018254663159\n",
      "Iteration: 856 Loss: 1545.696258735161 Diff loss: 25.602860292451624\n",
      "Iteration: 857 Loss: 1563.0584653710707 Diff loss: 17.362206635909615\n",
      "Iteration: 858 Loss: 1584.7088333294464 Diff loss: 21.650367958375682\n",
      "Iteration: 859 Loss: 1605.4940897769725 Diff loss: 20.78525644752608\n",
      "Iteration: 860 Loss: 1614.643453279861 Diff loss: 9.149363502888491\n",
      "Iteration: 861 Loss: 1502.7792669339026 Diff loss: 111.86418634595839\n",
      "Iteration: 862 Loss: 1506.6176939598056 Diff loss: 3.8384270259030018\n",
      "Iteration: 863 Loss: 1406.4179638112691 Diff loss: 100.19973014853645\n",
      "Iteration: 864 Loss: 1316.567378669048 Diff loss: 89.85058514222123\n",
      "Iteration: 865 Loss: 1237.8303778619986 Diff loss: 78.7370008070493\n",
      "Iteration: 866 Loss: 1301.6604416950368 Diff loss: 63.83006383303814\n",
      "Iteration: 867 Loss: 1272.2307012014783 Diff loss: 29.429740493558484\n",
      "Iteration: 868 Loss: 1204.0786377819195 Diff loss: 68.15206341955877\n",
      "Iteration: 869 Loss: 1142.20420354171 Diff loss: 61.87443424020944\n",
      "Iteration: 870 Loss: 1091.0933312353907 Diff loss: 51.11087230631938\n",
      "Iteration: 871 Loss: 1050.5260592448487 Diff loss: 40.56727199054194\n",
      "Iteration: 872 Loss: 1020.2587786575469 Diff loss: 30.267280587301798\n",
      "Iteration: 873 Loss: 1119.8267033501336 Diff loss: 99.56792469258664\n",
      "Iteration: 874 Loss: 1042.1838527481614 Diff loss: 77.64285060197221\n",
      "Iteration: 875 Loss: 1140.9318573244445 Diff loss: 98.74800457628317\n",
      "Iteration: 876 Loss: 1065.0219509289714 Diff loss: 75.90990639547317\n",
      "Iteration: 877 Loss: 1161.0051162409245 Diff loss: 95.98316531195314\n",
      "Iteration: 878 Loss: 1088.7350505887664 Diff loss: 72.27006565215811\n",
      "Iteration: 879 Loss: 1183.6015208375975 Diff loss: 94.86647024883109\n",
      "Iteration: 880 Loss: 1113.2982141076607 Diff loss: 70.30330672993682\n",
      "Iteration: 881 Loss: 1203.3881292488857 Diff loss: 90.08991514122499\n",
      "Iteration: 882 Loss: 1138.6989703721015 Diff loss: 64.68915887678418\n",
      "Iteration: 883 Loss: 1228.822098932506 Diff loss: 90.12312856040444\n",
      "Iteration: 884 Loss: 1164.9021010495057 Diff loss: 63.91999788300018\n",
      "Iteration: 885 Loss: 1211.3955282557376 Diff loss: 46.4934272062319\n",
      "Iteration: 886 Loss: 1196.177432902397 Diff loss: 15.218095353340686\n",
      "Iteration: 887 Loss: 1237.693663935669 Diff loss: 41.516231033272106\n",
      "Iteration: 888 Loss: 1223.0385415822948 Diff loss: 14.655122353374281\n",
      "Iteration: 889 Loss: 1265.2265713737677 Diff loss: 42.1880297914729\n",
      "Iteration: 890 Loss: 1250.856621055558 Diff loss: 14.369950318209703\n",
      "Iteration: 891 Loss: 1293.6699704459797 Diff loss: 42.81334939042176\n",
      "Iteration: 892 Loss: 1279.513672840284 Diff loss: 14.156297605695727\n",
      "Iteration: 893 Loss: 1322.8764659784256 Diff loss: 43.36279313814157\n",
      "Iteration: 894 Loss: 1308.9500779265754 Diff loss: 13.926388051850154\n",
      "Iteration: 895 Loss: 1352.7594741748014 Diff loss: 43.809396248225994\n",
      "Iteration: 896 Loss: 1339.128693233881 Diff loss: 13.630780940920431\n",
      "Iteration: 897 Loss: 1383.2559080781054 Diff loss: 44.127214844224454\n",
      "Iteration: 898 Loss: 1370.0226063285209 Diff loss: 13.233301749584598\n",
      "Iteration: 899 Loss: 1414.3119347923378 Diff loss: 44.2893284638169\n",
      "Iteration: 900 Loss: 1401.6100619490805 Diff loss: 12.701872843257206\n",
      "Iteration: 901 Loss: 1445.8767091777636 Diff loss: 44.26664722868304\n",
      "Iteration: 902 Loss: 1433.872011454664 Diff loss: 12.004697723099525\n",
      "Iteration: 903 Loss: 1477.9002006297708 Diff loss: 44.02818917510672\n",
      "Iteration: 904 Loss: 1466.7908145394263 Diff loss: 11.109386090344515\n",
      "Iteration: 905 Loss: 1510.3346972398685 Diff loss: 43.54388270044228\n",
      "Iteration: 906 Loss: 1500.349497756798 Diff loss: 9.985199483070573\n",
      "Iteration: 907 Loss: 1543.1407894262325 Diff loss: 42.791291669434486\n",
      "Iteration: 908 Loss: 1534.5312731148622 Diff loss: 8.609516311370271\n",
      "Iteration: 909 Loss: 1576.29748669586 Diff loss: 41.76621358099783\n",
      "Iteration: 910 Loss: 1569.3191513426543 Diff loss: 6.9783353532056935\n",
      "Iteration: 911 Loss: 1609.810305213146 Diff loss: 40.491153870491644\n",
      "Iteration: 912 Loss: 1604.695605144316 Diff loss: 5.114700068829961\n",
      "Iteration: 913 Loss: 1643.702924781141 Diff loss: 39.00731963682506\n",
      "Iteration: 914 Loss: 1640.6423803636544 Diff loss: 3.0605444174866534\n",
      "Iteration: 915 Loss: 1677.9864189736884 Diff loss: 37.34403861003398\n",
      "Iteration: 916 Loss: 1677.1404294398226 Diff loss: 0.8459895338658043\n",
      "Iteration: 917 Loss: 1712.6429177790665 Diff loss: 35.50248833924388\n",
      "Iteration: 918 Loss: 1714.1693430825142 Diff loss: 1.5264253034476951\n",
      "Iteration: 919 Loss: 1747.6642005835156 Diff loss: 33.494857501001434\n",
      "Iteration: 920 Loss: 1751.7057040959564 Diff loss: 4.0415035124408405\n",
      "Iteration: 921 Loss: 1782.8158500411168 Diff loss: 31.11014594516041\n",
      "Iteration: 922 Loss: 1789.7140438429053 Diff loss: 6.898193801788466\n",
      "Iteration: 923 Loss: 1806.007544286969 Diff loss: 16.29350044406374\n",
      "Iteration: 924 Loss: 1820.1041679161463 Diff loss: 14.096623629177202\n",
      "Iteration: 925 Loss: 1840.92255589709 Diff loss: 20.818387980943726\n",
      "Iteration: 926 Loss: 1858.834639023682 Diff loss: 17.91208312659205\n",
      "Iteration: 927 Loss: 1879.702480791619 Diff loss: 20.86784176793708\n",
      "Iteration: 928 Loss: 1897.8403675279437 Diff loss: 18.137886736324617\n",
      "Iteration: 929 Loss: 1917.0046220492213 Diff loss: 19.1642545212776\n",
      "Iteration: 930 Loss: 1937.1482678670195 Diff loss: 20.143645817798188\n",
      "Iteration: 931 Loss: 1940.8461323741446 Diff loss: 3.697864507125132\n",
      "Iteration: 932 Loss: 1845.5084759675874 Diff loss: 95.33765640655724\n",
      "Iteration: 933 Loss: 1760.104207857628 Diff loss: 85.4042681099595\n",
      "Iteration: 934 Loss: 1794.7964022876808 Diff loss: 34.69219443005295\n",
      "Iteration: 935 Loss: 1796.3129682964639 Diff loss: 1.516566008783002\n",
      "Iteration: 936 Loss: 1825.7049376036473 Diff loss: 29.391969307183444\n",
      "Iteration: 937 Loss: 1833.0426801852434 Diff loss: 7.337742581596103\n",
      "Iteration: 938 Loss: 1862.1128162339332 Diff loss: 29.070136048689847\n",
      "Iteration: 939 Loss: 1870.2394580112734 Diff loss: 8.126641777340183\n",
      "Iteration: 940 Loss: 1892.220178412072 Diff loss: 21.980720400798646\n",
      "Iteration: 941 Loss: 1907.8729331952977 Diff loss: 15.652754783225646\n",
      "Iteration: 942 Loss: 1929.6008045777075 Diff loss: 21.72787138240983\n",
      "Iteration: 943 Loss: 1945.8526722888903 Diff loss: 16.25186771118274\n",
      "Iteration: 944 Loss: 1965.6444733066523 Diff loss: 19.791801017762054\n",
      "Iteration: 945 Loss: 1984.2902309942656 Diff loss: 18.64575768761324\n",
      "Iteration: 946 Loss: 2002.500117408817 Diff loss: 18.20988641455142\n",
      "Iteration: 947 Loss: 2021.7909094353424 Diff loss: 19.290792026525423\n",
      "Iteration: 948 Loss: 2038.0993594955512 Diff loss: 16.30845006020877\n",
      "Iteration: 949 Loss: 1944.96804624799 Diff loss: 93.13131324756114\n",
      "Iteration: 950 Loss: 1861.1536582627357 Diff loss: 83.81438798525437\n",
      "Iteration: 951 Loss: 1890.3512769394422 Diff loss: 29.197618676706497\n",
      "Iteration: 952 Loss: 1896.2747957913775 Diff loss: 5.9235188519353414\n",
      "Iteration: 953 Loss: 1923.8323678269523 Diff loss: 27.55757203557482\n",
      "Iteration: 954 Loss: 1931.867577454711 Diff loss: 8.035209627758604\n",
      "Iteration: 955 Loss: 1957.8628623007482 Diff loss: 25.995284846037293\n",
      "Iteration: 956 Loss: 1967.9147031718715 Diff loss: 10.051840871123204\n",
      "Iteration: 957 Loss: 1991.7197103875183 Diff loss: 23.80500721564681\n",
      "Iteration: 958 Loss: 2004.3954382320546 Diff loss: 12.675727844536368\n",
      "Iteration: 959 Loss: 2027.0861429082938 Diff loss: 22.690704676239193\n",
      "Iteration: 960 Loss: 2041.3225327995624 Diff loss: 14.236389891268573\n",
      "Iteration: 961 Loss: 2025.1731588420143 Diff loss: 16.14937395754805\n",
      "Iteration: 962 Loss: 1940.3372992065986 Diff loss: 84.83585963541577\n",
      "Iteration: 963 Loss: 1864.6787606796693 Diff loss: 75.6585385269293\n",
      "Iteration: 964 Loss: 1798.0532439209487 Diff loss: 66.62551675872055\n",
      "Iteration: 965 Loss: 1740.3129355127514 Diff loss: 57.74030840819728\n",
      "Iteration: 966 Loss: 1793.9664419555172 Diff loss: 53.653506442765774\n",
      "Iteration: 967 Loss: 1769.3443109489679 Diff loss: 24.622131006549353\n",
      "Iteration: 968 Loss: 1819.768594580505 Diff loss: 50.42428363153704\n",
      "Iteration: 969 Loss: 1799.0058697352176 Diff loss: 20.762724845287266\n",
      "Iteration: 970 Loss: 1848.8686724032216 Diff loss: 49.86280266800395\n",
      "Iteration: 971 Loss: 1829.2534818151462 Diff loss: 19.615190588075393\n",
      "Iteration: 972 Loss: 1864.1695115322125 Diff loss: 34.916029717066294\n",
      "Iteration: 973 Loss: 1860.5006549572222 Diff loss: 3.6688565749902864\n",
      "Iteration: 974 Loss: 1894.1258725780053 Diff loss: 33.62521762078313\n",
      "Iteration: 975 Loss: 1891.6548484311465 Diff loss: 2.471024146858781\n",
      "Iteration: 976 Loss: 1925.0833594735777 Diff loss: 33.42851104243118\n",
      "Iteration: 977 Loss: 1923.4125548612387 Diff loss: 1.6708046123389977\n",
      "Iteration: 978 Loss: 1956.2873493455209 Diff loss: 32.87479448428212\n",
      "Iteration: 979 Loss: 1955.6981230007302 Diff loss: 0.5892263447906316\n",
      "Iteration: 980 Loss: 1987.5295347477772 Diff loss: 31.831411747047014\n",
      "Iteration: 981 Loss: 1988.4912663631967 Diff loss: 0.9617316154194668\n",
      "Iteration: 982 Loss: 2018.954489206941 Diff loss: 30.463222843744234\n",
      "Iteration: 983 Loss: 2021.7814867506763 Diff loss: 2.826997543735388\n",
      "Iteration: 984 Loss: 2050.760198009276 Diff loss: 28.97871125859956\n",
      "Iteration: 985 Loss: 2055.556147110838 Diff loss: 4.795949101562201\n",
      "Iteration: 986 Loss: 2082.9391560089775 Diff loss: 27.38300889813945\n",
      "Iteration: 987 Loss: 2089.802123403694 Diff loss: 6.862967394716634\n",
      "Iteration: 988 Loss: 2115.460706667746 Diff loss: 25.658583264051686\n",
      "Iteration: 989 Loss: 2124.5040174455357 Diff loss: 9.043310777789884\n",
      "Iteration: 990 Loss: 2148.269634771457 Diff loss: 23.765617325921085\n",
      "Iteration: 991 Loss: 2159.6369151617873 Diff loss: 11.367280390330507\n",
      "Iteration: 992 Loss: 2181.184120314636 Diff loss: 21.54720515284862\n",
      "Iteration: 993 Loss: 2195.1150873202673 Diff loss: 13.930967005631373\n",
      "Iteration: 994 Loss: 2213.4735804326483 Diff loss: 18.358493112380984\n",
      "Iteration: 995 Loss: 2229.2414890368987 Diff loss: 15.767908604250351\n",
      "Iteration: 996 Loss: 2161.7071012960364 Diff loss: 67.53438774086226\n",
      "Iteration: 997 Loss: 2084.7479574160857 Diff loss: 76.95914387995072\n",
      "Iteration: 998 Loss: 2016.2564767074443 Diff loss: 68.49148070864135\n",
      "Iteration: 999 Loss: 1956.102269640284 Diff loss: 60.154207067160314\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [60]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m pred_ratings, loss \u001B[38;5;241m=\u001B[39m FrankWolfe(X_test_norm, FW_objective_function, delta \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m7250\u001B[39m, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-7\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "pred_ratings, loss = FrankWolfe(X_test_norm, FW_objective_function, delta = 7250, max_iter=1000, patience=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SP5L59OqmqMA",
    "outputId": "0deb189f-335a-49de-e9e3-873fd0884ff6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01674913, 0.01434759, 0.01507895, ..., 0.01425873, 0.01171163,\n",
       "        0.01912285],\n",
       "       [0.02097993, 0.01797177, 0.01888787, ..., 0.01786046, 0.01466997,\n",
       "        0.02395326],\n",
       "       [0.01818798, 0.01558014, 0.01637433, ..., 0.01548364, 0.01271773,\n",
       "        0.02076562],\n",
       "       ...,\n",
       "       [0.01870729, 0.01602499, 0.01684185, ..., 0.01592574, 0.01308085,\n",
       "        0.02135853],\n",
       "       [0.01441318, 0.01234658, 0.01297594, ..., 0.01227011, 0.01007824,\n",
       "        0.01645585],\n",
       "       [0.01650178, 0.01413571, 0.01485627, ..., 0.01404816, 0.01153867,\n",
       "        0.01884045]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ratings*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "TFmkDoxPmqMA",
    "outputId": "0b51ca71-cff2-435f-f9e9-0dccb54fcc2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01674913, 0.01434759, 0.01507895, ..., 0.01404816, 0.01153867,\n",
       "       0.01884045])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_ratings = np.argwhere(~np.isnan(X_test))\n",
    "idx_rows = idx_ratings[:,0]\n",
    "idx_cols = idx_ratings[:,1]\n",
    "pred_ratings[idx_rows,idx_cols]*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "g0Cx8xFtmqMB"
   },
   "source": [
    "#### Our data prediction"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "new_data = np.nan_to_num(data_matrix, 0)"
   ],
   "metadata": {
    "id": "kvTz-usn3I0b"
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "b9mDrB5PmqMB",
    "outputId": "28c2ca97-b340-4c5b-ffb4-803e5fece55f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(412898,)\n",
      "[5. 4. 5. ... 5. 2. 3.]\n",
      "Iteration: 0 Err: 169521072.7652239 Diff err: 166171807.7652239\n",
      "Iteration: 1 Err: 128322206.78089207 Diff err: 41198865.98433182\n",
      "Iteration: 2 Err: 59513961.45147687 Diff err: 68808245.3294152\n",
      "Iteration: 3 Err: 49811164.41752395 Diff err: 9702797.033952922\n",
      "Iteration: 4 Err: 30249657.409918882 Diff err: 19561507.00760507\n",
      "Iteration: 5 Err: 26538908.856101528 Diff err: 3710748.553817354\n",
      "Iteration: 6 Err: 18209640.242947124 Diff err: 8329268.613154404\n",
      "Iteration: 7 Err: 16855018.78428113 Diff err: 1354621.458665993\n",
      "Iteration: 8 Err: 12632848.066807121 Diff err: 4222170.71747401\n",
      "Iteration: 9 Err: 12131203.513994992 Diff err: 501644.55281212926\n",
      "Iteration: 10 Err: 9659089.161567688 Diff err: 2472114.352427304\n",
      "Iteration: 11 Err: 9500811.138292564 Diff err: 158278.0232751239\n",
      "Iteration: 12 Err: 7894403.550649143 Diff err: 1606407.5876434213\n",
      "Iteration: 13 Err: 7889573.968425838 Diff err: 4829.582223304547\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-42-551cbd27d3d0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpred_ratings\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFrankWolfe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mFW_objective_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdelta\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m43200\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_iter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-38-dbfc48bc01a5>\u001B[0m in \u001B[0;36mFrankWolfe\u001B[0;34m(X, objective_function, delta, Z_init, max_iter, patience)\u001B[0m\n\u001B[1;32m     56\u001B[0m         \u001B[0;31m#alpha - as studied in class\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m         \u001B[0malpha_k\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m         \u001B[0mZ\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0malpha_k\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mZ\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0malpha_k\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mupdate_Z\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[0;31m# Error\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "pred_ratings, loss = FrankWolfe(new_data, FW_objective_function, delta = 43200, max_iter=10000, patience=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y-VOlLMvfq8"
   },
   "source": [
    "# Frank-Wolfe In-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0EVyOx4mqMB",
    "outputId": "ae2ab9f6-667d-48ee-c1cc-79b75085dbd5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 4, 3, 8, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "a = np.array([1,2,1,2,1])\n",
    "b = np.array([1,2,3,4,5])\n",
    "np.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bRpuyws1mqMB"
   },
   "outputs": [],
   "source": [
    "def FW_objective_function(diff_vec):\n",
    "    return 0.5*(np.power(diff_vec,2).sum())\n",
    "    #return 0.5 * np.linalg.norm(diff_vec, 2)**2\n",
    "    \n",
    "def alpha_binary_search(Zk, Dk, delta, min_value = 0):\n",
    "        \n",
    "    #Inizialization\n",
    "    \n",
    "    best_alpha = (delta - min_value) / 2\n",
    "    \n",
    "    testing_matrix = Zk + best_alpha * Dk\n",
    "    \n",
    "    testing_mat_nuclear_norm = np.linalg.norm(testing_matrix, ord = 'nuc')\n",
    "    \n",
    "    #Binary Search\n",
    "    \n",
    "    while testing_mat_nuclear_norm <= delta:\n",
    "        \n",
    "        min_value = best_alpha\n",
    "        \n",
    "        best_alpha = (delta - min_value) / 2\n",
    "\n",
    "        testing_matrix = Zk + best_alpha * Dk\n",
    "    \n",
    "        testing_mat_nuclear_norm = np.linalg.norm(testing_matrix, ord = 'nuc')\n",
    "        \n",
    "    return best_alpha\n",
    "\n",
    "def FW_inface(X, objective_function, delta, Z_init = None, max_iter=150, patience=1e-3):\n",
    "    '''\n",
    "    :param X: sparse matrix with ratings and 'empty values', rows - users, columns - books.\n",
    "    :param objective_function: objective function that we would like to minimize with FW.\n",
    "    :param delta: feasible set ball radius\n",
    "    :param Z_init: In case we want to initialize Z with a known matrix, if not given Z_init will be a zeros matrix.\n",
    "    :param max_iter: max number of iterations for the method.\n",
    "    :param patience: once reached this tolerance provide the result.\n",
    "    :return: Z: matrix of predicted ratings - it should be like X but with no 'empty values'\n",
    "            loss: difference between original values (X) and predicted ones (Z).\n",
    "    '''\n",
    "\n",
    "    # Get X indexes for not empty values\n",
    "    idx_ratings = np.argwhere(X != 0)\n",
    "    #idx_ratings = np.argwhere(~np.isnan(X))\n",
    "    idx_rows = idx_ratings[:,0]\n",
    "    idx_cols = idx_ratings[:,1]\n",
    "\n",
    "    # Initialize Z_{-1}\n",
    "    if Z_init is not None:\n",
    "        Z = Z_init\n",
    "    else:\n",
    "        Z = np.zeros(X.shape)\n",
    "\n",
    "    # Create vectors with the not empty features of the sparse matrix\n",
    "    X_rated = X[idx_rows, idx_cols]\n",
    "    Z_rated = Z[idx_rows, idx_cols]\n",
    "    diff_vec = Z_rated - X_rated\n",
    "\n",
    "    # Initial gradient and Z0\n",
    "    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "    u_max, s_max, v_max = sparse.linalg.svds(grad, k = 1, which='LM')\n",
    "    Z = -delta*np.outer(u_max,v_max)\n",
    "    Z_rated = Z[idx_rows, idx_cols]\n",
    "\n",
    "    # Initialize lower bound on the optimal objective function (f*)\n",
    "    diff_vec = Z_rated - X_rated\n",
    "    new_low_bound = np.max((objective_function(diff_vec) + np.multiply(diff_vec,Z_rated)),0)\n",
    "\n",
    "    # Set L and D constants and gamma1, gamma2 constraints\n",
    "    L = 1\n",
    "    D = 2*delta\n",
    "    gamma1 = 0\n",
    "    gamma2 = 1\n",
    "\n",
    "    # Compute first iteration thin SVD\n",
    "    grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "    r_grad = sparse.csgraph.structural_rank(grad)   # Compute rank of the gradient sparse matrix to find thin SVD size\n",
    "    U_thin, D_thin, Vh_thin = sparse.linalg.svds(grad, k = r_grad, solver='propack', which='LM')   # Compute k = rank singular values # replaced r_grad with 1\n",
    "    print(r_grad, np.shape(U_thin), np.shape(Vh_thin))\n",
    "\n",
    "    # Additional needed parameters\n",
    "    diff_objective = patience + 1\n",
    "    objective = objective_function(diff_vec)\n",
    "    it = 0\n",
    "    while (diff_objective > patience) and (it < max_iter):\n",
    "\n",
    "        # Lower bound update\n",
    "        low_bound = new_low_bound\n",
    "\n",
    "        # In-face direction with the away step strategy: two calculations depending of where Z lies within the feasible set\n",
    "        if D_thin.sum() == delta: # Z in border (sum of singular values == radius of feasible set)\n",
    "            G = 0.5*(Vh_thin.dot(grad.T.dot(U_thin)) + U_thin.T.dot(grad.dot(Vh_thin.T)))\n",
    "            u = sparse.linalg.eigs(G, k = 1, which = 'SM')#unitary eigenvector corresponding to smallest eigenvalue of G\n",
    "            M = np.outer(u,u)\n",
    "            update_Z = delta*U_thin.dot(M.dot(Vh_thin))  # Zk tilde, right?\n",
    "            update_direction = Z-update_Z\n",
    "            alpha_B = scipy.linalg.inv(delta*u.T.dot(scipy.linalg.inv(update_direction).dot(u))-1)\n",
    "            \n",
    "        else: #inside\n",
    "            idx_max_s = np.argmax(D_thin)\n",
    "            update_Z = delta*np.outer(U_thin[idx_max_s,:],Vh_thin[idx_max_s,:])\n",
    "            update_direction = Z-update_Z\n",
    "            #BINARY SEARCH (xd)\n",
    "            alpha_B = alpha_binary_search(Z, # Zk\n",
    "                                          update_direction, # direction matrix\n",
    "                                          delta) \n",
    "\n",
    "        nuclear_norm = D_thin.sum()\n",
    "        U = nuclear_norm*D_thin # standardize the simplex\n",
    "        r = D_thin.shape\n",
    "        no_obs = idx_rows.shape[0]\n",
    "        THRES = 0.001\n",
    "        \n",
    "        if ((abs(delta - nuclear_norm) < THRES) and r > 1):\n",
    "            Z_B = Z + alpha_B*update_direction\n",
    "            diff_vec_B = Z_B[idx_rows, idx_cols] - X_rated\n",
    "            \n",
    "            if 1/(objective_function(diff_vec_B)-low_bound) >= (1/(objective-low_bound)+gamma1/(2*L*D**2)):\n",
    "                # 1. Move to a lower dimensional face\n",
    "                Z = Z_B\n",
    "            else:\n",
    "                beta = 0.5 # FIND A GOOD VALUE -- a binary search is also suggested by the paper xdd\n",
    "                Z_A = Z + beta*update_direction\n",
    "                diff_vec_A = Z_A[idx_rows, idx_cols] - X_rated\n",
    "\n",
    "                if 1/(objective_function(diff_vec_A)-low_bound) >= (1/(objective-low_bound)+gamma2/(2*L*D**2)):\n",
    "                    # 2. Stay in the current face\n",
    "                    Z = Z_A\n",
    "                else:\n",
    "                    # 3. Do a regular FW step and update the lower bound\n",
    "                    #Zk update\n",
    "                    idx_max_s = np.argmax(D_thin)\n",
    "                    update_Z = -delta*np.outer(U_thin[idx_max_s,:],Vh_thin[:,idx_max_s]) # Am i selecting right the vectors??\n",
    "                    alpha_k = 2/(it+2)\n",
    "\n",
    "                    print('Z BEFORE')\n",
    "                    print(Z)\n",
    "\n",
    "                    Z = (1-alpha_k)*Z + alpha_k*update_Z\n",
    "\n",
    "                    print('Z AFTER')\n",
    "                    print(Z)\n",
    "\n",
    "                    # Lower bound update\n",
    "                    direction_vec = update_Z.flatten() - Z.flatten()\n",
    "                    #grad = grad.toarray() # this method converts the sparse matrix into a numpy array!\n",
    "                    wolfe_gap = grad.T.dot(direction_vec)\n",
    "                    #wolfe_gap = grad.T.flatten() * direction_vec #added the flatten otherwise you can't do the operation\n",
    "                    B_w = objective + wolfe_gap.sum() # - bound_slack This is in matlab(Gabri)s code -- is it necessary? --> Gabri's answer: No, I don't think so\n",
    "                    #new_low_bound = np.max(low_bound, B_w)   # gave problems during the execution: wanted both of the number as integers??\n",
    "\n",
    "                    #Z, update_Z, low_bound = FrankWolfe(X, objective_function, delta = 43200, Z_init=None, max_iter=1, printing_res = False)\n",
    "\n",
    "                    # Update Lower Bound\n",
    "\n",
    "                    ###### TRIED THIS INSTEAD ####\n",
    "\n",
    "                    if low_bound >= B_w:\n",
    "                      new_low_bound = low_bound\n",
    "                    else:\n",
    "                      new_low_bound = B_w\n",
    "\n",
    "        else:\n",
    "            # 3. Do a regular FW step and update the lower bound\n",
    "            #Zk update\n",
    "            idx_max_s = np.argmax(D_thin)\n",
    "            update_Z = -delta*np.outer(U_thin[idx_max_s,:],Vh_thin[:,idx_max_s]) # Am i selecting right the vectors??\n",
    "            alpha_k = 2/(it+2)\n",
    "\n",
    "            print('Z BEFORE')\n",
    "            print(Z)\n",
    "\n",
    "            Z = (1-alpha_k)*Z + alpha_k*update_Z\n",
    "\n",
    "            print('Z AFTER')\n",
    "            print(Z)\n",
    "\n",
    "            # Lower bound update\n",
    "            direction_vec = update_Z.flatten() - Z.flatten()\n",
    "            #grad = grad.toarray() # this method converts the sparse matrix into a numpy array!\n",
    "            wolfe_gap = grad.T.dot(direction_vec)\n",
    "            #wolfe_gap = grad.T.flatten() * direction_vec #added the flatten otherwise you can't do the operation\n",
    "            B_w = objective + wolfe_gap.sum() # - bound_slack This is in matlab(Gabri)s code -- is it necessary? --> Gabri's answer: No, I don't think so\n",
    "            #new_low_bound = np.max(low_bound, B_w)   # gave problems during the execution: wanted both of the number as integers??\n",
    "\n",
    "            #Z, update_Z, low_bound = FrankWolfe(X, objective_function, delta = 43200, Z_init=None, max_iter=1, printing_res = False)\n",
    "\n",
    "            # Update Lower Bound\n",
    "\n",
    "            ###### TRIED THIS INSTEAD ####\n",
    "\n",
    "            if low_bound >= B_w:\n",
    "              new_low_bound = low_bound\n",
    "            else:\n",
    "              new_low_bound = B_w\n",
    "\n",
    "        # Loss\n",
    "        diff_vec = Z[idx_rows, idx_cols] - X_rated\n",
    "        new_objective = objective_function(diff_vec)\n",
    "\n",
    "        # Improvement at this iteration\n",
    "        diff_objective = np.abs(objective - new_objective)\n",
    "        objective = new_objective\n",
    "\n",
    "        # Gradient\n",
    "        grad = sparse.csr_matrix((diff_vec, (idx_rows, idx_cols)))\n",
    "\n",
    "        # Thin SVD\n",
    "        r_grad = sparse.csgraph.structural_rank(grad)   # Compute rank of the gradient sparse matrix to find thin SVD size\n",
    "        U_thin, D_thin, Vh_thin = sparse.linalg.svds(grad, k = r_grad-1, solver='propack', which='LM')   # Compute k = rank singular values # replaced r_grad with 1\n",
    "\n",
    "        # Count iteration\n",
    "        it += 1\n",
    "        \n",
    "        print('Iteration:', it, 'f(Z_k):', objective, 'f(Z_{k-1}) -f(Z_k):', diff_objective)\n",
    "\n",
    "    return Z, objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PwkTaaHMmqMC"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`solver='propack'` is opt-in due to potential issues on Windows, it can be enabled by setting the `USE_PROPACK` environment variable before importing scipy",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [98]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m pred_ratings, loss \u001B[38;5;241m=\u001B[39m \u001B[43mFW_inface\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mFW_objective_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelta\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7250\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-7\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [97]\u001B[0m, in \u001B[0;36mFW_inface\u001B[1;34m(X, objective_function, delta, Z_init, max_iter, patience)\u001B[0m\n\u001B[0;32m     75\u001B[0m grad \u001B[38;5;241m=\u001B[39m sparse\u001B[38;5;241m.\u001B[39mcsr_matrix((diff_vec, (idx_rows, idx_cols)))\n\u001B[0;32m     76\u001B[0m r_grad \u001B[38;5;241m=\u001B[39m sparse\u001B[38;5;241m.\u001B[39mcsgraph\u001B[38;5;241m.\u001B[39mstructural_rank(grad)   \u001B[38;5;66;03m# Compute rank of the gradient sparse matrix to find thin SVD size\u001B[39;00m\n\u001B[1;32m---> 77\u001B[0m U_thin, D_thin, Vh_thin \u001B[38;5;241m=\u001B[39m \u001B[43msparse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msvds\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mr_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msolver\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpropack\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhich\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mLM\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m   \u001B[38;5;66;03m# Compute k = rank singular values # replaced r_grad with 1\u001B[39;00m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28mprint\u001B[39m(r_grad, np\u001B[38;5;241m.\u001B[39mshape(U_thin), np\u001B[38;5;241m.\u001B[39mshape(Vh_thin))\n\u001B[0;32m     80\u001B[0m \u001B[38;5;66;03m# Additional needed parameters\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\linalg\\_eigen\\_svds.py:319\u001B[0m, in \u001B[0;36msvds\u001B[1;34m(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors, solver, random_state, options)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m solver \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpropack\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m HAS_PROPACK:\n\u001B[1;32m--> 319\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`solver=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpropack\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m` is opt-in due to potential issues on Windows, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mit can be enabled by setting the `USE_PROPACK` environment \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariable before importing scipy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    322\u001B[0m     jobu \u001B[38;5;241m=\u001B[39m return_singular_vectors \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[0;32m    323\u001B[0m     jobv \u001B[38;5;241m=\u001B[39m return_singular_vectors \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvh\u001B[39m\u001B[38;5;124m'\u001B[39m}\n",
      "\u001B[1;31mValueError\u001B[0m: `solver='propack'` is opt-in due to potential issues on Windows, it can be enabled by setting the `USE_PROPACK` environment variable before importing scipy"
     ]
    }
   ],
   "source": [
    "pred_ratings, loss = FW_inface(X_test_norm, FW_objective_function, delta = 7250, max_iter=1000, patience=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration: 1 f(Z_k): 169521072.7652234 f(Z_{k-1}) -f(Z_k): 165722575.63213205\n",
      "Iteration: 2 f(Z_k): 169521072.76522353 f(Z_{k-1}) -f(Z_k): 1.1920928955078125e-07\n"
     ]
    }
   ],
   "source": [
    "pred_ratings, loss = FW_inface(new_data, FW_objective_function, delta = 7000, max_iter=201)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "H7N0qlhVmqMC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "RQCKKKQgmqMC"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xnA8pnJfmqMC"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpS5jRz4vpHa"
   },
   "source": [
    "## Sub-Chapter"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FW_GoodReads_recommender - Copia.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}